# Abstract

Cloud-native applications have evolved into a standardized architecture consisting of multiple loosely coupled components called microservices (often typically implemented as containers) that are supported by an infrastructure for providing application services, such as service mesh. Both of these components are usually hosted on a container orchestration and resource management platform.

In this architecture, the entire set of source code involved in the application environment can be divided into five code types:
1. **Application code** (which embodies the application logic)
2. **Application services code** (for services such as session establishment, network connection, etc.)
3. **Infrastructure as code** (for provisioning and configuring computing, networking, and storage resources)
4. **Policy as code** (for defining runtime policies such as zero trust expressed as a declarative code)
5. **Observability as code** (for the continuous monitoring of an application runtime state)

Due to security, business competitiveness, and the inherent structure of loosely coupled application components, this class of applications needs a different development, deployment, and runtime paradigm.

**DevSecOps** (consisting of acronyms for Development, Security, and Operations, respectively) has been found to be a facilitating paradigm for these applications with primitives such as continuous integration, continuous delivery, and continuous deployment (CI/CD) pipelines. These pipelines are workflows for taking the developer’s source code through various stages, such as building, testing, packaging, deployment, and operations supported by automated tools with feedback mechanisms.

The objective of this document is to provide guidance for the implementation of DevSecOps primitives for cloud-native applications with the architecture and code types described above. The benefits of this approach for high security assurance and for enabling continuous authority to operate (C-ATO) are also discussed.

# Executive Summary

Cloud-native applications have evolved into a standardized architecture consisting of the following components:
- Multiple loosely coupled components called microservices (often or typically implemented as containers)
- An application services infrastructure that provides services such as secure communication, authentication, and authorization for users, services, and devices (e.g., service mesh)

Due to security, business competitiveness, and its inherent structure (loosely coupled application components), this class of applications needs a different application, deployment, and runtime monitoring paradigm – collectively called the software life cycle paradigm.

**DevSecOps** (consisting of acronyms for Development, Security, and Operations, respectively) is one of the facilitating paradigms for the development, deployment, and operation of these applications.# CI/CD Pipelines and Code Types

## Overview of CI/CD Pipelines
CI/CD pipelines are workflows for taking the developer’s source code through various stages, such as building, functional testing, security scanning for vulnerabilities, packaging, and deployment. These processes are supported by automated tools with feedback mechanisms.

## Code Types in Application Environment
For the purpose of this document, the entire set of source code involved in the application environment is classified into five code types:

1. **Application Code**: Embodies the application logic for carrying out one or more business functions.
2. **Application Services Code**: For services such as session establishment, network connection, etc.
3. **Infrastructure as Code**: Computing, networking, and storage resources in the form of declarative code.
4. **Policy as Code**: Runtime policies (e.g., zero trust) expressed as declarative code.
5. **Observability as Code**: Continuous monitoring of the health of the application, where monitoring functions are expressed as declarative code.

Separate CI/CD pipelines can be created for all five code types. The functions performed by each of these code types are also described to highlight the roles they play in the overall execution of the application.

## Platform Considerations
Though cloud-native applications have a common architectural stack, the platform on which the components of the stack run may vary. The platform is an abstraction layer over a physical (bare metal) or virtualized (e.g., virtual machines, containers) infrastructure.

In this document, the chosen platform is a container orchestration and resource management platform (e.g., Kubernetes). To unambiguously refer to this platform or application environment throughout this document, it is called the **Reference Platform for DevSecOps Primitives**, or simply the **reference platform**.

## Objective of the Document
The objective of this document is to provide guidance for the implementation of DevSecOps primitives for the reference platform. The benefits of this implementation for high security assurance and the use of the artifacts within the pipelines for providing continuous authority to operate (C-ATO) using risk management tools and dashboard metrics are also described.

# Introduction
Cloud-native applications are made up of multiple loosely coupled components (called microservices, typically implemented as containers), operate in perimeter-less network environments requiring zero trust concepts (on-premises or cloud), and are accessed by users from a diverse set of locations (e.g., campus, home office, etc.). Cloud-native applications do not just refer to applications that run in the cloud; they also refer to the class of applications with...# Design and Runtime Architectures

## Introduction
Design and runtime architectures, such as microservices, require a dedicated infrastructure for providing all application services, including security. The incorporation of zero trust principles into this class of application provides techniques wherein access to all protected resources is enforced through identity-based protection and network-based protections (e.g., microsegmentation), where applicable.

## Cloud-Native Applications
Cloud-native applications necessitate agile and secure updates and deployment techniques for business reasons, as well as the necessary resilience to respond to cybersecurity events. Hence, they call for a different application development, deployment, and runtime monitoring paradigm (collectively called the software life cycle paradigm) than the ones used for traditional monolithic or multi-tier applications.

### DevSecOps
DevSecOps (Development, Security, and Operations) is a facilitating paradigm for this class of applications since it facilitates agile and secure development, delivery, deployment, and operations through:
- **Primitives**: Such as continuous integration, continuous delivery/continuous deployment (CI/CD) pipelines (explained in section 3).
- **Security Testing**: Throughout the life cycle.
- **Continuous Monitoring**: During runtime, all of which are supported by automation tools.

The paradigm that meets the above objectives was originally given the term DevOps to indicate the fact that it seeks to remove the silos between development and operations groups and promotes (or drives) increased collaboration. The term DevSecOps was later coined by a portion of the community to emphasize the role of the security team in the whole process.

### Culture and Practices
Thus, DevSecOps denotes a culture and set of practices with automation tools to drive increased collaboration, trust, shared responsibility, transparency, autonomy, agility, and automation across the key stakeholders responsible for delivering software, including development, operations, and security organizations. DevSecOps has the necessary primitives and other building blocks to meet the design goals of cloud-native applications.

## Community Consensus
It should be noted that there is no community-wide consensus on the term “DevSecOps.” The term was primarily coined to emphasize that security must be tested and incorporated in all stages of the software development life cycle (i.e., build, test, package, deploy, and operate). A portion of the community continues to use the term “DevOps” based on the argument that there is no need to define a new term since security must be an integral part of any software life cycle process.

## 1.1 Scope
In theory, DevSecOps primitives can be applied to many application architectures but are best suited to microservices-based ones, which permit agile development paradigms due to the fact that the application is made up of relatively small, loosely coupled modules called microservices. Even within microservices-based architectures, the implementation of DevSecOps...# Reference Platform for DevSecOps Primitives

Primitives can take on different forms, depending on the platform. In this document, the chosen platform is a container orchestration and resource management platform (e.g., Kubernetes). The platform is an abstraction layer over a physical (bare metal) or virtualized (e.g., virtual machines, containers) infrastructure. To unambiguously refer to this platform or application environment throughout this document, it is called the **Reference Platform for DevSecOps Primitives**, or simply the **reference platform**.

## Due Diligence for Service Mesh Component

Before describing the implementation of DevSecOps primitives for the reference platform, it is assumed that the following due diligence is applied with respect to the deployment of the service mesh component:

- **Secure design patterns** for deploying and managing service mesh-based components for infrastructure (e.g., network routing), policy enforcement, and monitoring.
- **Tests** to prove that these service mesh components work as intended in a variety of scenarios for all aspects of the application, such as ingress, egress, and inside services.

## Guidance for Implementation

The guidance provided for the implementation of DevSecOps primitives for the reference platform is agnostic to:

1. The tools used in DevSecOps pipelines.
2. The service mesh software, which provides application services.

Though examples from service mesh offerings, such as Istio, are used to link them to real-world application artifacts (e.g., containers, policy enforcement modules, etc.).

## Code Types in the Reference Platform

A slightly more detailed description of the code types (referred to in the executive summary) in the entire application environment presented by the reference platform is given below. Please note that these code types include those that support the implementation of DevSecOps primitives.

1. **Application Code** – Embodies the application logic for carrying out one or more business functions and is made up of code describing the business transactions and database access.

2. **Application Services Code** (e.g., service mesh code) – Provides various services for the application, such as service discovery, establishing network routes, network resiliency services (e.g., load balancing, retries), and security services (e.g., enforcing authentication, authorization, etc. based on policies).

3. **Infrastructure as Code** – Expresses the computing, networking, and storage resources needed to run the application in the form of declarative code.

4. **Policy as Code** – Contains declarative code for generating the rules and configuration parameters for realizing security objectives, such as zero trust through security controls (e.g., authentication, authorization) during runtime.

5. **Observability as Code** – Triggers software related to logging (recording all transactions) and tracing (communication pathways involved in executing application requests) and monitors (keeping track of application states during runtime).

**Note:** Code types 3, 4, and 5 may have overlap with code type 2.# Implementation of Pipelines and Workflows

This document covers the implementation of pipelines or workflows associated with all five code types listed above. Thus, the entire application environment (not just the application code) benefits from all of the best practices that exist for application code (e.g., agile iterative development, version control, governance, etc.).

## Declarative Code

Infrastructure as code, policy as code, and observability as code belong to a special class called declarative code. When using ‘as code’ techniques, the code that is written (e.g., for provisioning a resource) is managed similarly to application source code. This implies that it is:

- Versioned
- Documented
- Has access controls defined similar to what is done for application source code repository.

Often, domain-specific declarative languages are used: the requirements are declared, and an associated tool converts them into artifacts that make up a runtime instance.

### Example: Infrastructure as Code (IaC)

In the case of infrastructure as code (IaC), the declarative language models the infrastructure as a series of resources. The associated configuration management tool pulls together these resources and generates what are known as manifests that define the final shape and state of the platform (runtime instance) associated with the defined resources.

These manifests are stored in servers associated with a configuration management tool and are used by the tool to create compiled configuration instructions for the runtime instance on the designated platform. Manifests are generally encoded in platform-neutral representations (e.g., JSON) and fed to platform resource provisioning agents through REST APIs.

## Related DevSecOps Initiatives

There are several DevSecOps initiatives in various agencies of the Federal Government with varying emphasis and focus, depending on the processes enabled by software and mission needs. Though not exhaustive, here is a brief overview of those initiatives:

- **DevSecOps pipelines** are involved in building, checking in, and checking out from a container registry called Iron Bank, a repository of DOD-vetted hardened container images.

- **The Air Force’s Platform One** is the DevSecOps platform that enabled the concept of continuous authority to operate (C-ATO), which in turn streamlined the DOD’s authorization process to accommodate the speed and frequency of modern continuous software deployments.

- **The National Geospatial-Intelligence Agency (NGA)** outlined its DevSecOps strategy in “The NGA Software Way,” where three key metrics – availability, lead time, and deployment frequency – are laid out for each of its software products, along with specification of seven distinct product lines for enabling DevSecOps pipelines, including messaging and workflow tools.

- **The Centers for Medicare and Medicaid Services (CMS)** is adopting a DevSecOps approach where one emphasis is on laying the groundwork for the software bill of materials (SBOM) – a formal record that contains the details and supply chain relationships of various components used in building software.# Producing SBOMs

Producing SBOMs is to meet the goals established under the Continuous Diagnostics and Mitigation (CDM) program.

## DevSecOps Implementation

- At the Naval Surface Warfare Center (NSWC), the implementation methodology of DevSecOps primitives is used to teach and train the software workforce on various software metrics and the role of automation as enablers for achieving those metrics.
- The Army’s DevSecOps initiative is called the Army Software Factory and is focused on building skillsets rather than building software. It utilizes DevSecOps capabilities (pipelines and platform-as-a-service features) as a technology accelerator to gain efficiency and proficiency in product management, user experience, user interface (UI/UX) design, platform, and software engineering.

### 1.3 Target Audience

Since DevSecOps primitives span development (build and test for security, package), delivery/deployment, and continuous monitoring (to ensure secure states during runtime), the target audience for the recommendations in this document includes software development, operations, and security teams.

### 1.4 Relationship to Other NIST Guidance Documents

Since the reference platform is made up of container orchestration and resource management platform and service mesh software, the following publications offer guidance for securing this platform as well as provide background information for the contents of this document:

- **Special Publication (SP) 800-204**: Security Strategies for Microservices-based Application Systems discusses the characteristics and security requirements of microservices-based applications and the overall strategies for addressing those requirements.
- **SP 800-204A**: Building Secure Microservices-based Applications Using Service-Mesh Architecture provides deployment guidance for various security services (e.g., establishment of secure sessions, security monitoring, etc.) for a microservices-based application using a dedicated infrastructure (i.e., a service mesh) based on service proxies that operate independent of the application code.
- **SP 800-204B**: Attribute-based Access Control for Microservices-based Applications Using a Service Mesh provides deployment guidance for building an authentication and authorization framework within the service mesh that meets the security requirements, such as (1) zero trust by enabling mutual authentication in communication between any pair of services and (2) a robust access control mechanism based on an access control model, such as attribute-based access control (ABAC) model, that can be used to express a wide set of policies and is scalable in terms of user base, objects (resources), and deployment environment.
- **SP 800-190**: Application Container Security Guide explains the security concerns.# Container Technologies and DevSecOps Implementation

## 1. Introduction
This document addresses concerns associated with container technologies and provides practical recommendations for planning, implementing, and maintaining containers. The recommendations are structured for each tier within the container technology architecture.

## 1.5 Organization of This Document
This document is organized as follows:

- **Chapter 2**: Provides a brief description of the reference platform for which guidance for the implementation of DevSecOps primitives is provided.
- **Chapter 3**: Introduces the DevSecOps primitives (i.e., pipelines), the methodology for designing and executing the pipelines, and the role that automation plays in the execution.
- **Chapter 4**: Covers all facets of pipelines, including:
- Common issues to be addressed for all pipelines.
- Descriptions of the pipelines for the five code types in the reference platform listed in Section 1.1.
- The benefit of DevSecOps for security assurance for the entire application environment (the reference platform with five code types) during the entire life cycle, including the “Continuous Authority to Operate (C-ATO).”
- **Chapter 5**: Provides a summary and conclusion.

## 2. Reference Platform for the Implementation of DevSecOps Primitives
As stated in Section 1.1, the reference platform is a container orchestration and management platform. In modern application environments, the platform serves as an abstraction layer over a physical (bare metal) or virtualized (e.g., virtual machines, containers) infrastructure. Before the implementation of DevSecOps primitives, the platform simply contains the application code, which includes the application logic and the service mesh code that provides application services. This section will consider the following:

- A container orchestration and resource management platform that houses both the application code and most of the service mesh code.
- The service mesh software architecture.

### 2.1 Container Orchestration and Resource Management Platform
Since microservices are typically implemented as containers, a container orchestration and resource management platform is used for the deployment, operations, and maintenance of services. A typical orchestration and resource management platform consists of various logical (forming the abstraction layer) and physical artifacts for the deployment of containers.

For example, in Kubernetes, containers run inside the smallest unit of deployment called a pod. A pod can theoretically host a group of containers, though usually, only one container runs inside a pod. A group of pods is defined inside what is known as a node, where a node can be either a physical or virtual machine (VM). A group of nodes constitutes a cluster.

Usually, multiple instances of a single microservice are needed to distribute the workload to achieve the desired performance level. A cluster is a pool of resources (nodes) that is used to distribute the workload of microservices. One of the techniques used is horizontal scaling, where microservices that are...# Security Limitations of Orchestration Platform

Microservices-based applications require several application services, including security services such as authentication and authorization, as well as the generation of metrics for individual pods (monitoring), consolidated logging (to ascertain causes of failures of certain requests), tracing (sequence of service calls for an application request), traffic control, caching, secure ingress, service-to-service (east/west traffic), and egress communication.

## Secure Communications Between Containers

Taking the example of secure communications between containers, specific code needs to be added in order to secure the communications between pods in a platform such as Kubernetes (e.g., with mTLS). Pods that communicate do not apply identity and access management between themselves.

Though there are tools that can be implemented to act as a firewall between pods, such as the Kubernetes Network Policy, this is a layer 3 solution rather than a layer 7 solution, which is what most modern firewalls are. This means that while one can know the source of traffic, one cannot peek into the data packets to understand what they contain.

Further, it does not allow for making vital metadata-driven decisions, such as routing on a new version of a pod based on an HTTP header. There are Kubernetes ingress objects that provide a reverse proxy based on layer 7, but they do not offer anything more than simple traffic routing.

## Deployment and Traffic Management Limitations

Kubernetes does offer different ways of deploying pods that can form A/B testing or canary deployments, but they are done at the connection level and provide no fine-grained control or fast failback. For example, if a developer wants to deploy a new version of a microservice and pass 10% of traffic through it, they will have to scale the containers to at least 10—nine for the old version and one for the new version.

Further, Kubernetes cannot split the traffic intelligently and instead balances loads between pods in a round-robin fashion. Every Kubernetes container within a pod has a separate log, and a custom solution over Kubernetes must be implemented to capture and consolidate them.

## Monitoring and Logging Challenges

Although the Kubernetes dashboard offers monitoring features on individual pods and their states, it does not expose metrics that describe how application components interact with each other or how much traffic flows through each of the pods. Consolidated logging is required to determine error conditions that cause an application request or transaction to fail.

Tracing is required to trace the sequence of containers that are invoked as part of any application request based on the application logic that underlies a transaction. Since traffic flow cannot be traced through Kubernetes pods out of the box, it is unclear where on the chain the failure for the request occurred.

## Conclusion

This is where the service mesh software can provide the needed application support.# Service Mesh Software Architecture

Having looked at the various application services required by microservices-based applications, consider the architecture of service mesh software that provides those services. The service mesh software consists of two main components: the control plane and the data plane.

## 2.2.1 Control Plane

The control plane has several components. While the data plane of the service mesh mainly consists of proxies running as containers within the same pod as application containers, the control plane components run in their own pods, nodes, and associated clusters. The following are the various functions of the control plane:

1. Service discovery and configuration of the Envoy sidecar proxies
2. Automated key and certificate management
3. API for policy definition and the gathering of telemetry data
4. Configuration ingestion for service mesh components
5. Management of an inbound connection to the service mesh (Ingress Gateway)
6. Management of an outbound connection from the service mesh (Egress Gateway)
7. Inject sidecar proxies into those pods, nodes, or namespaces where application microservice containers are hosted

Overall, the control plane helps the administrator populate the data plane component with the configuration data that is generated from the policies resident in the control plane. The policies for function 3 above may include network routing policies, load balancing policies, policies for blue-green deployments, canary rollouts, timeout, retry, and circuit-breaking capabilities. These last three are collectively called by the special name of resiliency capabilities of the networking infrastructure services. Last but not least are security-related policies (e.g., authentication and authorization policies, TLS establishment policies, etc.). These policy rules are parsed by a module that converts them into configuration parameters for use by executables in data plane proxies that enforce those policies.

## 2.2.2 Data Plane

The data plane component performs three different functions:

1. Secure networking functions
2. Policy enforcement functions
3. Observability functions

The primary component of the data plane that performs all three functions listed above is called the sidecar proxy. This L7 proxy runs in the same network namespace (which, in this platform, is the same pod) as the microservice for which it performs proxy functions. There is a proxy for every microservice to ensure that a request from a microservice does not bypass its associated proxy and that each proxy is run as a container in the same pod as the application microservice. Both containers have the same IP address and share the same IP Table rules. That makes the proxy take complete control over the pod and handle all traffic that passes through it.

The first category of functions (secure networking) includes all functions related to the actual routing or communication of messages between microservices.# Service Mesh Overview

## Introduction
Service meshes are critical in managing microservices architecture, providing functionalities such as service discovery, secure communication, and policy enforcement.

## Key Functions of Service Mesh

### 1. Service Discovery and Secure Communication
- **Service Discovery**: Identifying and locating services within the mesh.
- **Establishing Secure Sessions**: Using TLS to secure communication between services.
- **Routing Rules**: Defining paths for requests to reach the appropriate microservices.
- **Authentication and Authorization**: Validating requests from services or users.

### 2. Mutual TLS Session Establishment
- The proxy initiates communication and interacts with the control plane to determine if traffic needs encryption.
- **Certificate Management**: Each pod requires a valid certificate, leading to the management of potentially hundreds of short-lived certificates in large applications.
- **Identity and Access Management**: The service mesh must include an access manager, certificate store, and validation capabilities.

### 3. Proxy Types
- **Ingress Proxies**: Intercept client calls to the first microservice.
- **Egress Proxies**: Handle requests to external application modules.

### 4. Policy Enforcement
- The data plane enforces policies defined in the control plane through configuration parameters in proxies.
- **JWT Token Usage**: Utilized for authenticating calling services.
- **Access Control Policies**: Enforced either through proxy code or external authorization services.

### 5. Telemetry and Monitoring
- Proxies gather telemetry data to monitor service health and state.
- **Log Aggregation**: Transfer logs to the control plane for analysis.
- **Request Tracing**: Append necessary data to application request headers for tracing transactions.

### 6. Response Handling
- Proxies convey application responses back to the calling service, including return codes, descriptions, or retrieved data.

## Integration with Container Orchestration
- The service mesh is aware of the container orchestration platform and interacts with the API server.
- It monitors for new microservices and automatically injects sidecar containers into the relevant pods.

## Conclusion
Service meshes play a vital role in managing microservices, ensuring secure communication, enforcing policies, and providing monitoring capabilities essential for maintaining the health of applications.# DevSecOps: Organizational Preparedness, Key Primitives, and Implementation

## Introduction
DevSecOps incorporates security into the software engineering process early on. It integrates and automates security processes and tooling into all of the development workflow (or pipeline) in DevOps so that it is seamless and continuous. In other words, it can be looked upon as a combination of the three processes: Development + Security + Operations.

## Key Aspects of DevSecOps
This section discusses the following aspects of DevSecOps:
- Organizational preparedness for DevSecOps
- DevSecOps Platform
- Fundamental building blocks or key primitives for DevSecOps

### 1. Organizational Preparedness for DevSecOps
DevSecOps is a software development, deployment, and life cycle management methodology that involves a shift from one large release for an entire application or platform to the continuous integration, continuous delivery, and continuous deployment (CI/CD) approach. This shift requires changes in the structure of a company’s IT department and its workflow.

The most pronounced change involves organizing a DevSecOps group that consists of software developers, security specialists, and IT operations experts for each portion of the application (i.e., the microservice). This smaller team not only promotes efficiency and effectiveness in initial agile development and deployment but also in subsequent life cycle management activities, such as monitoring application behavior, developing patches, fixing bugs, or scaling the application. The composition of this cross-functional team with expertise in three areas forms a critical success factor for introducing DevSecOps in an organization.

### 2. DevSecOps Platform
DevSecOps is an agile, automated development and deployment process that uses primitives called CI/CD pipelines aided by automated tools to take the software from the build phase to deployment phase and finally to the runtime/operations phase. These pipelines are workflows that take the developer’s source code through various stages, such as building, testing, packaging, delivery, and deployment supported by testing tools in various phases.

A DevSecOps platform denotes the set of resources on which various CI/CD processes are executed, ensuring that security is integrated at every stage of the development lifecycle.# DevSecOps Platform Overview

## Components of a DevSecOps Platform

At the minimum, this platform consists of the following components:

### (a) Pipeline Software
- **CI Software**:
- Pulls code from a code repository.
- Invokes the build software.
- Invokes test tools and stores back tested artifacts to the image registry.
- **CD Software**:
- Pulls out artifacts, packages, and based on computing, network, and storage resource descriptions in Infrastructure as Code (IaC), deploys the package.

### (b) SDLC Software
- **Build Tools**: (e.g., IDEs)
- **Testing Tools**: (SAST, DAST, SCA)

### (c) Repositories
- **Source Code Repositories**: (e.g., GitHub)
- **Container Image Repositories or Registries**

### (d) Observability or Monitoring Tools
- **Logging and Log Aggregation Tools**
- **Tools that Generate Metrics**
- **Tracing Tools**: (sequence of application calls)
- **Visualization Tools**: (combine data from above to generate dashboards/alerts)

## Security Assurance in DevSecOps

In a DevSecOps platform, security assurance is provided during the build and deployment phases through built-in design features, such as zero trust, and testing using a comprehensive set of security testing tools, including:

- **Static Application Security Tools (SAST)**
- **Dynamic Security Testing Tools (DAST)**
- **Software Composition Analysis (SCA) Tools**

Additionally, security assurance is also provided during the runtime/operations phase by continuous behavior detection/prevention tools, some of which may utilize sophisticated techniques such as artificial intelligence (AI) and machine learning (ML). Therefore, a DevSecOps platform operates not only during build and deployment phases but also during the runtime/operations phase.

## Integration of Security Tools

In some DevSecOps platforms, security tools (e.g., SAST, DAST, and SCAs) that perform application security analysis—such as identifying vulnerabilities and bugs through efficient scanning in the background—can be tightly integrated with integrated development environments (IDEs) and other DevOps tools. This feature, when present, makes these tools transparent to developers and eliminates the necessity for them to call separate APIs for running these tools. Depending on the IDE, the task performed, or the resources consumed by the tool, the tool may alternatively execute separately from the IDE.

## Deliverables for DevSecOps Platform

The use of SAST, DAST, and SCA tools may not be limited to testing just the application code. DevSecOps may include the use of these tools for other code types such as IaC, as IaC defines the deployment architecture of the application and thus serves as a key avenue to automatically assess and remediate security design gaps.

### Summary of Deliverables
A DevSecOps platform delivers the following:
- Provides security assurance through the incorporation of adequate testing/checking within pipelines associated with all code types in the application environment.
- Security is not relegated to a separate task or phase.# DevSecOps Platform Overview

The DevSecOps platform operates during runtime (in production), providing real-time assurance of security by assisting enforcement of zero trust principles and through continuous monitoring followed by alerts and correction mechanisms, thus enabling the certification of continuous authority to operate (C-ATO).

## 3.3 DevSecOps – Key Primitives and Implementation Tasks

The key primitives and implementation tasks involved are:

- Concept of pipelines and the CI/CD pipeline
- Building blocks for the CI/CD pipeline
- Designing and executing the CI/CD pipeline
- Strategies for automation
- Requirements for security automation tools in the CI/CD pipeline

### 3.3.1 Concept of Pipelines and the CI/CD Pipeline

DevSecOps, being a methodology or framework for agile application development, deployment, and operations, is made up of stages just like any other methodology. The sequence and flow of information through the stages is called workflow, where some stages can be executed in parallel while others have to follow a sequence. Each stage may require the invocation of a unique job to execute the activities in that stage.

A unique concept that DevSecOps introduces in the process workflow is the concept of “pipelines.” With pipelines, there is no need to individually write jobs for initiating/executing each stage of the process. Instead, there is only one job that starts from the initial stage, automatically triggers the activities/tasks pertaining to other stages (both sequential and parallel), and creates an error-free smart workflow.

The pipeline in DevSecOps is called the CI/CD pipeline based on the overall tasks it accomplishes and the two individual stages it contains. CD can denote either the continuous delivery or continuous deployment stage. Depending on this latter stage, CI/CD can involve the following tasks:

- **Build, Test, Secure, and Deliver** – the tested modified code is delivered to the staging area.
- **Build, Test, Secure, Deliver, and Deploy** – the code in the staging area is automatically deployed.

In the former, automation ends at the delivery stage, and the next task of deployment of the modified application in the hosting platform infrastructure is performed manually. In the latter, the deployment is also automated. Automation of any stage in the pipeline is enabled by tools that express the pipeline stage as code.

The workflow process for a CI/CD pipeline is depicted in Figure 1 below:

The unit and integration tests shown in the diagram use the SAST, DAST, and SCA tools described in Section 3.2. It should be noted that an organization has the option to continue the build process when a test fails. Depending on how the organization balances risk tolerance versus business needs, it may choose to fail-open (log and continue) or fail-closed (stop/break) when a specific test fails. In the fail-closed event, the developer gets the test outcome report, must fix the issues, and restart the CI process.# Continuous Integration and Continuous Delivery

## Continuous Integration (CI)
Continuous integration involves developers frequently merging code changes into a central repository where automated builds and tests run.

- **Build Process**: The build is the process of converting the source code to executable code for the platform on which it is intended to run.
- In the CI/CD pipeline software, the developer’s changes are validated by creating a build and running automated tests against the build.
- This process avoids the integration challenges that can happen when waiting for release day to merge changes into the release branch.

## Continuous Delivery (CD)
Continuous delivery is the stage after continuous integration where code changes are deployed to a testing and/or staging environment after the build stage.

- **Release Frequency**: Continuous delivery to a production environment involves the designation of a release frequency – daily, weekly, fortnightly, or some other period – based on the nature of the software or the market in which the organization operates.
- This means that on top of automated testing, there is a scheduled release process, though the application can be deployed at any time by clicking a button.
- The deployment process in continuous delivery is characterized as manual, but tasks such as the migration of code to a production server, the establishment of networking parameters, and the specification of runtime configuration data may be performed by automated scripts.

## Continuous Deployment
Continuous deployment is similar to continuous delivery except that the releases happen automatically, and changes to code are available to customers immediately after they are made.

- **Automatic Release Process**: The automatic release process may in many instances include A/B testing to facilitate slow rollout of new features so as to mitigate the impact of failures if there is a bug/error.
- The distinction between continuous delivery and continuous deployment is illustrated in the accompanying figure.

## Building Blocks for CI/CD Pipelines
The primary software for defining CI/CD pipeline resources, building the pipelines, and executing those pipelines is the CI/CD pipeline software.

- There may be slight variations in the architecture of this class of software depending on the particular offering.
- The following is an overview of the landscape in which CI/CD tools (pipeline software) operate:

### CI/CD Tool Characteristics
- Some CI/CD tools natively operate on the platform on which the application and the associated resources are hosted (i.e., container orchestration and resource management platform), while others need to be integrated into the application hosting platform through its API.

#### Advantages of Native CI/CD Tools
- It makes it easier to deploy, maintain, and manage the CI/CD tool itself.
- Every pipeline defined by the CI/CD tool becomes another platform-native resource and is managed the same way.
- In fact, all entities required for executing pipelines, such as Tasks and Pipelines (which then act as blueprints for other entities, such as Task Runs and Pipeline Runs, respectively), can be created as custom resources.# Definitions

## CI/CD Tools and Architecture
- Continuous Release Definitions (CRDs) built on top of resources native to the platform. Software with this type of architecture may be used by other CI/CD pipeline software offerings to facilitate faster defining of pipelines.
- Some CI/CD tools integrate with code repositories to scan/inspect application code. These types of tools have an association with code repositories for each application and for each environment. When changes in application modules, infrastructure, or configuration are made, they are stored in these code repositories. The CI/CD pipeline software connected to the code repositories through webhooks or some other means is activated on commits (push workflow model) or through pull requests from these repositories.
- Some CI/CD tools perform Continuous Deployment (CD) functions alone for the native platform (e.g., Jenkins X for Kubernetes platform) or for multiple technology stacks (e.g., Spinnaker for multi-cloud deployment). The difficulty with some in this class of tools is that they may lack native tools for completing the Continuous Integration (CI) functions (e.g., tools to test code, build application images, or push them to the registry).

## Preparing and Executing the CI/CD Pipeline
The purpose of creating the CI/CD pipeline is to enable frequent updates to source code, rebuilds, and the automatic deployment of updated modules into the production environment. The key tasks involved are:

### Preparatory Tasks
1. Ensure that all individual components in the DevSecOps platform (Pipeline software, SDLC software, Code Repositories, Observability tools, etc.) are available.
2. Ensure that these components are secure either through certification, validation, or customized testing.
3. Integrate CI & CD tools with SDLC tools – Access tokens, Calling scripts, pipeline definitions.
4. Set up configuration details in Infrastructure as Code (IaC) tool (with GitOps) based on deployment environment (i.e., application hosting platform in-premise or in the cloud).
5. Integrate the runtime tools to the deployment environment.
6. Design the dashboard and define the events to monitor, the alerts to be generated, and the application state variables (e.g., Memory utilization, etc.) to monitor through connection to tools such as log aggregators, metric generators, and trace generators.

### Execution Tasks
- **Setting up the source code repository:** Set up a repository (e.g., GitHub or GitLab) for storing application source code with proper version control.
- **Build process:** Configure and execute the build process for generating the executables (for those portions of the code that need to be updated) using an automated code build tool.
- **Securing the process:** Ensure that the build is free of static and dynamic vulnerabilities through unit testing with Static Application Security Testing (SAST) and Dynamic Application Security Testing (DAST) tools. This and the above tasks are activated by the CI tool.# CI/CD Pipeline Overview

## Deployment Environment
- Describing the deployment environment: This may involve describing (using the IaC) the physical/virtual resources to deploy the application either in the cloud or in the enterprise data center.

## Delivery Pipeline
- Creating the delivery pipeline: Create a pipeline that will automatically deploy the application. This and the previous task are enabled by the CD tool.

## Testing and Execution
- Test the code and execute the pipeline: After proper testing, execute the CI tool whenever a new code appears in the repository. When the build process is successful, execute the CD tool to deploy the application into the staging/production environment.

## Runtime Monitoring
- Activate the run time tool and the dashboard to initiate run time monitoring.

## CI/CD Process Stages
To reiterate, the three primary stages of the CI/CD process are:
1. Build/Test
2. Ship/Package
3. Deploy

### Pipeline Features
The following features transform this into a pipeline:
- When an update is made to the source code for a service, the code changes pushed to the source code repository trigger the code building tool.
- The code development environment or a code building tool (such as an IDE) is often integrated with security testing tools (e.g., static vulnerability analysis tool) to facilitate the generation of secure compiled code artifacts, thus integrating security into the CI pipeline.
- The generation of compiled code artifacts in code building tools triggers the shipping/package tool, which may be integrated with its own set of tools (e.g., dynamic vulnerability analysis, dynamic penetration testing tools, software composition analysis tools for identifying vulnerabilities in the attached libraries) and also creates the configuration parameters relevant to the deployment environment.
- The output of the shipping/packaging tool is then automatically fed to the CD tool, which deploys the package into the desired environment (e.g., staging, production).

## Human Element in CI/CD
The workflow of the CI/CD pipeline should not create the impression that there is no human element involved. The following teams/role players contribute to the CI/CD pipeline:

### Development Team
- Members of this team declare third-party off-the-shelf software (OSS) dependencies for their application, review recommendations from the DevSecOps system around vulnerable dependencies, update them as suggested, and write adequate test cases to ensure all functional verifications (to eliminate runtime bugs).

### Chief Information Security Officer (CISO)
- In consultation with the security team, the CISO defines the overall scope (depth and breadth) of the DevSecOps system so that it can be configured appropriately to meet the mission-critical needs of the applications.

### Security Team
- Members of this team create pipelines following best practices, including tasks to perform all required security functions (e.g., SBOM generation, vulnerability scanning, code building, code signing, introduction of new testing tools, conducting audits, etc.). Specifically, in some instances, members of the security team may be responsible for designing, building, and maintaining Policy as Code and the associated pipeline.# Infrastructure and Team Roles

## Infrastructure Team
Members of this team create, maintain, and upgrade the infrastructure.

## QA Team
Members of this team develop integration test cases.

## Deployment/Release Team
Members of this team create pipelines and packages for various environments (UAT/PreProd/Prod) and perform the configuration and provisioning appropriate for these environments.

## Activities Performed by Teams
Some of the many activities performed by these teams include the customization, update, and enhancement of the tools employed in the CI/CD pipeline (e.g., updating the static vulnerability analysis tool with the latest database of known vulnerabilities). Caution should be exercised during manual operations so that they do not block pipelines.

The targets for mean time to production should be set up while also mitigating risks, such as insider threats, through the use of “merge (GitLab) or pull (GitHub) requests” and multiple approvers for those requests. This pipeline is designed, maintained, and executed by the post-release team who – in addition to monitoring functions – performs other processes, such as compliance management, backup processes, and asset tracking.

# Strategies for Automation

Compared to other models of software development, which involve a linear progression from coding to release, DevOps uses a forward process with a delivery pipeline (i.e., build/secure, ship/package, and release) and a reverse process with a feedback loop (i.e., plan and monitor) that form a recursive workflow. The role of automation in these activities is to improve this workflow.

## Benefits of Automation
Continuous integration emphasizes testing automation to ensure that the application is not broken whenever new commits are integrated into the main branch. Automation results in the following benefits:
- Generation of data regarding software static and runtime flows.
- Reduction of development and deployment times.
- Built-in security, privacy, and compliance for the architecture.

## Recommended Strategies for Automation
The following strategies are recommended for automation to facilitate better utilization of organizational resources and derive the greatest benefit in terms of an efficient, secure application environment.

### Choice of Activities to Automate
For example, the following are productive candidates for the automation of testing activities:
- Testing of modules whose functions are subject to regulatory compliance (e.g., PCI-DSS, HIPAA, Sarbanes-Oxley).
- Tasks that are repetitive with moderate to high frequency.
- Testing of modules that perform time-sequenced operations, such as message publishers and message subscribers.
- Testing of workflows (e.g., request tracing) involving transactions that span multiple services.
- Testing of services that are resource-intensive and likely to be performance bottlenecks.

After choosing the candidates for automation based on the above criteria, the usual risk analysis should be conducted.# Security Automation and DevSecOps

## Introduction
Security automation must be applied to choose a subset that provides an optimum return on investment and maximizes desirable security metrics (e.g., defense in depth). Some recommended strategies include:

- Using the cost-benefit ratio in hours saved per year to prioritize which processes to automate.
- Using key performance indicators (KPI) (e.g., mean time to identify faults or problems, rectify, or recover) as markers to refine the DevSecOps processes.
- Based on the application, applying different weights to infrastructure services (e.g., authorization and other policies enforcement, monitoring of system states to ensure secure runtime states, network resilience in terms of system availability, latency, mean time to recover from an outage, etc.) to determine the allocation of resources to DevSecOps processes.

## Requirements for Security Automation Tools in CI/CD Pipelines
The security automation tools for various functions (e.g., static vulnerability analysis, dynamic vulnerability analysis, software composition analysis) used in CI/CD pipelines need to have different interface and alerting/reporting requirements since they have to operate seamlessly depending on the pipeline stage (e.g., build, package, release) during which they are used. These requirements are:

- Security automation tools should work with integrated development environment (IDE) tools and help developers prioritize and remediate static vulnerabilities. These capabilities are needed to facilitate developer adoption and improve productivity.
- Security automation tools should be flexible to support specific workflows and provide scaling capabilities for security services.
- Tools that perform static vulnerability checks at the build phase ensure safe data flows, and those that perform dynamic vulnerability checks ensure safe application states during runtime.

It must be mentioned that security automation tools come with costs, and hence, the extent of the usage of these tools is based on risk factor analysis.

## Implementing DevSecOps Primitives for the Reference Platform
Various CI/CD pipelines are involved in the reference platform (i.e., microservices-based application with service mesh that provides infrastructure services). Though the reference application is a microservices-based application, the DevSecOps primitives can be applied to monolithic applications as well as applications that are both on-premises and cloud-based (e.g., hybrid cloud, single public cloud, and multi-cloud).

In section 2.1, we referred to the five code types in our reference application environment. We also mentioned that separate CI/CD pipelines can be created for each of these five code types as well. The location of these five code types within the reference platform components will be discussed followed by separate sections that will describe the associated CI/CD pipelines.# CI/CD Pipelines Overview

## 4.1 CI/CD Pipeline Types
1. CI/CD pipeline for application code and application services code (Section 4.2)
2. CI/CD pipeline for infrastructure as code (IaC) (Section 4.3)
3. CI/CD pipeline for policy as code (Section 4.4)
4. CI/CD pipeline for observability as code (Section 4.5)

## 4.2 Implementation Issues
Implementation issues for all CI/CD pipelines irrespective of code types will be addressed in the following sections:
- Securing the CI/CD pipelines (Section 4.6)
- Workflow models in the CI/CD pipelines (Section 4.7)
- Security testing in the CI/CD pipelines (Section 4.8)

## 4.3 Benefits of DevSecOps
This section will also consider the overall benefits of DevSecOps with a subsection on specific advantages in the context of the reference platform and the ability to leverage DevSecOps for continuous authorization to operate (C-ATO) in Sections 4.9 and 4.10, respectively.

## 4.4 Description of Code Types and Reference Platform Components
A brief description of the five types of codes stated above (i.e., application, application services, infrastructure, policy, and monitoring) is as follows:

### Application Code and Application Services Code
- The former contains the data and application logic for a specific set of business transactions, while the latter contains code for all services, such as network connections, load balancing, and network resilience.

### Infrastructure as Code (IaC)
- The code for provisioning and configuring infrastructure resources which host application deployment in a repeatable and consistent manner. This code is written in a declarative language and – when executed – provisions, deprovisions, and configures the infrastructure for the application that is being deployed. This type of code is like any other code found in an application’s microservice except that it provides an infrastructure service (e.g., provisioning a server) rather than a transaction service (e.g., payment processing for an online retail application).

### Policy as Code
- This describes many policies, including security policies, as executable modules. One example is the authorization policy, the code for which contains verbs or artifacts specific to the policy (e.g., allow, deny, etc.) and to the domain where it applies (e.g., REST API with verbs such as method [GET, PUT, etc.], path, etc.). This code can be written in a special-purpose policy language, such as Rego, or languages used in regular applications, such as Go. This code may have some overlap with the configuration code of IaC. However, for implementing policies associated with critical security services that are specific to the application domain, a separate policy as code that resides in the policy enforcement points (PEPs) of the reference platform is required.

### Observability as Code
- The ability to infer a system’s internal state and provide actionable insights into when and, more importantly, why errors occur within a system. It is a full-stack observability that includes monitoring and analytics and that offers key insights into the overall performance of applications and the systems hosting them.# Observability as Code in Microservices Applications

In the context of the reference platform, observability as code is the portion of the code that creates agencies in proxies and creates functionality for gathering three types of data (i.e., logs, traces, and telemetry) from microservices applications. This type of code also supplies or transfers data to external tools (e.g., log aggregation tools that aggregate log data from individual microservices, provide analysis of tracing data for bottleneck services, generate metrics that reflect the application health from telemetry data, etc.).

## Functions Enabled by Observability as Code

Brief descriptions of the three functions enabled by observability as code are:

1. **Logging**: Captures detailed error messages, as well as debug logs and stack traces for troubleshooting.
2. **Tracing**: Follows application requests as they wind through multiple microservices to complete a transaction in order to identify an issue or performance bottleneck in a distributed or microservices-based ecosystem.
3. **Monitoring (Metrics)**: Gathers telemetry data from applications and services.

Each of the code types has an associated CI/CD pipeline and is described in Sections 4.2 through 4.5. There may be overlaps among application service code, infrastructure as code, policy as code, and observability as code types.

## Components of the Reference Platform

The constituent components of the reference platform hosting the five code types are:

1. **Business Function Component**: Consisting of several microservices modules, each often implemented as a container, which embody the application logic (e.g., interacting with data, performing transactions, etc.), thus forming the application code.
2. **Infrastructure Component**: Containing computer, networking, and storage resources whose constituents can be provisioned using infrastructure as code.
3. **Service Mesh Component**: Implemented through a combination of control plane modules and service proxies, which provides application services, enforces policies (e.g., authentication and authorization), and contains application services code and policy as code.
4. **Monitoring Component**: Modules involved in ascertaining the parameters that indicate the health of the application, which perform functions (e.g., log aggregation, the generation of metrics, the generation of displays for dashboards, etc.) and contain observability as code.

## Distribution of Policy and Observability Code Types

The distribution of policy and observability code types within the components of the service mesh are as follows:

- **Proxies (ingress, sidecar, and egress)**: These house encoding policies related to session establishment, routing, authentication, and authorization functions.
- **Control Plane of the Service Mesh**: This houses code for relaying telemetry information from services captured and sent by proxies to specialized monitoring tools, authentication, and more.# Certificate Generation and Maintenance

## Overview
- **Certificate Generation and Maintenance**: This involves updating policies in the proxies and monitoring overall configuration in the service orchestration platform for generating new proxies and deleting obsolete proxies associated with discontinued microservices.

## External Modules
- **Functionality**: These modules perform specialized functions at the application and enterprise levels, such as:
- Centralized authorization or entitlement server
- Centralized logger
- Monitoring/alerting server status through dashboards
- **Integration**: These modules are called by code from the proxies or the control plane, providing a comprehensive view of the application status.

# CI/CD Pipeline for Application Code and Application Services Code

## Application Code Management
- **Location**: Application code and application services code reside in the container orchestration and resource management platform.
- **CI/CD Software**: The CI/CD software that implements the workflows associated with it usually resides in the same platform.
- **Security Measures**:
- The pipeline should be protected using the steps outlined in Section 4.6.
- The application code under the control of this pipeline should be subject to the security testing described in Section 4.8.

## Runtime Security
- **Protection**: The orchestration platform should be protected using a runtime security tool (e.g., Falco) that can:
- Read OS kernel logs, container logs, and platform logs in real-time.
- Process logs against a threat detection rules engine to alert users of malicious behavior (e.g., creation of a privileged container, reading of a sensitive file by an unauthorized user).
- **Features**:
- Comes with a set of default (predefined) rules.
- Allows for the addition of custom rules.
- Spins up agents for each node in the cluster to monitor containers running in various pods.

## Advantages
- Complements existing platform’s native security measures, such as access control models and pod security policies, by detecting violations when they occur.

# CI/CD Pipeline for Infrastructure as Code

## Conventional Approach
- **Resource Allocation**: The conventional approach to allocating infrastructure for applications includes:
- Initially provisioning compute and networking resources with configuration parameters.
- Ongoing tasks such as patch management (e.g., OS and libraries).
- Establishing conformity to compliance regulations (e.g., data privacy).
- Making drift corrections when the current configuration no longer provides the intended operational state.

## Infrastructure as Code (IaC)
- **Definition**: IaC is a declarative style of code that encodes computer instructions necessary to deploy virtual infrastructure on a public cloud service or private data center via a service’s management APIs.
- **Version Control**: Infrastructures are defined in a declarative way and versioned using the same source code control tools (e.g., GitOps) used for application code.
- **Languages**: Depending on the particular IaC tool, the language can be a scripting language (e.g., JavaScript, Python, TypeScript) or a proprietary language.# Infrastructure as Code (IaC)

Infrastructure as Code (IaC) is a configuration language (e.g., HCL) that may or may not be compatible with standardized languages (e.g., JSON). The basic instructions consist of telling the system how to provision and manage infrastructure, whether that is an individual compute instance or a complete server, such as physical servers or virtual machines, containers, storage, network connections, connection topology, and load balancers.

In some cases, the infrastructure may be short-lived or ephemeral, and the lifespan of the infrastructure (whether immutable or mutable) does not warrant continued configuration management. Provisioning could be tied to individual commits of application code using tools that can connect application code and infrastructure code in a way that is logical, expressive, and familiar to development and operations teams, where application code increasingly defines the infrastructure resource requirements for a cloud application.

IaC thus involves codifying all software deployment tasks (allocation of type of servers, such as bare metal, VMs or containers, resource content of servers) and the configuration of these servers and their networks. The software containing this code type is also called a resource manager or deployment manager. In other words, IaC software automates the management of the whole IT infrastructure life cycle (provisioning and de-provisioning of resources) and enables a programmable infrastructure. The integration of this software as part of the CI/CD pipeline not only results in agile deployment and maintenance but also in a robust application platform that is secure and meets performance needs.

## 4.3.1 Protection for IaC

When infrastructure is code as in IaC, it can include bugs and oversights that can potentially become vulnerabilities and, therefore, be exploited just as in application code. Thus, protecting the IaC is protecting the infrastructure definitions and eventually the deployment environment. Any piece of IaC has to be scanned for potential vulnerabilities before it enters the GitOps and is merged.

In addition, the assurance of a secure application platform can be obtained only if there is a methodical drift management process in place. This assurance can be obtained only if the architecture defined in IaC is what actually exists in the deployed environment since this equivalence could be altered by an inadvertent or intentional change made through a console or CLI – thus bypassing the IaC. Ensuring this equivalence has to be done immediately after deployment and periodically during runtime as any change to the architecture could result in the introduction of security design flaws and may require making changes to IaC.

## 4.3.2 Distinction Between Configuration and Infrastructure

Infrastructure is often confused with configuration, which maintains computer systems, software, dependencies, and settings in a desired, consistent state. For example, putting a newly purchased server onto a rack and connecting it to the switches so that it is...# Infrastructure and Configuration Management

Connected to the existing networks (or launching a new virtual machine and assigning network interfaces to it) belongs to the definition of “infrastructure.” In contrast, after the server is launched, installing an HTTPS server and configuring it belongs to configuration management.

## 4.4 CI/CD Pipeline for Policy as Code

Policy as code involves codifying all policies and running them as part of the CI/CD pipeline so that they become an integral part of the application runtime. Examples of policy categories include authorization policies, networking policies, and implementation artifact policies (e.g., container policies).

Policy management capabilities in a typical “policy as code software” may come with a set of predefined policy categories and policies and also support the definition of new policy categories and associated policies by providing policy templates. The due diligence required for policy as code is that it should provide protection against all known threats that are relevant for the application environment including the infrastructure, and this can be ensured only if that code is periodically scanned and updated with appropriate changes to counter the threats relevant to the application class (e.g., web application) and the hosting infrastructure.

Some examples of policy categories and associated policies are given in Table 1 below.

The policies defined in the “policy as code software” may translate into the following in the application infrastructure runtime configuration parameters:

- Policy-enforcing executable (e.g., WASM in service proxies)
- Triggers for calling an external policy decision module (e.g., calling an external authorization server for an allow/deny decision based on the evaluation of access control policies relevant to the current access request)
- It may also impact the IaC to ensure that appropriate resources are provisioned in the deployment environment to enforce security, privacy, and compliance requirements.

## 4.5 CI/CD Pipeline for Observability as Code

Observability as code deploys a monitoring agent in each of the application’s service components to collect the three types of data (described in Section 4.1), send them to specialized tools that correlate them, perform analysis, and display the analyzed consolidated data on dashboards to present an overall application-level picture. An example of such consolidated data are log patterns, which provide a view of log data that is presented after the log data are filtered using some criterion (e.g., a service or an event).

The data are grouped into clusters based on common patterns (e.g., based on timestamp or range of IP addresses) for easy interpretation. Unusual occurrences are identified, and those findings can then be used to steer and accelerate further investigation.

## 4.6 Securing the CI/CD Pipeline

There are some common implementation issues to be addressed for CI/CD pipelines irrespective of the specific context.# Securing the CI/CD Pipeline

Securing the processes involves the assignment of roles for operating the build tasks. Automation tools (e.g., Git Secrets) are available for this purpose. The following security tasks should be considered as a minimum for securing the CI/CD pipeline:

- Harden servers hosting code & artifact repositories
- Secure the credentials used for accessing repositories such as authorization tokens and for generating pull requests
- Implement controls on who can check-in and check-out in container image registries since they are the storage for artifacts produced by the CI pipeline and serve as bridges between CI and CD pipelines
- Log all code and build update activities
- If a build or test fails in the CI pipeline, send build reports to developers and stop further pipeline tasks. The code repositories should be configured to automatically block all pull requests from the CD pipeline.
- If an audit fails, send build reports to the security team and stop further pipeline tasks
- Ensure that developers can only access the application code and not any of the five pipeline code types
- During the build and release process, sign the release artifact during each required CI/CD stage (preferably multi-party signing)
- During production release, verify that all required signatures (generated with multiple phase keys) are present to ensure that no one bypasses the pipeline.

## Workflow Models in CI/CD Pipelines

The next common issue involves workflow models. All CI/CD pipelines can have two types of workflow models, which depend on the automated tools that are deployed as part of the pipeline:

1. **Push-based model**
2. **Pull-based model**

In the CI/CD tools that support the push-based model, changes made in one stage or phase of the pipeline trigger changes in the subsequent stages and phases. For example, through a series of encoded scripts, the new builds in the CI system trigger changes to the CD portion of the pipeline and thus change the deployment infrastructure (e.g., Kubernetes cluster). The security downside of using the CI system as the basis for change in deployments is the possibility of exposing credentials outside of the deployment environment in spite of best efforts to secure the CI scripts, which operate outside of the trusted domain of the deployment infrastructure. Since CD tools have the keys to production systems, push-based models are rendered insecure.

In a pull-based workflow model, an operator that pertains to the deployment environment (e.g., Kubernetes Operator, Flux, ArgoCD) pulls new images from inside of the environment as soon as the operator observes that a new image has been pushed to the registry. The new image is pulled from the registry, the deployment manifest is automatically updated, and the new image is deployed in the environment (e.g., cluster). Thus, the convergence of the actual deployment infrastructure state with the state declaratively described in the Git repository is achieved.# Deployment Repository and GitOps Workflow

## Overview
The deployment repository is crucial for achieving a secure deployment environment. It ensures that credentials (e.g., cluster credentials) are not exposed outside of the production environment. A pull-based model, which typically uses a GitOps repository for storing the source code and builds, is highly recommended.

## 4.7.1 GitOps Workflow Model for CI/CD – A Pull-based Model
The GitOps workflow model enhances the CI/CD pipeline, particularly in the delivery portion, by utilizing a pull-based workflow instead of the traditional push-based model supported by many CI/CD tools.

### CI Portion of the Pipeline
The CI portion remains unchanged, as the CI engine (e.g., Jenkins, GitLab CI) is still responsible for:
- Creating builds for the changed code
- Conducting regression testing
- Integrating/merging with the main source code in the relevant repositories

However, it does not trigger continuous delivery (i.e., push updates directly) in the pipeline.

### Deployment Management
A separate GitOps operator manages the deployment based on updates to the main (trunk) code.

#### Operators
An operator (e.g., Flux, ArgoCD) is an actor managed by an orchestration platform and can inherit the cluster’s configuration, security, and availability. The use of this actor enhances security because:
- An agent residing within the cluster listens for updates to all code and image repositories it is authorized to access.
- It pulls images and configuration updates into the cluster.

### Security Features of the Pull Approach
The pull approach used by the agent has the following security features:
- **Authorization Policies**: Only carry out operations permitted by authorization policies defined in the orchestration platform; trust is shared with the cluster and not managed separately.
- **Native Binding**: Bind natively to all orchestration platform objects and know whether operations have completed or need to be retried.

## 4.8 Security Testing – Common Requirement for CI/CD Pipelines for All Code Types
Security testing is a common requirement across all code types (e.g., application service, Infrastructure as Code (IaC), Policy as Code (PaC), or observability). The CI/CD pipelines of DevSecOps for microservices-based infrastructure with service mesh should include Application Security Testing (AST) enabled by either automated tools or offered as a service.

### AST Technologies
According to Gartner, there are four main AST technologies:

1. **Static AST (SAST) Tools**:
- Analyze an application’s source, bytecode, or binary code for security vulnerabilities, typically during the programming and/or testing software life cycle (SLC) phases.
- This technology involves techniques that examine the application in a commit and analyze its dependencies. If any dependencies contain issues or known security vulnerabilities, the commit will be marked as insecure and will not be allowed to proceed to deployment. This can also include identifying hardcoded passwords/secrets in code that should be removed.

2. **Dynamic AST (DAST) Tools**:
- Analyze applications in their dynamic, running state.

(Additional AST technologies may follow in the original document.)# Application Security Testing (AST) Tools

## Overview
Application Security Testing (AST) tools are essential for identifying vulnerabilities in applications during testing or operational phases. They simulate attacks against web-enabled applications, services, and APIs, analyze the application's reactions, and determine whether it is vulnerable.

## Types of AST Tools

### 1. Dynamic Application Security Testing (DAST)
DAST tools go beyond Static Application Security Testing (SAST) by spinning up a copy of the production environment inside the Continuous Integration (CI) job to scan the resulting containers and executables. This dynamic aspect helps catch dependencies that are loaded at launch time, which SAST may not detect.

### 2. Interactive Application Security Testing (IAST)
IAST tools combine elements of DAST with the instrumentation of the application under test. They are typically implemented as an agent within the test runtime environment (e.g., instrumenting the Java Virtual Machine [JVM] or .NET CLR) that observes operations or identifies and attacks vulnerabilities.

### 3. Software Composition Analysis (SCA)
SCA tools are used to identify open-source and third-party components in use in an application, their known security vulnerabilities, and typically adversarial license restrictions.

## Functional and Coverage Requirements for AST Tools
The overall metrics that testing tools, including AST tools, should satisfy include:

- Increase the quality of application releases by identifying security, privacy, and compliance gaps.
- Integrate with the tools that developers are already using.
- Minimize the number of test tools while providing necessary coverage risk.
- Ensure lower-level unit tests at the API and microservices level have sufficient visibility to determine coverage.
- Include higher-level UI/UX and system tests.
- Have deep code analysis capabilities to detect runtime flaws.
- Increase the speed of releases.
- Be cost-efficient.

### Specific Functional Requirements for AST Tools
AST tools should perform the following types of scans:

- **Vulnerability Scans**: Probe applications for security weaknesses that could expose them to attacks.
- **Container Image Scans**: Analyze the contents and build process of a container image to detect security issues, vulnerabilities, or deficient practices (e.g., hardcoded passwords/secrets).
- **Regulatory/Compliance Scans**: Assess adherence to specific compliance requirements.

Vulnerability scans should be performed whenever the code in the source code repository is revised to ensure that the current revision does not contain any vulnerable dependencies.

## Desirable Features of AST Tools
The desirable features of AST tools and/or services, along with techniques for behavioral analysis, include:

- Analyze source, byte, or binary code.
- Observe the behavior of applications to identify coding, design, packaging, deployment, and runtime conditions that introduce security vulnerabilities.# Scanning Application Code for Security Vulnerabilities and Misconfiguration

Scanning application code for security vulnerabilities and misconfiguration as part of CI/CD pipeline tasks should involve the following artifacts:

## Key Artifacts to Scan

- **Container Images**: Should be scanned for vulnerabilities.
- **Container File System**: After the container is built from a base image (that is scanned as stated above), the container’s file system should be scanned for both vulnerabilities and misconfigurations.
- **Git Repositories**: Containing application source code should be scanned for both vulnerabilities and misconfigurations.

Container images include OS packages (e.g., Alpine, UBI, RHEL, CentOS, etc.) and language-specific packages (e.g., Bundler, Composer, npm, yarn, etc.).

## Importance of Scanning Infrastructure as Code

Scanning infrastructure as code for security vulnerabilities reduces the operations workload by preventing those vulnerabilities from making it to production. However, it cannot replace checking for runtime security since the risk of drift will always exist.

The reasons for all post-deployment (runtime) changes to the architecture (due to drift) must be analyzed and addressed by pushing appropriate updates to the IaC so that it becomes part of the pipeline and does not reoccur in subsequent deployments. This approach facilitates the use of runtime checks for remediating security design flaws.

## Locations of Infrastructure-as-Code Files

The infrastructure-as-code files can be found in the following:

- **Container Orchestration Platform**: To facilitate deployments (e.g., Kubernetes YAML infrastructure-as-code files).
- **CI/CD Pipeline Software**: Dedicated infrastructure-as-code files (e.g., HashiCorp Terraform infrastructure-as-code files, AWS CloudFormation infrastructure-as-code files).

Application services code, policy-as-code, and observability-as-code files can be found in the data plane and control plane components of the dedicated application services component (e.g., service mesh) and should be scanned for both security vulnerabilities (e.g., information leakage in authorization policies) and misconfiguration.

## Benefits of DevSecOps Primitives to Application Security in the Service Mesh

The benefits of DevSecOps include:

- **Improved Communication and Collaboration**: Better communication and collaboration between various IT teams, especially between developers, operations, and security teams, and other stakeholders. This results in better productivity.
- **Streamlined Processes**: Streamlined software development, delivery, and deployment processes lead to less downtime, faster time to market, and lower infrastructure & operational costs due to automation.
- **Reduction of Attack Surfaces**: Implementing zero trust reduces attack surfaces, restricts lateral movement, and prevents attack escalation. This is further facilitated by continuous monitoring with modern behavior prevention capabilities.
- **Enhanced Security**: Better security through validation of every request, monitoring, alerts, and feedback mechanisms because of Observability as code. Specific capabilities include:
- **Runtime**: Killing a malicious container.
- **Feedback**: Providing feedback to the right repository due to an errant program to update code.# Security and Monitoring in DevSecOps

## Overview
This document outlines the processes and mechanisms involved in monitoring services, enabling security assertions, and validating requests within a DevSecOps framework.

## Service Monitoring
1. **Retriggering Pipelines**
- Ensure that pipelines are retriggered as necessary.

2. **Monitoring Services**
- Monitor new and terminated services.
- Adjust associated services, such as service proxies.

## Security Assertions
- **Enable Security Assertions:**
- Non-bypassable policies enforced by proxies executing in the same space.
- Secure sessions, robust authentication & authorization, and secure state transitions.

- **Continuous Authorization to Operate (C-ATO)**
- Detailed description provided at the end of this section.

## Request Validation
### Validation of Every Request
- Every request from a user or client application (service) is authenticated and authorized.
- Mechanisms include:
- Open Policy Agent (OPA) or any external authorization engine.
- Admission controllers that are integral parts of the platform.

### Role of Admission Controllers
- **Authorization Engines:**
- Provide application domain-specific policy enforcement.

- **Admission Controllers:**
- Provide platform-specific policy relating to endpoint objects (e.g., pods, deployments, namespaces).
- **Mutating Admission Controllers:**
- Parse each request and make changes (mutate) before forwarding it.
- Examples:
- Setting default values for unspecified specifications.
- Adding resource limits for pods if not set by the user.

- **Validating Admission Controllers:**
- Reject requests that do not adhere to specifications.
- Example: Pod requests cannot have a security context set to run as the root user.

## Feedback Mechanisms
- **Remediation Processes:**
- Automatically open issues against the appropriate code repository to address problems.
- Retrigger the DevSecOps pipeline as needed.

- **Feedback Loops:**
- Notify the application-hosting platform to take action (e.g., kill a pod with a malicious container).

- **Proactive Dynamic Security:**
- Monitor application configuration.
- Generate and inject proxies for secure communication needs of new pods/containers.

- **Security Assertions:**
- Ensure non-bypassable policies are enforced under all usage scenarios.
- Differentiate between trusted and untrusted portions of application code.
- Prevent credential and privilege leaks.# Communication Paths and Secure State Transitions

- Enable assertions regarding performance parameters (e.g., network resilience parameters, such as continued operation under failures, redundancy, and recoverability features).
- Overall, faster incorporation of feedback results in quicker software improvements.

## 4.10 Leveraging DevSecOps for Continuous Authorization to Operate (C-ATO)

In the reference platform, the runtime status or execution state of the entire application system is due to a combination of executions of:

- **Infrastructure Code**: e.g., networking routes for interservice communication, resources provisioning code.
- **Policy Code**: e.g., code that specifies authentication and authorization policies.
- **Session Management Code**: e.g., code that establishes an mTLS session, code that generates JWT tokens.

This is revealed by the execution of observability as code. The observability as code of the service mesh relays the output from the execution of infrastructure, policy, and session management codes during runtime to various monitoring tools that generate applicable metrics, log aggregation tools, and tracing tools. These tools, in turn, relay their output to a centralized dashboard. The analytics that are integral to the output of these tools enable system administrators to obtain a comprehensive global view of the runtime status of the entire application system.

It is the runtime performance of a DevSecOps platform enabled through continuous monitoring with zero trust design features that provides all of the necessary security assurance for cloud-native applications.

### Activities in the DevSecOps Pipelines that Enable Continuous ATO

- **Checking for Compliant Code**: The following codes can be checked for compliance with a Risk Management Framework:
- (a) **IaC** – Generate network routes, Resource provisioning
- (b) **Policy as Code** – Encodes AuthN and AuthZ policies
- (c) **Session Management Code** – mTLS session, JWT tokens
- (d) **Observability Code**

Specific risk assessment features include the capability to generate actionable tasks, specify code-level guidance, and test plans for verifying compliance. In addition, risk assessment tools can provide complete traceability for all of the artifacts displayed in the dashboard, as well as the reporting capabilities needed for continuous ATO.

- **Dashboard that Displays Runtime Status**: Provides alerts and feedback necessary to fix security and performance bottleneck issues (that impact availability) by the process that triggers new pipelines. In addition, dashboard generation tools have features that enable system administrators to analyze macro-level features, as well as dynamically change the composition of the artifacts to be displayed based on the evolving system and consumer needs of the environment in which the application operates.

## 5. Summary and Conclusion

This document provides comprehensive guidance for the implementation of DevSecOps primitives for a reference platform hosting a cloud-native application. It includes an overview of the reference platform and describes the basic DevSecOps primitives (i.e., CI/CD pipelines).# Building Blocks of CI/CD Pipelines

## Overview
The design and execution of the pipelines, along with the role of automation, are crucial for the efficient execution of workflows in CI/CD pipelines.

## Architecture of the Reference Platform
The architecture of the reference platform includes:

- **Application Code**: The core code that defines the application.
- **Application Services Code**: Code that provides necessary services for the application.

### Functional Elements
The architecture consists of several functional elements:

- **Infrastructure**: The underlying hardware and software resources.
- **Runtime Policies**: Guidelines that govern the execution of the application.
- **Continuous Monitoring**: Ongoing assessment of the application's health.

These elements can be deployed through declarative codes with separate CI/CD pipeline types.

## Runtime Behaviors
The runtime behaviors of these codes are essential for understanding:

- **Benefits of Implementation**: High-assurance security measures.
- **Use of Artifacts**: Artifacts within the pipelines are utilized to provide a continuous authority to operate (C-ATO).

## Risk Management
The implementation also involves:

- **Risk Management Tools**: Tools that help in assessing and managing risks.
- **Dashboard Metrics**: Metrics that provide insights into the performance and health of the application.