# Abstract
Security is an essential component of high-performance computing (HPC). HPC systems often differ based on the evolution of their system designs, the applications they run, and the missions they support. An HPC system may also have its own unique security requirements, follow different security guidance, and require tailored security solutions. Their complexity and uniqueness impede the sharing of security solutions and knowledge. This NIST Special Publication aims to standardize and facilitate the information and knowledge-sharing of HPC security using an HPC system reference architecture and key components as the basis of an HPC system lexicon. This publication also analyzes HPC threats, considers current HPC security postures and challenges, and makes best-practice recommendations.

## 1. Introduction
In 2015, Executive Order 13702 established the National Strategic Computing Initiative (NSCI) to maximize the benefits of high-performance computing (HPC) for economic competitiveness and scientific discovery. The ability to process large volumes of data and perform complex calculations at high speeds is a key part of the Nation’s vision for maintaining its global competitive edge.

Security is essential to achieving the anticipated benefits of HPC. HPC systems bear some resemblance to enterprise IT computing, which allows for the effective application of traditional IT security solutions. However, they also have significant differences. An HPC system is designed to maximize performance, so its architecture, hardware components, software stacks, and working environment are very different from enterprise IT. Additionally, most HPC systems serve multiple projects and user communities, leading to more complex security concerns than those encountered in enterprise systems. As such, security solutions must be tailored to the HPC system’s requirements.

Furthermore, HPC systems are often different from one another due to the evolution of their system designs, the applications they run, and the missions they support. An HPC system frequently has its own unique method of applying security requirements and may follow different security guidance, which can impede the sharing of security solutions and knowledge.

This NIST Special Publication aims to standardize and facilitate the sharing of HPC security information and knowledge through the development of an HPC system reference architecture and key components, which are introduced as the basics of the HPC system lexicon. The reference architecture divides an HPC system into four function zones. A zone-based HPC reference architecture captures the most common features across the majority of HPC systems and segues into HPC system threat analysis. Key HPC security characteristics and use requirements are laid out alongside the major threats faced by the system and individual users.# HPC Security Postures and Architecture

## Introduction
This publication serves as a conceptual guide to high-performance computing (HPC) security postures, challenges, recommendations, and currently recognized variants. It is not intended to be a checklist of requirements. Emerging technologies, such as novel networking technology, data storage solutions, and hardware acceleration technology, have the potential to influence the HPC reference architecture and security posture. Consequently, this document will be updated as needed.

## 1. HPC System Reference Architecture and Main Components
The HPC system is complex and evolving. A common lexicon can help describe and identify an HPC system’s architecture, critical elements, security threats, and potential risks. An HPC system is divided into four functional zones:

### 1.1 High-Performance Computing Zone
The high-performance computing zone consists of a pool of compute nodes connected by one or more high-speed networks. This zone provides key services specifically designed to run parallel jobs at scale.

### 1.2 Data Storage Zone
The data storage zone comprises one or multiple high-speed parallel file systems that provide data storage services for user data. These file systems are designed to store very large data sets and provide fast access to data for reading and writing.

### 1.3 Access Zone
The access zone has one or more nodes connected to external networks, such as the broader organizational network or the internet. This zone provides the means for authenticating and authorizing the access and connections of users and administrators. It offers various services, including interactive shells, web-based portals, data transfer, and data visualization.

### 1.4 Management Zone
The management zone comprises multiple management nodes and/or cloud service clusters through which HPC management services are provided. This zone allows HPC system administrators to configure and manage the HPC system, including the configuration of compute nodes, storage, networks, provisioning, identity management, auditing, system monitoring, and vulnerability assessment. Various management software modules (e.g., job schedulers, workflow management, and the Domain Name System [DNS]) run in the management zone.

## 2. Main Components

### 2.1 Components of the High-Performance Computing Zone
An HPC cluster consists of a collection of independent computing systems, called compute nodes, which are interconnected via high-speed networks. Compute nodes have the same components as a laptop, desktop, or server, including central processing units (CPUs), memory, disk space, and networking interface cards. However, they are architecturally tuned for the requirements of HPC workloads.

In some HPC architectures, a compute node may not have local disks and instead use the data storage services of remote storage servers. Additionally, there may be different types of nodes for different types of tasks, and some compute nodes are equipped with hardware accelerators to speed up specific applications.# High-Performance Computing (HPC) Overview

## 1. Introduction to HPC Compute Nodes
HPC compute nodes often utilize graphics processing units (GPUs) to accelerate modeling and simulation or AI and machine learning (ML) model training. An HPC compute node has its own software stack installed (e.g., operating system [OS], compilers, software libraries, etc.) to support applications. The installation and configuration of the software stacks are cluster-wide, centrally managed, and controlled by the management zone. The number of compute nodes in an HPC system ranges from a few nodes to hundreds and even tens of thousands of nodes.

## 2. Networking in HPC
A frequent requirement of HPC networking, which interconnects computer nodes and file systems, is to have high throughput and low latency so that the compute nodes and parallel file system (PFS) in the data storage zone can work as one supercomputer. The exact requirements for bandwidth and latency are dictated by the intended workload. HPC networking often employs specifically designed protocols, networking cards, processor nodes, and switches to optimize network performance. The popular HPC interconnect networking includes:
- InfiniBand
- Omni-Path
- Slingshot
- Ethernet
- Others

A high-performance computing zone typically utilizes non-high-performance communication networks, like Ethernet, as cluster internal networks that connect the high-performance computing zone with the management zone and access zone for traffic associated with maintenance activities as opposed to HPC traffic.

## 3. Components of the Data Storage Zone
Several different classes of storage systems may be present inside the data storage zone. In general, storage systems within this zone cannot be effectively separated from the HPC resources that they support from an administrative privilege perspective. Typical classes of storage found within this zone include:
- Parallel File Systems (PFSs)
- Archival file systems that support campaign storage and protect against data loss

HPC systems may have other file systems that store non-user data. For instance, the management zone often has its own file system that stores the OS images and configuration files. In that case, the file system is included in the corresponding function zone.

HPC applications’ initial data, intermediate results, and results are stored in the data storage zone and can be accessed during the application runtime and after the application’s completion. External HPC users can also access user data through the login nodes and/or data transfer nodes in the access zone.

The storage capacity of these file systems is often measured in petabytes and can reach up to exabytes. File systems within the data storage zone will generally use a transport mechanism appropriate to the tier. For example, high-bandwidth file systems may be attached to the HPC resource’s high-performance network, while lower bandwidth file systems may use 10 Gbps or 100 Gbps Ethernet. Access control for most HPC file systems is enforced by the appropriate mechanisms.# Operating System Software and File Systems

## 2.1.3. Parallel File System

Since HPC workloads can vary significantly, a Parallel File System (PFS) is often required to support read-intensive and write-intensive applications with sequential and random-access patterns at speeds of up to terabytes per second. Commonly seen file systems include Lustre, GPFS, and IBM Spectrum Scale.

During procurement, a PFS will typically be designed to hit a particular aggregate bandwidth target rather than a capacity requirement. These PFSs will typically consist of a cluster of systems to maintain metadata about files and locations, as well as servers that act as storage targets. Clients that mount the file system will typically load the file system client software via a kernel module. Storage target servers will have backing storage arrays configured with dozens of disks in a redundant array of inexpensive disk (RAID) strategy.

Both GPFS and Lustre-based PFSs are prone to performance degradation when a certain capacity threshold is reached. These file systems may be regularly pruned of unwanted files with a strategy decided by the file system administrators. Some deployments will delete files older than a certain age, which forces HPC users to transfer job output to a longer-term file system, such as campaign storage.

PFSs tend to be somewhat unreliable, depending on the types of activities being performed by running jobs and users. Because these are distributed file systems, file-system software must solve distributed locking of files to ensure deterministic file updates when multiple clients are writing to the same file at once. Additionally, PFSs are susceptible to denial-of-service conditions even during legitimate user operations, such as listing a directory with millions of files or applications that perform poor file locking semantics.

## 2.1.4. Archival and Campaign Storage

The term “campaign” is understood here as a collection of coordinated projects that are working toward a common set of goals and deliverables. Archival and campaign storage systems represent a class of storage that is more resilient to failure conditions than a PFS and is often less expensive per gigabyte.

These advantages come at the cost of bandwidth and an increased latency of data transfer. While a PFS acts as a temporary short-term scratch file system, campaign storage supports the long-term storage needs of a project over its life cycle. Finished data products that support scientific publications or other high-value datasets may also be stored in an archival file system. The retention time for data on a...# Campaign Storage File

## 1. Overview
The system is measured in years, while the retention of data within an archival storage system is measured in decades. Both campaign and archival storage systems might employ low-latency disks — such as solid-state drives (SSD) or non-volatile memory express (NVMe) drives within a small tier of storage that acts as a cache — and are backed by cheaper, higher capacity media, such as spinning disks and/or tape media.

## 2. Burst Buffer
Burst buffer commonly refers to a caching mechanism for storage systems. For applications that require extremely low latency or high-bandwidth memory-to-disk data transfer during runtime, intermediate storage layers that contain “burst buffers” have been incorporated as brokers to primarily mitigate the effects of input/output (I/O) contention and the bandwidth burden on parallel file systems (PFSs).

These burst buffers can pre-fetch data from the PFS before a computing job begins and stage data out to a parallel file system after a computing job has completed. This saves job runtime that would normally be spent performing bulk I/O to the PFS and allows it to be spent on computation instead. Typical HPC infrastructures contain the following intermediate storage architectures:

### 2.1 Node-local Burst Buffer Architectures
Each burst buffer is colocated with a corresponding HPC compute node. This is advantageous for its scalability and improves the checkpoint bandwidth because the aggregate bandwidth increases in proportion to the number of compute nodes.

### 2.2 Remote-shared Burst Buffer Architectures
Burst buffers are shared between multiple HPC compute nodes that are hosted on an I/O node. This is advantageous for facilitating the development, deployment, and maintenance of these architectures.

There are also HPCs that can contain mixed burst buffer intermediate storage architectures, which combine the strengths of node-local and remote-shared burst buffer architectures.

## 3. Components of the Access Zone
A typical HPC system provides one or more nodes through which users and administrators access the system. At least one of these nodes is a login node where users have access to shells to launch interactive or batch jobs. Some of these login nodes may also have specialized visualization hardware and software with which users can conduct interactive and/or post-execution visualization of their datasets.

There may also be one or more data transfer nodes that provide services to transfer data into and out of the HPC system and may even provide storage-mounting services, like Network File System (NFS), Common Internet File System (CIFS), Server Message Block (SMB), and Filesystem in Userspace (FUSE) based SSH Filesystem (SSHFS).

The security posture for data transfer nodes often uses the architecture of ScienceDMZ instead of a pass-through firewall device because of the performance impact that firewalls may impose. Many HPC systems now provide web portals via web portal nodes that enable a variety of web interfaces to HPC system services.# 2.1.7. Components of the Management Zone

The complexity of HPC systems requires a significant infrastructure to operate and manage it, which is collectively referred to as the management zone. The management zone may consist of servers and network switches that enable various functions for operating the system with efficiency, effectiveness, and stability.

# 2.1.8. General Architecture and Characteristics

One important characteristic of the management zone is that it has a separate security posture because non-privileged users do not need to access the management servers or services in a direct way. Privileged users responsible for configuring, maintaining, and operating the HPC system access the management zone servers and switches through extra security controls.

For example, from the public-facing login nodes, they may go through a bastion host that is typically located in the management zone, or they may establish a virtual private network (VPN) with separate authentication and authorization or other appropriate security controls to reach the management zone. All systems are configured on networks that are not routed beyond the perimeter of the HPC system so that only nodes like compute nodes and storage nodes can access the services.

The services provided by the management zone have clearly defined protocols and can be implemented as running on assigned hardware platforms or run as virtual machines (VMs) on a dedicated set of hardware resources. The fact that the management zone has a clear and separate security posture helps with risk assessment and the selection of controls to secure the management zone and manage the risk.

# 2.1.9. Basic Services

The HPC resources inside of the computing and data storage zones need various services to operate. Examples include:

- Domain Name Services (DNS)
- Dynamic Host Configuration Protocol (DHCP)
- Configuration definitions, authentication, and authorization services, such as those provided by an LDAP server
- Network Time Protocol (NTP) for synchronization, log management, version-controlled repositories.

The management zone includes storage systems to store configuration data, node images, current versions, development and test versions, and historical versions. Storing logs from the entire HPC system is also part of the management zone as well as the servers to process the logs and alert administrators of events, problems, and incidents.

Many of these services will be implemented with high availability and failover capabilities to avoid failure of the HPC resources. The network switches for the management network and the fast interconnects are often managed as part of the management zone because non-privileged users do not need direct access to those resources.

# 2.1.10. Configuration Management

Automated configuration management is crucial to ensure the stable operation of...# Complex Systems in HPC

The systems that hold the configuration database and run the server to place configurations on compute nodes, storage servers, and network switches are part of the management zone. The nodes are subject to a regularly scheduled process to verify configuration and enforce consistency with what the configuration management nodes and databases specify.

Often, the configuration management systems in the management zone have an even more restricted security posture than the management zone as a whole, with a smaller number of privileged users having access.

## 2.1.11. HPC Scheduler and Workflow Management

Because of the distributed nature of HPC systems, requesting resources for given workloads is coordinated by a scheduler or workload manager, such as Slurm or Kubernetes. These services are run on servers in the management zone alongside the configurations and job logs. Non-privileged users access the scheduler through specific commands or an application programming interface (API). Access to the service is restricted to nodes within the HPC system perimeter. There may also be a web interface that provides a separate authenticated and authorized path for scheduling workloads, often within the strict constraints of certain application domains.

## 2.1.12. HPC Software

In addition to the management software that installs, boots, configures, and manages HPC-related systems, HPC application codes rely on several layers of scientific and performance-enhancing libraries. The layers of software that are available to users are referred to as the software stack. The lower layers of this stack are typically focused on performance and include compilers, communication libraries, and user-space interfaces to HPC hardware components. The middle layer includes performance tools, math libraries, and data or computation abstraction layers. The top of the stack consists of end-user science or production applications.

Each software product within this stack may require certain versions or variants of other products and have many dependencies. For example, Hierarchical Data Format Version 5 (HDF5) is a scientific data formatting library with only seven dependencies, while Data Mining Classification and Regression Methods (rminer) — an R-based data mining application — has 150 software dependencies. The full software stack can be split into three general categories that differ based on the maintainer: user software, facility software, and vendor software.

## 2.1.13. User Software

Often, the end users themselves best understand how to tune their software to the bespoke hardware of an HPC system to ensure sufficient performance for their workload. Users regularly modify and recompile their software to enhance performance, fix bugs, and adapt to changes in the system.# Software Management in HPC

## User-Built Software and Open Source Concerns
The sharing of user-built software between teams may be common. User software that is widely used is often open source and, therefore, subject to open-source software supply chain concerns.

## Continuous Integration (CI) in HPC
Continuous integration (CI) pipelines and tests of scientific code on HPC platforms have recently become commonplace. Industry-standard identification of software weaknesses and the publication of Common Vulnerabilities and Exposures (CVEs) is not routine, but the identification and remediation of performance regressions is generally a higher priority within the user community.

There is a value-per-cycle trade-off for CI tests since cheaper cycles on commodity hardware may not expose bugs on much more expensive HPC resources. Complex test suites will eat into user allocations, and users and staff prefer that only a cardinal set of smoke tests run within user-developed testing pipelines on HPC systems.

## Site-Provided Software and Vendor Software
Site staff and administrators generally build applications and libraries that are most likely to be used. Tools such as Conda, EasyBuild, and Spack are often used to manage the complexity of software dependency resolution. Staff may also wrap compiler and job submission utilities with custom scripts to collect usage information about software libraries, I/O read and write patterns, or other system telemetry that is useful for decision making.

Vendor software includes low-level system tools to facilitate the running of other software. For instance, remote direct memory access, inter-node memory sharing, performance counters, temperature and power telemetry, and debugging are all vendor-provided software. Users can choose specific versions of installed vendor and site-provided software libraries by manipulating environment variables. Tools such as wrapper scripts or module files are usually provided to help users find and choose which versions of installed software to use.

## Containerized Software in HPC
A container is a software bundle that includes the application along with some or all of the dependencies, libraries, other binaries, and configuration files needed to run it. Containers provide self-contained, portable, and reproducible environments that abstract away the differences in OS distributions and underlying infrastructure.

Containers can make applications more portable, easier to deploy, and easier to distribute. For instance, containers allow users to use a package manager (e.g., apt or yum) to install software without modifying the host system, effectively decoupling the application dependencies from the host operating system.

Containers are executed by a container runtime, such as Docker, Containerd, Apptainer, or Charliecloud. Container runtimes create an isolated execution space for the application by leveraging technologies like Linux Namespaces and Cgroups.# Seccomp and Container Execution

The runtime is responsible for preparing the execution environment, establishing an isolated execution namespace, and executing the application. Runtimes may handle other tasks, such as obtaining and attaching the container image, mounting filesystems within the container, and creating pass-through access to devices. Container execution may be further abstracted using a container orchestration service, such as Kubernetes, OpenShift, or others.

## 3. HPC Threat Analysis

HPC poses unique security and privacy challenges, and collaboration and resource-sharing are integral. For instance, scientific experiments frequently employ unique hardware, software, and configurations that may not be maintained or well-vetted or that present entirely new classes of vulnerabilities that are absent in more traditional environments. HPC can store large amounts of sensitive research data, personally identifiable information (PII), and intellectual property (IP) that need to be safeguarded. Finally, HPC data and computation are encumbered with a variety of different security and policy constraints that stem from the fact that HPC systems are often operated as shared resources with different user groups, each of which are required to operate under the goals and constraints set by their organizations. The solutions to protecting data, computation, and workflows must balance these trade-offs.

### 3.1. Key HPC Security Characteristics and Use Requirements

HPC systems possess some unique security characteristics and distinctive use requirements that differentiate themselves from enterprise IT systems:

- **Tussles between performance and security:** HPC users may consider security to be valuable only to the extent that it does not significantly slow down the HPC system and impede research. Ensuring the usability of security mechanisms with a tolerable performance penalty is therefore critical to adoption by the scientific HPC community.

- **Varying security requirements for different HPC applications:** Individual platforms, projects, and data may have significantly different security sensitivities and need to follow different security policies. An HPC system may need to enforce multiple security policies simultaneously.

- **Limited resources for security tools:** Most HPC systems are designed to devote their resources to maximizing performance rather than acquiring and operating security tools.

- **Open-source software and self-developed research software:** Open-source software and self-developed research software are widely used in HPC. Open-source software is vulnerable to open-source software supply chain threats, while HPC software input data may be vulnerable to data supply chain threats. Self-developed software is susceptible to low software quality.

- **Granular access control on databases:** Since different research groups may have a need...# Access Control and Threats in HPC Systems

To know for different portions of data, granular access control capabilities are necessary. Access control requirements may need to be dynamically adjusted as some scientific experiments may increase data needs based on the outcome of experiments.

## 3.2. Threats to HPC Function Zones

The following subsections discuss threats to the four functional zones in the HPC reference architecture.

### 3.2.1. Access Zone Threats

The access zone provides an interface for external users to access the HPC system and oversees the authentication and authorization of users. Among the four function zones, the access zone is the only one that is directly connected to the external networks. Hence, the nodes and their software stacks in this zone are susceptible to external attacks, such as:

- Denial-of-service (DoS) attacks
- Perimeter network scanning and sniffing (when not done as part of security practices)
- Authentication attacks (e.g., brute force login attempts and password guessing)
- User session hijacking
- Attacker-in-the-middle attacks

Some nodes are even subject to extra attacks due to their specific software stacks. For instance, a web server may be subject to:

- Website defacement
- Phishing
- Misconfiguration
- Code injection attacks

The access zone also provides access to the file systems hosted in the data storage zone. It is important that permissions to directories and files are only given to authorized users. Applying multi-factor authentication (MFA) methods to HPC system access, which requires a user to provide one or more verification methods at login, is a proven method to mitigate the risk of unauthorized access.

Authenticated users sometimes use external networks to download data or code for use inside of the HPC system, which introduces the risk of unintentionally downloading malicious content. The nodes in the access zone are usually configured to support limited computation (e.g., modest debugging). Access zone nodes are susceptible to computational resource abuse.

The access zone is also often shared by multiple users. One user’s activities, such as commands issued and jobs submitted, can be viewed by other users. A port opened by one user can potentially be used by others. Fortunately, the nodes in the access zone work similarly to enterprise servers, and general IT security tools and measures are available to harden the zone.

### 3.2.2. Management Zone Threats

The management zone is responsible for managing the entire HPC system. It is connected to clustered internal networks through which other zones can be reached. It runs a plethora of system management, out-of-band hardware management, job scheduling, and workflow management software, all of which are susceptible to unique threats.

Processes that run in the management zone, such as schedulers and data tiering or orchestration processes, act on behalf of users. These are privileged processes, and if they are...# High-Performance Computing Security Threats

## 3.2.3 High-Performance Computing Zone Threats

The high-performance computing zone offers core computational functions in an HPC system. The compute nodes are shared by multiple users or tenants. The exploitation of multi-tenancy environments is a major threat (e.g., side-channel attacks, user data/program leakage, etc.).

Other threat sources that often cause extreme resource consumption, performance degradation, or the outage of the HPC system entirely include:

- Accidental misconfiguration
- Software bugs introduced by user-developed software
- System abuse by running applications that are not aligned with the HPC mission

Container escape, side-channel attacks, and DoS can also be threats if virtualization technologies (e.g., containers) are used in the high-performance computing zone.

As a security mitigating technology, the applications in HPC are mostly run in the user space, except for system calls that must run in the kernel with elevated privilege. Accelerators, high-performance interconnects, special protocols, and direct memory access between nodes are commonly used in the high-performance computing zone. Some of these technologies may not be thoroughly tested from a security point of view, and their speed, novelty, and complexity can make monitoring and detecting suspicious activity difficult. Direct memory access and communication between nodes may bypass the kernel, and the protections provided by the kernel (e.g., Security-Enhanced Linux [SELinux]) are lost.

## 3.2.4 Data Storage Zone Threats

Protecting the confidentiality, integrity, and availability (CIA) of user data is essential for the data storage zone. Data integrity can be compromised by malicious data deletion, corruption, pollution, or false data injection. Legitimate users may also mishandle sensitive data, leading to confidentiality breakdown. File metadata (e.g., file name, author, size, creation date) can also leak sensitive information about the files.

HPC file systems in the data storage zone provide superior data access speed and much larger storage capacity compared to average enterprise file systems. Hard disk failure is a threat due to...# Data Storage Challenges in HPC

## Introduction
The large number of disks deployed in the data storage zone presents significant challenges for incident response and contingency planning controls. Implementing effective file backup, recovery, and forensic imaging can become infeasible, and the security measures that can be applied to enterprise file systems may take an unacceptably long time, degrading HPC file system performance.

## Data Backup Services
Providing data backup services in High-Performance Computing (HPC) is particularly challenging due to the large volume of data. By default, user data is often not backed up, placing the responsibility on users to maintain their own data copies. Inadvertent operations, such as accidentally deleting a file subdirectory, can lead to permanent data loss. One way to mitigate this risk is to make data READ ONLY.

Some organizations offer backup services using their own HPC systems; however, these systems may be located in the same geographic areas and thus be subject to the same environmental threats.

## Sensitive Information and Compliance
Data stored in HPC systems may contain sensitive information, including:
- Personally Identifiable Information (PII)
- Patient Health Information (PHI)
- Controlled Unclassified Information (CUI)

Such data often requires compliance with security standards, such as HIPAA and NIST SP 800-171. To protect this data, a variety of privacy-preserving technologies are available, including:
- Data anonymization
- Obfuscation
- Randomization
- Masking
- Differential privacy

However, applying these technologies while maintaining HPC usability and performance can be challenging.

## Other Threats
In addition to threats unique to individual function zones, general HPC systems face several other threats:

### Environmental and Physical Threats
Physical or cyber attacks against facilities (e.g., power, cooling, water), unauthorized physical access, and natural disasters (e.g., fire, flood, earthquake, hurricane) are all potential threats to an HPC system.

### Vulnerabilities from Performance Prioritization
HPC systems are designed to process large volumes of data and perform complex computations at very high speeds. While achieving the highest performance is a priority, this can introduce security vulnerabilities. For example:
- Designers may consciously choose to build less redundant systems to enhance performance, which can make the system less robust and more vulnerable to attacks, such as Denial of Service (DoS) attacks.
- Although using a backup system can improve robustness and high availability, building a spare or backup HPC system is often prohibitively expensive. Consequently, HPC systems frequently lack storage backup due to the vast size of the data stored, and many HPC missions do not have a service-level agreement justifying the need for a full backup system.# Backup Location

All resources are poured into building the single best HPC system possible.

## Supply Chain Threats
The HPC supply chain faces a variety of threats, including:
- Theft of proprietary information
- Attacks on critical hardware components
- Software manipulation to gain unauthorized access

Some HPC software (e.g., OS, applications), firmware, and hardware components have limited manufacturers, suppliers, and integrators, making diversification difficult. Limited suppliers also lead to shortages in the qualified workforce who can provide the required technical support.

## Insider Threats
Insider threats come from people within the organization who have internal information and may have the privileges needed to access the HPC system. Insider threats can be classified into:
- **Accidental/Unintentional Threats**: These arise from the unintended side effects of normal actions and activity.
- **Malicious/Intentional Threats**: A malicious insider may intentionally upload malicious code into the HPC system.

# HPC Security Posture, Challenges, and Recommendations

## HPC Access Control via Network Segmentation
Access control is a security technique that regulates who can access and/or use resources in a computing environment. In HPC, multiple physical networks are constructed as an effective means of access control:

### Management Network
The management network is a dedicated network that allows system administrators to remotely control, monitor, and configure computer nodes in an HPC system. Modern computers are often equipped with the Intelligent Platform Management Interface (IPMI), which provides management and monitoring capabilities that are independent of the host system’s CPU, firmware (e.g., BIOS, Unified Extensible Firmware Interface [UEFI]), and operating system.

For example, IPMI allows system administrators to remotely turn unresponsive machines on or off and install custom operating systems. IPMIs are connected to the management network, which can only be accessed by authorized system administrators. Collecting logs from the HPC system and forwarding the relevant logs to the security and information event management (SIEM) system, which could be external to the HPC system, is a component of the management zone.

### High-Performance Networks
High-performance networks offer high bandwidth and low latency to connect computer nodes inside the high-performance zone and data storage zone. They also support features that are unique to HPC, such as remote direct memory access (RDMA) over the network and the message passing interface (MPI). High-performance networks often use special communications standards and architectures (e.g., InfiniBand, Slingshot, Omni-Path, etc.).

### Auxiliary Networks
Additional auxiliary networks can be added to support usability and system manageability. For instance, a user network is constructed to allow users to manipulate or share data or remotely log into and access the compute nodes. Depending on the purpose of the networks, a subset of nodes from different zones are selected to be part of the networks. As an example, a user network contains the nodes...# High-Performance Computing (HPC) System Overview

## 1. Multiple Networks in HPC Systems

There are many benefits to having multiple networks in an HPC system:

- **Private Networks**: All networks are private and use different IP address ranges. Network traffic remains on one network, facilitating monitoring and measurement.

- **Purpose-Specific Networks**: Individual networks often serve specific purposes, connecting only the relevant nodes. This segmentation improves security.

- **Fault Tolerance**: Multiple physical networks provide a degree of fault tolerance. If one network goes down, the system administrator can use another network for diagnosis and troubleshooting.

### 1.1 Access Zone and Compute Nodes

The compute nodes in the access zone are connected to the external network and assigned public IP addresses, allowing users to remotely access the HPC systems. User data can be shared through the login nodes or the data transfer nodes. Some systems allow storage to be exported using CIFS or SMB (e.g., via a SAMBA server).

If necessary, a network address translation (NAT) or a proxy can be installed to allow users on a private network to access the internet and download new versions of software or share software data. However, NAT and Squid proxy can also pose security risks that require extra caution and mitigation considerations.

Employing multiple physical networks is a common and effective means for access control and fault tolerance, and it is highly recommended.

## 2. Compute Node Sanitization

High-performance compute nodes are often used by multiple tenants and projects. At the end of a task run, the previous project may leave behind a residual “footprint,” such as data in memory and GPUs. It is important to sanitize the compute node to prevent accidental data leakage from previous jobs and to ensure that the new job starts with a clean slate.

### 2.1 Examples of Compute Node Sanitization

- Conducting a node health check at the end of a job
- Removing a node or forcing a reboot if a node is deemed “unhealthy”
- Working with hardware and software vendors to provide management hooks to sanitize the GPU
- Resetting GPUs to remove residual data between jobs
- Validating and checking firmware
- Rebooting nodes after the completion of a job at the OS level to remove accumulated residuals and ensure a consistent node state
- Checking critical files to ensure that they have not been changed

Compute node sanitization is highly recommended as the compute nodes are equipped with sophisticated hardware accelerators. Nevertheless, the process of sanitization must be carefully balanced with the goal of maximizing the utilization of HPC systems.

## 3. Data Integrity Protection

Cryptographic mechanisms are an effective means of providing data integrity. HPC data storage systems typically support uniform encryption at the file level or block level. Such data encryption at the file system level protects data from unauthorized access.# Data Integrity and Security in HPC

## Granular Access
However, it does not provide granular access (i.e., segmenting one user from others). Granular access can be achieved using user-level or group-level encryption. Not even a system administrator can access a user’s data with granular access since they do not have access to the decryption key.

## User Authentication
Additionally, file systems do not authorize users. Rather, users access the file system via the HPC access zone, which is responsible for authenticating the users and their access rights. In general, file systems should not be mounted outside of their local security boundaries unless the file system protocol appropriately authenticates users. For example, an attacker-controlled system that can remotely mount a file system will have complete control over all file system data. This access could be used to gain privileges elsewhere within other zones of the HPC security enclave.

## Data Integrity Monitoring Techniques

### Hashing
Hashing is another technique for monitoring data integrity. Data files can be hashed at the beginning to acquire hashing keys. A file is not modified if its hashing key remains the same. Parallel file systems maintain many types of metadata (e.g., user ID, group ID, modification time, checksum, etc.). Hashing metadata is another way to check whether a file has been modified.

### Malware Scanning
Periodically scanning file systems for malware is a proven technique for monitoring data integrity. However, scanning an HPC system for malware is challenging. HPC data storage can easily contain a petabyte or more of data. Existing malware scanning tools are mainly designed for a single machine or laptop. They are not efficient or fast enough to scan large HPC data storage systems. Furthermore, the scanning operation can adversely affect the performance of running jobs. HPC systems may consider alternate implementation options for malware scanning to alleviate some of the stated impacts. For example, conduct scanning at write, and conduct subsequent batch scanning at read.

### Ransomware Protection
Protection against ransomware attacks on HPC data is especially important. It is recommended to conduct risk assessments of ransomware attacks, and plan and implement mechanisms to protect the HPC system from ransomware attacks, as applicable.

## Conclusion
Protecting data integrity is vital to HPC security. Granular data access provides the best protection and is highly recommended when possible.

# Securing Containers
Containers bundle an application’s code, related libraries, configuration files, and required dependencies to allow the applications to run seamlessly across environments. Containers provide the benefits of portability, reproducibility, and productivity, but they can hide software. By decoupling operating system dependencies from the host system, a container image may contain unknown vulnerable or insecure software, which poses a software supply chain risk.# Security in High-Performance Computing (HPC)

## Container Vulnerabilities and Security

Additionally, container contents may not be observable by some security auditing tools. In a well-managed HPC system, most software has already been installed as a baseline system environment. The applications developed using native libraries often run faster than a container. Hence, training users to develop programs in the HPC programming environment is one way to reduce exposure to container vulnerabilities.

Many container ecosystems include the ability to attest that a container and its dependencies are supplied by a trusted provider, and there are tools (e.g., Qualys, Anchore, etc.) designed to audit container contents that provide supply chain and software audit management for HPC applications.

Containers, container runtimes, and container orchestrators all increase the security attack surface for HPC systems, but carefully implemented container environments may potentially improve the overall security posture. Vulnerable or poorly configured container runtimes or container orchestrators can lead to exploitable environments and, in some cases, even privilege escalation.

Additionally, many orchestration and runtime tools expose APIs that must be appropriately isolated and secured. However, when carefully implemented, container isolation can reduce risk by restricting a container from performing disallowed operations. For instance, data access to a container can be limited to the least required filesystem mounts, network access can be limited through network namespaces, and tools like seccomp can be used to restrict containers from making disallowed system calls.

## Achieving Security While Maintaining HPC Performance

HPC security measures often come with an undesirable performance penalty. The following are several effective ways to balance performance and security:

- **Conduct tests** to measure the performance penalty of security tools, which can be benchmarked to determine whether they are acceptable. Testing and measurement would also encourage more performance-aware tool design.

- **Incorporate security requirements** in the initial HPC design rather than as an afterthought. For instance, independent add-on security tools tend to have more impact on performance than native security measures that come with the HPC software stack. Key management architecture plays an important role in HPC security but requires careful performance analysis.

- **Avoid “one size fits all” security.** Differentiate the types of nodes in the HPC system, and apply appropriate security rules and controls to different node types. For instance, classify the nodes in HPC systems into three categories: external nodes, internal nodes, and backend nodes. Apply individual security controls to each node category. Such differentiation also mitigates performance impacts.

## Challenges to HPC Security Tools

Many enterprise security tools are designed with stand-alone devices in mind (e.g., laptops, desktops, or mobile devices). HPC is a large-scale, complex system with strict requirements that may not align with the capabilities of traditional security tools.# Performance Requirements in HPC Security

Security tools that are effective for individual devices may not work well in an HPC environment. For example, a forensic tool that aids the recovery and preservation of a hard drive and memory for a single server works well in practice. It is unreasonable, however, to install forensic tools on all compute and storage nodes in an HPC system.

## Challenges in HPC Security Tools

As another example, HPC nodes may use remotely mounted storage, which may disable some security tools. Moreover, different HPC applications may require different tools. Security tool vendors are often not accustomed to HPC use cases and requirements, which forces HPC security teams to develop analogous tools that may introduce new security vulnerabilities and are sometimes not accepted by organizations.

The HPC community needs to work closely with security tool vendors to address these challenges.

## Security Technical Implementation Guide (STIG)

A Security Technical Implementation Guide (STIG) is a configuration standard and offers a security baseline that reflects security guidance requirements. The security checking tool can measure how well the STIG is satisfied. However, available STIGs are typically written for servers or desktops rather than for HPC systems.

In addition, security baseline checking tools developed for commodity operating systems and applications require customizations to run on HPC. The Lawrence Livermore National Laboratory, Sandia National Laboratories, and the Los Alamos National Lab have collaborated with the Defense Information Systems Agency (DISA) to develop the TOSS 4 STIG, which is geared toward HPC systems. Still, a more general STIG library and corresponding security checking tools are desirable to handle diverse subsystems and components inside an HPC system.

# Conclusions

Securing HPC systems is challenging due to their size, performance requirements, diverse and complex hardware, software, and applications, varying security requirements, and the nature of shared resources. The security tools suitable for HPC are inadequate, and current standards and guidelines on HPC security best practices are lacking. The continuous evolution of HPC systems makes the task of securing them even more difficult.

This Special Publication aims to set a foundation for standardizing and facilitating HPC security information and knowledge-sharing. A zone-based HPC system reference architecture is introduced to serve as a foundation for a system lexicon and captures common features across the majority of HPC systems. HPC system threat analysis is discussed, security postures and challenges are considered, and recommendations are made.