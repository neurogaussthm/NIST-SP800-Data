NIST Special Publication 800-90B Recommendation for the Entropy Sources Used for Random Bit Generation Meltem Sönmez Turan Elaine Barker John Kelsey Kerry A. McKay Mary L. Baish Mike Boyle This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B C O M P U T E R S E C U R I T YNIST Special Publication 800-90B Recommendation for the Entropy Sources Used for Random Bit Generation Meltem Sönmez Turan Elaine Barker John Kelsey Kerry McKay Computer Security Division Information Technology Laboratory Mary L. Baish Mike Boyle National Security Agency Fort Meade, MD This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B January 2018 U.S. Department of Commerce Wilbur L. Ross, Jr., Secretary National Institute of Standards and Technology Walter Copan, NIST Director and Under Secretary of Commerce for Standards and TechnologyAuthority This publication has been developed by NIST in accordance with its statutory responsibilities under the Federal Information Security Modernization Act (FISMA) of 2014, 44 U.S.C. § 3551 et seq., Public Law (P.L.) 113-283. NIST is responsible for developing information security standards and guidelines, including minimum requirements for federal information systems, but such standards and guidelines shall not apply to national security systems without the express approval of appropriate federal officials exercising policy authority over such systems. This guideline is consistent with the requirements of the Office of Management and Budget (OMB) Circular A-130. Nothing in this publication should be taken to contradict the standards and guidelines made mandatory and binding on federal agencies by the Secretary of Commerce under statutory authority. Nor should these guidelines be interpreted as altering or superseding the existing authorities of the Secretary of Commerce, Director of the OMB, or any other federal official. This publication may be used by nongovernmental organizations on a voluntary basis and is not subject to copyright in the United States. Attribution would, however, be appreciated by NIST. National Institute of Standards and Technology Special Publication 800-90B Natl. Inst. Stand. Technol. Spec. Publ. 800-90B, 84 pages (January 2018) CODEN: NSPUE2 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B Any mention of commercial products or reference to commercial organizations is for information only; it does not imply recommendation or endorsement by the United States Government, nor does it imply that the products mentioned are necessarily the best available for the purpose. There may be references in this publication to other publications currently under development by NIST in accordance with its assigned statutory responsibilities. The information in this publication, including concepts and methodologies, may be used by Federal agencies even before the completion of such companion publications. Thus, until each publication is completed, current requirements, guidelines, and procedures, where they exist, remain operative. For planning and transition purposes, Federal agencies may wish to closely follow the development of these new publications by NIST. Organizations are encouraged to review all draft publications during public comment periods and provide feedback to NIST. Many NIST cybersecurity publications, other than the ones noted above, are available at https://csrc.nist.gov/publications. Comments on this publication may be submitted to: National Institute of Standards and Technology Attn: Computer Security Division, Information Technology Laboratory 100 Bureau Drive (Mail Stop 8930) Gaithersburg, MD 20899-8930 Email: rbg_comments@nist.gov All comments are subject to release under the Freedom of Information Act (FOIA).ii This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B Reports on Computer Systems Technology The Information Technology Laboratory (ITL) at the National Institute of Standards and Technology (NIST) promotes the U.S. economy and public welfare by providing technical leadership for the Nation’s measurement and standards infrastructure. ITL develops tests, test methods, reference data, proof of concept implementations, and technical analyses to advance the development and productive use of information technology. ITL’s responsibilities include the development of management, administrative, technical, and physical standards and guidelines for the cost-effective security and privacy of other than national security-related information in Federal information systems. The Special Publication 800-series reports on ITL’s research, guidelines, and outreach efforts in information system security, and its collaborative activities with industry, government, and academic organizations. Abstract This Recommendation specifies the design principles and requirements for the entropy sources used by Random Bit Generators, and the tests for the validation of entropy sources. These entropy sources are intended to be combined with Deterministic Random Bit Generator mechanisms that are specified in SP 800-90A to construct Random Bit Generators, as specified in SP 800-90C. Keywords Conditioning functions; entropy source; health testing; min-entropy; noise source; predictors; random number generators Acknowledgements The authors of this Recommendation gratefully acknowledge and appreciate contributions by their colleagues at NIST, Apostol Vassilev and Timothy A. Hall; and Aaron H. Kaufer and Darryl M. Buller of the National Security Agency for assistance in the development of this Recommendation. NIST also thanks the many contributions by the public and private sectors. Conformance Testing Conformance testing for implementations of this Recommendation will be conducted within the framework of the Cryptographic Algorithm Validation Program (CAVP) and the Cryptographic Module Validation Program (CMVP). The requirements of this Recommendation are indicated by the word “shall.” Some of these requirements may be out-of-scope for CAVP or CMVP validation testing, and thus are the responsibility of entities using, implementing, installing or configuring applications that incorporate this Recommendation.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION iii This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B Table of Contents 1 Introduction ............................................................................................................ 1 1.1 Scope .............................................................................................................. 1 1.2 Organization ................................................................................................... 2 1.3 Symbols .......................................................................................................... 2 2 General Discussion ................................................................................................ 4 2.1 Min-Entropy .................................................................................................... 4 2.2 The Entropy Source Model ............................................................................. 5 2.2.1 Noise Source ........................................................................................ 5 2.2.2 Conditioning Component ...................................................................... 6 2.2.3 Health Tests ......................................................................................... 6 2.3 Conceptual Interfaces ..................................................................................... 6 2.3.1 GetEntropy: An Interface to the Entropy Source .................................. 6 2.3.2 GetNoise: An Interface to the Noise Source ......................................... 7 2.3.3 HealthTest: An Interface to the Entropy Source ................................... 7 3 Entropy Source Validation .................................................................................... 9 3.1 Validation Process .......................................................................................... 9 3.1.1 Data Collection ..................................................................................... 9 3.1.2 Determining the track: IID track vs. non-IID track ............................... 11 3.1.3 Initial Entropy Estimate ....................................................................... 12 3.1.4 Restart Tests ...................................................................................... 12 3.1.5 Entropy Estimation for Entropy Sources Using a Conditioning Component .................................................................................................... 14 3.1.6 Additional Noise Sources ................................................................... 17 3.2 Requirements for Validation Testing ............................................................. 18 3.2.1 Requirements on the Entropy Source................................................. 18 3.2.2 Requirements on the Noise Source .................................................... 18 3.2.3 Requirements on the Conditioning Component .................................. 19 3.2.4 Requirements on Data Collection ....................................................... 20 4 Health Tests .......................................................................................................... 22 4.1 Health Test Overview .................................................................................... 22 4.2 Types of Health Tests ................................................................................... 22NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION iv This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 4.3 Requirements for Health Tests ..................................................................... 23 4.4 Approved Continuous Health Tests .............................................................. 25 4.4.1 Repetition Count Test ......................................................................... 25 4.4.2 Adaptive Proportion Test .................................................................... 26 4.5 Developer-Defined Alternatives to the Continuous Health Tests .................. 28 5 Testing the IID Assumption ................................................................................. 29 5.1 Permutation Testing ...................................................................................... 29 5.1.1 Excursion Test Statistic ...................................................................... 31 5.1.2 Number of Directional Runs ............................................................... 31 5.1.3 Length of Directional Runs ................................................................. 31 5.1.4 Number of Increases and Decreases ................................................. 32 5.1.5 Number of Runs Based on the Median ............................................... 32 5.1.6 Length of Runs Based on Median ...................................................... 33 5.1.7 Average Collision Test Statistic .......................................................... 33 5.1.8 Maximum Collision Test Statistic ........................................................ 34 5.1.9 Periodicity Test Statistic ..................................................................... 34 5.1.10 Covariance Test Statistic .................................................................... 35 5.1.11 Compression Test Statistics ............................................................... 35 5.2 Additional Chi-square Statistical Tests .......................................................... 35 5.2.1 Testing Independence for Non-Binary Data ....................................... 35 5.2.2 Testing Goodness-of-fit for Non-Binary Data ...................................... 37 5.2.3 Testing Independence for Binary Data ............................................... 37 5.2.4 Testing Goodness-of-fit for Binary Data ............................................. 38 5.2.5 Length of the Longest Repeated Substring Test ................................ 39 6 Estimating Min-Entropy ....................................................................................... 40 6.1 IID Track: Entropy Estimation for IID Data .................................................... 40 6.2 Non-IID Track: Entropy Estimation for Non-IID Data..................................... 40 6.3 Estimators ..................................................................................................... 41 6.3.1 The Most Common Value Estimate .................................................... 41 6.3.2 The Collision Estimate ........................................................................ 42 6.3.3 The Markov Estimate .......................................................................... 43 6.3.4 The Compression Estimate ................................................................ 45 6.3.5 t-Tuple Estimate ................................................................................. 47NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION v This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 6.3.6 Longest Repeated Substring (LRS) Estimate ..................................... 48 6.3.7 Multi Most Common in Window Prediction Estimate .......................... 49 6.3.8 The Lag Prediction Estimate .............................................................. 51 6.3.9 The MultiMMC Prediction Estimate .................................................... 52 6.3.10 The LZ78Y Prediction Estimate .......................................................... 55 6.4 Reducing the Symbol Space ......................................................................... 58 List of Appendices Acronyms ............................................................................................ 60 Glossary .............................................................................................. 61 References .......................................................................................... 67 Min-Entropy and Optimum Guessing Attack Cost ........................... 69 The Narrowest Internal Width ............................................................ 72 CBC-MAC Specification ...................................................................... 73 Different Strategies for Entropy Estimation ..................................... 74 G.1 Entropic Statistics ......................................................................................... 74 G.1.1 Approximation of ..................................................................... 74 G.2 Predictors ...................................................................................................... 75 𝐅𝐅(𝟏𝟏/𝐳𝐳)NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION vi This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B List of Figures Figure 1 Entropy Source Model ....................................................................................... 5 Figure 2 Entropy Estimation Strategy ............................................................................ 10 Figure 3 Entropy of the Conditioning Component ......................................................... 15 Figure 4 Generic Structure for Permutation Testing ...................................................... 29 Figure 5 Pseudo-code of the Fisher-Yates Shuffle ........................................................ 30 List of Tables Table 1 The narrowest internal width and output lengths of the vetted conditioning functions. ................................................................................................................ 16 Table 2 Example cutoff values of the Adaptive Proportion Test .................................... 27 Table 3 values for different r values when L=1 000 000. .................................. 76 𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 1 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 1 Introduction 1.1 Scope Cryptography and security applications make extensive use of random numbers and random bits. However, the generation of random bits is problematic in many practical applications of cryptography. The NIST Special Publication (SP) 800-90 series of Recommendations provides guidance on the construction and validation of Random Bit Generators (RBGs) in the form of Deterministic Random Bit Generators (DRBGs) (also known as pseudorandom number generators) or Non-deterministic Random Bit Generators (NRBGs) that can be used for cryptographic applications. This Recommendation specifies how to design and test entropy sources that can be used by these RBGs. SP 800-90A addresses the construction of approved DRBG mechanisms, while SP 800-90C addresses the construction of RBGs from the mechanisms in SP 800-90A and the entropy sources in SP 800-90B. These Recommendations provide a basis for validation by NIST's Cryptographic Algorithm Validation Program (CAVP) and Cryptographic Module Validation Program (CMVP). An entropy source that conforms to this Recommendation can be used by RBGs to produce a sequence of random bits. The outputs of entropy sources should contain a sufficient amount of randomness to provide security. This Recommendation describes the properties that an entropy source must have to make it suitable for use by cryptographic random bit generators, as well as the tests used to validate the quality of the entropy source. The development of entropy sources that construct unpredictable outputs is difficult, and providing guidance for their design and validation testing is even more so. The testing approach defined in this Recommendation assumes that the developer understands the behavior of the source of randomness within the entropy source and has made a good-faith effort to produce an entropy source suitable for cryptographic applications (e.g., produces bitstrings that can provide entropy at a rate that meets (or exceeds) a specified value). It is expected that, over time, improvements to the guidance and testing will be made, based on experience in using and validating against this Recommendation. This Recommendation is intended for use by entropy source developers (the entity that designs and builds the entropy source or a portion thereof), submitters1 (the entity that submits the entropy source for validation testing), NVLAP-accredited laboratories that validate entropy sources and any entity with an interest in having an entropy source validated. This Recommendation was developed in concert with American National Standard (ANS) X9.82, a multi-part standard on random number generation. 1 The submitter may or may not be a developer; if the submitter is not the developer then the submitter may need to acquire required information from the developer before submission or during validation testing.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 2 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 1.2 Organization Section 2 gives a general discussion on min-entropy, the entropy source model and the conceptual interfaces. Section 3 explains the validation process and lists the requirements on the entropy source, data collection, documentation, etc. Section 4 describes the health tests. Section 5 includes various statistical tests to check whether or not the entropy source outputs are IID (independent and identically distributed). Section 6 provides several methods to estimate the entropy of the noise source. The appendices include a list of acronyms, a glossary, references, a discussion on min- entropy and the optimum-guessing-attack cost, information about the narrowest internal width, Cipher Block Chaining – Message Authentication Code (CBC-MAC) specification, and the underlying information on different entropy estimation strategies used in this Recommendation. 1.3 Symbols The following symbols and functions are used in this Recommendation. The alphabet, i.e., the set of all possible symbols that a (digitized) noise A={x ,x ,…,x } 1 2 k source produces. The min-entropy of the samples from a (digitized) noise source or of the H output from an entropy source; the min-entropy assessment for a noise source or entropy source. H Initial entropy estimate. I H Entropy estimate of the sequential dataset original H The entropy estimate provided by the submitter. submiter L The number of samples. log (x) The logarithm of x with respect to base b. b ln(x) The natural logarithm. min(a, b) A function that returns the minimum of the two values a and b. max(a, b) A function that returns the maximum of the two values a and b. M[i][j] The jth sample from the ith restart of the noise source. n The length of x i in bits. nw Narrowest width of the conditioning componentNIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 3 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B k The number of possible symbols, i.e., the size of the alphabet. α The probability of falsely rejecting the null hypothesis (type I error). |a| A function that returns the absolute value of a. p i The probability for an observation (or occurrence) of the symbol x i in A. The probability of observing the most common symbol from a noise p max source. S=(s 1,…,s L) A dataset that consists of an ordered collection of L samples, where s i ϵ A. x A possible output from the (digitized) noise source. i [a, b] The interval of numbers between a and b, including a and b. A function that returns the smallest integer greater than or equal to x; also x known as the ceiling function. A function that returns the largest integer less than or equal to x; also x known as the floor function. || Concatenation. ⊕ Bit-wise exclusive-or operation.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 4 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 2 General Discussion The three main components of a cryptographic RBG are a source of random bits (an entropy source), an algorithm for accumulating and providing random bits to the consuming applications, and a way to combine the first two components appropriately for cryptographic applications. This Recommendation describes how to design and test entropy sources. SP 800-90A describes deterministic algorithms that take an entropy input and use it to produce pseudorandom values. SP 800-90C provides the “glue” for putting the entropy source together with the algorithm to implement an RBG. Specifying an entropy source is a complicated matter. This is partly due to confusion in the meaning of entropy, and partly due to the fact that, while other parts of an RBG design are strictly algorithmic, entropy sources depend on physical processes that may vary from one instance of a source to another. This section discusses, in detail, both the entropy source model and the meaning of entropy. 2.1 Min-Entropy The central mathematical concept underlying this Recommendation is entropy. Entropy is defined relative to one’s knowledge of an experiment’s output prior to observation, and reflects the uncertainty associated with predicting its value – the larger the amount of entropy, the greater the uncertainty in predicting the value of an observation. There are many possible measures for entropy; this Recommendation uses a very conservative measure known as min-entropy, which measures the effectiveness of the strategy of guessing the most likely output of the entropy source. (see Appendix D and [Cac97] for more information). In cryptography, the unpredictability of secret values (such as cryptographic keys) is essential. The probability that a secret is guessed correctly in the first trial is related to the min-entropy of the distribution that the secret was generated from. The min-entropy of an independent discrete random variable X that takes values from the set A={x ,x ,…,x } with probability Pr(X=x) = p for i =1,…,k is defined as 1 2 k i i , 𝐻𝐻 == 1m≤𝑖𝑖i≤n𝑘𝑘(−log2 𝑝𝑝 𝑖𝑖) If X has min-entropy H, then the probabili− tyl o ofg 2 ob1m≤s𝑖𝑖ea≤rx𝑘𝑘vi𝑝𝑝 n𝑖𝑖 g. any particular value for X is no greater than 2 −H. The maximum possible value for the min-entropy of a random variable with k distinct values is log k, which is attained when the random variable has a uniform probability distribution, 2 i.e., p = p =…= p =1/k. 1 2 kNIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 5 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 2.2 The Entropy Source Model This section describes the entropy source model in detail. Figure 1 illustrates the model that this Recommendation uses to describe an entropy source and its components, which consist of a noise source, an optional conditioning component and a health testing component. Figure 1 Entropy Source Model 2.2.1 Noise Source The noise source is the root of security for the entropy source and for the RBG as a whole. This is the component that contains the non-deterministic, entropy-providing process that is ultimately responsible for the uncertainty associated with the bitstrings output by the entropy source. If the non-deterministic activity being sampled produces something other than binary data, the sampling process includes a digitization process that converts the output samples to bits. The output of the digitized noise source is called the raw data. This Recommendation assumes that the sample values (i.e., the symbols) obtained from a noise source consist of fixed-length bitstrings. Noise sources can be divided into two categories: Physical noise sources use dedicated hardware to generate randomness; whereas Non-physical noise sources use system data (such as output of Application Programming Interface (API) functions, Random Access Memory (RAM) data or system time) or human input (e.g., mouse movements) to generate randomness.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 6 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B If the noise source fails to generate random outputs, no other component in the RBG can compensate for the lack of entropy; hence, no security guarantees can be made for the application relying on the RBG. 2.2.2 Conditioning Component The optional conditioning component is a deterministic function responsible for reducing bias and/or increasing the entropy rate of the resulting output bits (if necessary to obtain a target value). There are various methods for achieving this. The developer should consider how the conditioning component to be used and how variations in the behavior of the noise source may affect the entropy rate of the output. In choosing an approach to implement, the developer may either choose to implement a cryptographic algorithm listed in Section 3.1.5.1.1 or use an alternative algorithm as a conditioning component. The use of either of these approaches is permitted by this Recommendation. 2.2.3 Health Tests Health tests are an integral part of the entropy source design that are intended to ensure that the noise source and the entire entropy source continue to operate as expected. When testing the entropy source, the end goal is to obtain assurance that failures of the entropy source are caught quickly and with a high probability. Another aspect of health testing strategy is determining the likely failure modes for the entropy source and, in particular, for the noise source. Health tests are expected to include tests that can detect these failure conditions. The health tests can be separated into three categories: start-up tests, continuous tests (primarily on the noise source), and on-demand tests (See Section 4 for more information). 2.3 Conceptual Interfaces This section describes three conceptual interfaces that can be used to interact with the entropy source: GetEntropy, GetNoise and HealthTest. However, it is anticipated that the actual interfaces used may depend on the entropy source employed. These interfaces can be used when constructing an RBG as specified in SP 800-90C. 2.3.1 GetEntropy: An Interface to the Entropy Source The GetEntropy interface can be considered to be a command interface into the outer entropy source box in Figure 1. This interface is meant to indicate the types of requests for services that an entropy source may support. A GetEntropy call could return a bitstring containing the requested amount of entropy, along with an indication of the status of the request. Optionally, an assessment of the entropy can be provided. Note that the length of the returned bitstring may be greater than the amount of entropy requested.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 7 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B GetEntropy Input: bits_of_entropy: the requested amount of entropy Output: entropy_bitstring: The string that provides the requested entropy. status: A Boolean value that is TRUE if the request has been satisfied, and is FALSE otherwise. 2.3.2 GetNoise: An Interface to the Noise Source The GetNoise interface can be considered to be a command interface into the noise source component of an entropy source. This could be used to obtain raw, digitized outputs from the noise source for use in validation testing or for external health (i.e., testing performed external to the entropy source). While it is not required to be in this form, it is expected that an interface be available that allows noise source data to be obtained without harm to the entropy source. This interface is meant to provide test data to credit a noise source with an entropy estimate during validation or for external health testing. It is permitted that such an interface be available only in “test mode” and that it is disabled when the source is operational. This interface is not intended to constrain real-world implementations, but to provide a consistent notation to describe the data collection from noise sources. A GetNoise call returns raw, digitized samples from the noise source, along with an indication of the status of the request. GetNoise Input: number_of_samples_requested: An integer value that indicates the requested number of samples to be returned from the noise source. Output: noise_source_data: The sequence of samples from the noise source with a length of number_of_samples_requested. status: A Boolean value that is TRUE if the request has been satisfied, and is FALSE otherwise. 2.3.3 HealthTest: An Interface to the Entropy Source A HealthTest call is a request to the entropy source to conduct a test of its health. Note that it may not be necessary to include a separate HealthTest interface if the execution of the tests can be initiated in another manner that is acceptable to FIPS 140 [FIPS140] validation.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 8 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B HealthTest Input: type_of_test_requested: A bitstring that indicates the type or suite of tests to be performed (this may vary from one entropy source to another). Output: status: A Boolean value that is TRUE if the entropy source passed the requested test, and is FALSE otherwise.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 9 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 3 Entropy Source Validation Entropy source validation is necessary in order to obtain assurance that all relevant requirements of this Recommendation are met. This Recommendation provides requirements for validating an entropy source at a stated entropy rate. Validation consists of testing by an NVLAP-accredited laboratory against the requirements of SP 800-90B, followed by a review of the results by CAVP and CMVP. Validation provides additional assurance that adequate entropy is provided by the source and may be necessary to satisfy some legal restrictions, policies, and/or directives of various organizations. The validation of an entropy source presents many challenges. No other part of an RBG is so dependent on the technological and environmental details of an implementation. At the same time, the proper operation of the entropy source is essential to the security of an RBG. The developer should make every effort to design an entropy source that can be shown to serve as a consistent source of entropy, producing bitstrings that can provide entropy at a rate that meets (or exceeds) a specified value. In order to design an entropy source that provides an adequate amount of entropy per output bitstring, the developer must be able to accurately estimate the amount of entropy that can be provided by sampling its (digitized) noise source. The developer must also understand the behavior of the other components included in the entropy source, since the interactions between the various components may affect any assessment of the entropy that can be provided by an implementation of the design. For example, if it is known that the raw noise-source output is biased, appropriate conditioning components can be included in the design to reduce the bias of the entropy source output to a tolerable level before any bits are output from the entropy source. 3.1 Validation Process An entropy source may be submitted to an accredited lab for validation testing by the developer or any entity with an interest in having an entropy source validated. After the entropy source is submitted for validation, the lab will examine all documentation and theoretical justifications submitted. The lab will evaluate these claims, and may ask for more evidence or clarification. The general flow of entropy source validation testing is summarized in Figure 2. The following sections describe the details of the validation testing process. 3.1.1 Data Collection The submitter provides the following inputs for entropy estimation, according to the requirements presented in Section 3.2.4.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 10 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B Start validation Data collection (Section 3.1.1) Non-IID track Determine the track (Section 3.1.2) IID track Estimate entropy - IID track Estimate entropy - Non-IID (Section 6.1) track (Section 6.2) Apply Restart Tests (Section 3.1.4) No Pass restart tests? Yes Update entropy estimate (Section 3.1.4) No Is conditioning used? Yes Update entropy estimate (Section 3.1.5) Validation at entropy Validation fails. No estimate. entropy estimate awarded. Figure 2 Entropy Estimation StrategyNIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 11 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 1. A sequential dataset of at least 1 000 000 sample values obtained directly from the noise source (i.e., raw data) shall be collected for validation 2. If the generation of 1 000 000 consecutive samples is not possible, the concatenation of several smaller sets of consecutive samples (generated using the same noise source) is allowed. Smaller sets shall contain at least 1000 samples. The concatenated dataset shall contain at least 1 000 000 samples. 2. If the entropy source includes a conditioning component that is not listed in Section 3.1.5.1.1, a conditioned sequential dataset of at least 1 000 000 consecutive conditioning component outputs shall be collected for validation. The output of the conditioning component shall be concatenated in the order in which it was generated and treated as a binary string for testing purposes. Note that the data collected from the noise source for validation may be used as input to the conditioning component for the collection of conditioned output values. 3. For the restart tests (see Section 3.1.4), the entropy source must be restarted 1000 times; for each restart, 1000 consecutive samples shall be collected directly from the noise source. The restart data shall be extracted whenever the noise source is ready and able to provide data that can be used for producing entropy source output. This data is stored in a 1000×1000 restart matrix M, where M[i][j] represents the jth sample from the ith restart. 3.1.2 Determining the track: IID track vs. non-IID track In this Recommendation, entropy estimation is done using two different tracks: an IID-track and a non-IID track. The IID-track (see Section 6.1) is used for entropy sources that generate IID (independent and identically distributed) samples, whereas the non-IID track (see Section 6.2) is used for noise sources that do not generate IID samples. The track selection is done based on the following rules. The IID track shall be used only when all of the following conditions are satisfied: 1. The submitter makes an IID claim on the noise source, based on the submitter’s analysis of the design. The submitter shall provide rationale for the IID claim. 2. The sequential dataset described in item 1 of Section 3.1.1 is tested using the statistical tests described in Section 5 to verify the IID assumption, and the IID assumption is verified (i.e., there is no evidence that the data is not IID). 3. The row and column datasets described in item 3 of Section 3.1.1 are tested using the statistical tests described in Section 5 to verify the IID assumption, and the IID assumption is verified. 2 Providing additional data beyond what is required will result in more accurate entropy estimates. Lack of sufficient data may result in lower entropy estimates due to the necessity of mapping down the output values (see Section 6.4). It is recommended that, if possible, more data than is required be collected for validation. However, it is assumed in subsequent text that only the required data has been collected.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 12 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 4. If a conditioning component that is not listed in Section 3.1.5.1.1 is used, the conditioned sequential dataset (described in item 2 of Section 3.1.1) is tested using the statistical tests described in Section 5 to verify the IID assumption, and the IID assumption is verified. If any of these conditions are not met, the estimation process shall follow the non-IID track. 3.1.3 Initial Entropy Estimate The submitter shall provide an entropy estimate for the noise source outputs, which is based on the submitter’s analysis of the noise source (see Requirement 3 in Section 3.2.2). This estimate is denoted as H . submitter After determining the entropy estimation track, a min-entropy estimate per sample, denoted as H , for the sequential dataset is calculated using the methods described in Section 6.1 (for the original IID track) or Section 6.2 (for the non-IID track). If the alphabet size is greater than 256, it shall be reduced to at most 256 symbols (see Section 6.4). If the sequential dataset is not binary, an additional entropy estimation (per bit), denoted H , bitstring is estimated. First, the sequential dataset that contains L samples (each having n bits) is considered as a bitstring of size nL. The bits after the first 1 000 000 bits may be ignored. Then, the estimation is done based on the entropy estimation track, as specified in the previous paragraph, and H bitstring is calculated. Then, the entropy per sample is estimated to be n×H . bitstring The initial entropy estimate of the noise source is calculated as H = min (H , n×H , I original bitstring H ) for non-binary sources and as H = min (H , H ) for binary sources. submitter I original submitter 3.1.4 Restart Tests The entropy estimate of a noise source, calculated from a single, long-output sequence, might provide an overestimate if the noise source generates correlated sequences after restarts. Hence, an attacker with access to multiple noise source output sequences after restarts may be able to predict the next output sequence with much better success than the entropy estimate suggests. The process of restarting a noise source may be different for different noise sources (e.g., powering off, cooling off, delaying ten seconds before extracting output from the noise source, etc.). The submitter shall define the restart process suitable for the submission. This process shall simulate the restart process expected in real-world use (e.g., the outputs are not generated until after the start-up tests are complete; see Section 4.2). All restarts are expected to be done in normal operating conditions. The restart tests described in this section re-evaluate the entropy estimate for the noise source using different outputs from many restarts of the noise source. These tests are designed to ensure that: - The noise source outputs generated after a restart are drawn from the same distribution as every other output. - The distribution of samples in a restart sequence is independent of its position in the restart sequence.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 13 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B - The knowledge of other restart sequences does not offer additional advantage in predicting the next restart sequence. 3.1.4.1 Constructing Restart Data To construct restart data, the entropy source shall be restarted r = 1000 times; for each restart, c = 1000 consecutive samples shall be collected directly from the noise source. The collection of the data shall be done as soon as the entropy source is ready to produce outputs for real-world use (e.g., after start-up tests). Note that an entropy source, in its real-world use and during restart testing, may inhibit outputs for a time immediately after restarting in order to allow any transient weak behavior to pass. The output samples are stored in an r by c matrix M, where M[i][j] represents the jth sample from the ith restart. Two datasets are constructed using the matrix M: - The row dataset is constructed by concatenating the rows of the matrix M, i.e., the row dataset is M ||…|| M[c] || M ||…|| M[c] || … || M[r] ||…|| M[r][c]. - The column dataset is constructed by concatenating the columns of the matrix M, i.e., the column dataset is M ||…|| M[r] || M ||…|| M[r] || …|| M[c] ||…|| M[r][c]. 3.1.4.2 Validation Testing The restart tests check the relations between noise source samples generated after restarting the entropy source, and compare the results to the initial entropy estimate, H (see Section 3.1.3). I First, the sanity check described in Section 3.1.4.3 is performed on the matrix M. If the test fails, the validation fails and no entropy estimate is awarded. If the noise source does not fail the sanity check, then the entropy estimation methods described in Section 6.1 (for the IID track) or Section 6.2 (for the non-IID track) are performed on the row and the column datasets, based on the track of the entropy source. Let H and H be the resulting r c entropy estimates of the row and the column datasets, respectively. The entropy estimates from the row and the column datasets are expected to be close to the initial entropy estimate H. If the I minimum of H and H is less than half of H, the validation fails, and no entropy estimate is r c I awarded. Otherwise, the entropy assessment of the noise source is taken as the minimum of the row, the column and the initial estimates, i.e., min(H, H , H). r c I If the noise source does not fail the restart tests, and the entropy source does not include a conditioning component, the entropy source will be validated at H=min(H, H , H). If the entropy r c I source includes a conditioning component, the entropy assessment of the entropy source is updated as described in Section 3.1.5. 3.1.4.3 Sanity Check - Most Common Value in the Rows and Columns This test checks the frequency of the most common value in the rows and the columns of the matrix M. If this frequency is significantly greater than the expected value, given the initial entropy estimate H calculated in Section 3.1.3, the restart test fails. In this case, the validation fails no I entropy estimate is awarded.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 14 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B This sanity check is based on a binomial test, where there are two possible outcomes for each trial: the most frequent value or any other value is observed. The purpose of the test is to determine whether the most frequent value appears more than would be expected, given the initial entropy estimate, H. The probability of type I error, denoted α, is set at 0.01 over the entire sanity check, I where each of the 2000 binomial experiments3 has type I error probability of 0.000 005. Only the experiment yielding the highest count is tested. If that experiment passes the test, then the other 1999 experiments will pass as well. If any of the 2000 experiments were to fail, one of the failed experiments would be the experiment having the highest count. Therefore, it is sufficient to test the experiment with the highest count. Given the 1000 by 1000 restart matrix and the initial entropy estimate H, the test is performed as I follows: 1. Let p = . Let α be 0.000 005. −𝐻𝐻𝐼𝐼 2. For each row (1≤ i ≤ 1000) of the matrix, count the number of occurrences of each sample 2 present in the row. Set X to the highest count value for row i. Let X be the maximum count Ri R value for all the rows, i.e., X = max (X ,…, X ). R R1 R1000 3. For each column (1≤ i ≤ 1000) of the matrix, count the number of occurrences of each sample present in the column. Set X to the highest count value for column i. Let X be the maximum Ci C count value for all the columns, i.e., X = max (X ,…, X ). C C1 C1000 4. Let . 5. Calc𝑋𝑋u𝑚𝑚la𝑚𝑚t𝑚𝑚e = max(𝑋𝑋𝑐𝑐,𝑋𝑋𝑅𝑅) . If Pr( , the test fails. Otherwise, the test passe1s0.0 0 1000 𝑗𝑗 1000−𝑗𝑗 P(𝑋𝑋 ≥ 𝑋𝑋𝑚𝑚𝑚𝑚𝑚𝑚) = ∑𝑗𝑗=𝑋𝑋𝑚𝑚𝑚𝑚𝑚𝑚� 𝑗𝑗 �𝑝𝑝 (1−𝑝𝑝) 𝑋𝑋 ≥ 𝑋𝑋𝑚𝑚𝑚𝑚𝑚𝑚) < 𝛼𝛼 3.1.5 Entropy Estimation for Entropy Sources Using a Conditioning Component The optional conditioning component gets inputs from the noise source and generates the output of the entropy source. The size of the input and the output of the conditioning component in bits, denoted as n and n ,respectively, shall be fixed and shall be specified by the submitter. Noise in out source outputs are concatenated to construct n -bit input to the conditioning function. The entropy in of the input, denoted h , depends on the number of samples needed to construct the n -bit input. in in If w samples are needed, then h is estimated to be w×h bits. The size of the conditioning in component input shall be a multiple of the size of the noise source output. Since the conditioning component is deterministic, the entropy of the output is at most h . in However, the conditioning component may reduce the entropy of the output. The entropy of the output from the conditioning component is denoted as h , i.e., h bits of entropy are contained out out within the n -bit output. The entropy of the output also depends on the internals of the out conditioning components. In this Recommendation, the narrowest internal width within the 3 The experiments done for each row or column are considered to be independent.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 15 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B conditioning component is denoted as nw. A discussion on the narrowest internal width is given in Appendix E. n in bits with n out bits with Noise h in bits of entropy Conditioning h out bits of entropy Output Source Component nw Figure 3 Entropy of the Conditioning Component The optional conditioning component can be designed in various ways. Section 3.1.5.1.1 provides a list of vetted cryptographic algorithms/functions for conditioning the noise source outputs. Submitters are allowed to use other conditioning components. If a conditioning component from Section 3.1.5.1.1 is used, the entropy estimation is performed as described in Section 3.1.5.1.2; if a non-listed algorithm is used, the entropy estimation is performed as described in Section 3.1.5.2. 3.1.5.1 Using Vetted Conditioning Components Both keyed and unkeyed algorithms have been vetted for conditioning. Section 3.1.5.1.1 provides a list of vetted conditioning components. Section 3.1.5.1.2 discusses the method for determining the entropy provided by a vetted conditioning component. 3.1.5.1.1 List of Vetted Conditioning Components Three keyed algorithms have been vetted for a keyed conditioning component: 1. HMAC, as specified in FIPS 198, with any approved hash function specified in FIPS 180 or FIPS 202, 2. CMAC, as specified in SP 800-38B, with the AES block cipher (see FIPS 197), and 3. CBC-MAC, as specified in Appendix F, with the AES block cipher. This Recommendation does not approve the use of CBC-MAC for purposes other than as a conditioning component in an RBG. Three unkeyed functions have been vetted for an unkeyed conditioning component: 1. Any approved hash function specified in FIPS 180 or FIPS 202, 2. Hash_df, as specified in SP 800-90A, using any approved hash function specified in FIPS 180 or FIPS 202, and 3. Block_Cipher_df, as specified in SP800-90A using the AES block cipher (see FIPS 197). The narrowest internal width and the output length for the vetted conditioning functions are provided in the following table.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 16 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B Table 1 The narrowest internal width and output lengths of the vetted conditioning functions. Conditioning Function Narrowest Internal Width Output Length (nw) (n ) out HMAC hash-function output size hash-function output size CMAC AES block size = 128 AES block size = 128 CBC-MAC AES block size = 128 AES block size = 128 Hash Function hash-function output size hash-function output size Hash_df hash-function output size hash-function output size Block_Cipher_df AES key size AES key size For Hash_df and Block_cipher_df, the output length indicated in the table is used as the no_of_bits_to_return input parameter for the invocation of Hash_df and Block_Cipher_df (see SP 800-90A). 3.1.5.1.2 Entropy Assessment using Vetted Conditioning Components When using a conditioning component listed in Section 3.1.5.1.1 (given the assurance of correct implementation by CAVP testing), the entropy of the output is estimated as where ℎ𝑜𝑜𝑜𝑜𝑜𝑜 = Output_E ins tdreospcyri(b𝑛𝑛e𝑖𝑖d𝑖𝑖 ,a𝑛𝑛s 𝑜𝑜f𝑜𝑜o𝑜𝑜l,l𝑛𝑛ow𝑛𝑛s,4ℎ:𝑖𝑖 𝑖𝑖) Output_Entropy(𝑛𝑛𝑖𝑖𝑖𝑖,𝑛𝑛𝑜𝑜𝑜𝑜𝑜𝑜,𝑛𝑛𝑛𝑛,ℎ𝑖𝑖𝑖𝑖) 1. Let and . (1−𝑃𝑃ℎ𝑖𝑖𝑖𝑖ℎ) 2. −ℎ𝑖𝑖𝑖𝑖 𝑃𝑃ℎ𝑖𝑖𝑖𝑖ℎ = 2 𝑃𝑃𝑙𝑙𝑜𝑜𝑙𝑙 = 2𝑖𝑖𝑖𝑖𝑖𝑖 −1 3. + 𝑛𝑛 = min(𝑛𝑛𝑜𝑜𝑜𝑜𝑜𝑜,𝑛𝑛𝑛𝑛). 4. U = 𝑖𝑖𝑖𝑖𝑖𝑖−𝑖𝑖 𝜓𝜓 = 2 𝑃𝑃𝑙𝑙𝑜𝑜𝑙𝑙 𝑃𝑃ℎ𝑖𝑖𝑖𝑖ℎ 5. 𝑖𝑖𝑖𝑖𝑖𝑖−𝑖𝑖 𝑖𝑖𝑖𝑖𝑖𝑖−𝑖𝑖 6. Retu2rn +�2 𝑛𝑛(2 ) ln(2) 𝜔𝜔 = 𝑈𝑈 ×𝑃𝑃𝑙𝑙𝑜𝑜𝑙𝑙 − log2(max(𝜓𝜓,𝜔𝜔)) The entropy source will be assessed at the min-entropy per conditioned output, h , computed out above. Vetted conditioning components are permitted to claim full entropy outputs. 4 The formula used to generate Output_Entropy() is adapted from the formula provided in Theorem 1 of 𝑚𝑚 𝑚𝑚 [RaSt98], such that m is equal to , n is equal to and is equal to 1. 𝑘𝑘𝛼𝛼 = 𝑖𝑖+𝛼𝛼�2𝑖𝑖ln𝑛𝑛 𝑖𝑖𝑖𝑖𝑖𝑖 𝑖𝑖 2 2 𝛼𝛼NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 17 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B Note that it is acceptable to truncate the outputs from a vetted conditioning component. If this is done, the entropy estimate is reduced to a proportion of the output (e.g., if there are six bits of entropy in an eight-bit output and the output is truncated to six bits, then the entropy is reduced to 3/4 x 6 = 4.5 bits). When additional noise sources are available, the length of the input ( ) shall only include the inputs from the primary noise source. 𝑛𝑛𝑖𝑖𝑖𝑖 3.1.5.2 Using Non-vetted Conditioning Components For non-vetted conditioning components, the entropy in the output depends on the entropy and size of the input (h and n ), the size of the output (n ), and the size of the narrowest internal in in out width (nw) and the entropy of the conditioned sequential dataset (as described in item 2 of Section 3.1.1), which shall be computed using the methods described in either Section 6.1 (for IID data) or Section 6.2 (for non-IID data). Let the obtained entropy estimate per bit be h'. The output of the conditioning component (n ) shall be treated as a binary string, for purposes of out the entropy estimation. The entropy of the conditioned output is estimated as . ′ The descriptiℎo𝑜𝑜n𝑜𝑜 𝑜𝑜of= min(Output_En itsr ogpivye(n𝑛𝑛 𝑖𝑖i𝑖𝑖n, 𝑛𝑛Se𝑜𝑜𝑜𝑜ct𝑜𝑜i,o𝑛𝑛n𝑛𝑛 3,.ℎ1𝑖𝑖.𝑖𝑖5).1,0.2..9 T99o na𝑜𝑜v𝑜𝑜o𝑜𝑜i,dℎ a×pp𝑛𝑛r𝑜𝑜o𝑜𝑜v𝑜𝑜i)ng an entropy source having a non-vetted conditioning component with full entropy, is multiplied by the constant 0.999. TheO uentptruotp_yE nsotruorcpey will be validated at the min-entropy per conditioned output, h , computed above. 𝑛𝑛𝑜𝑜𝑜𝑜𝑜𝑜 out Note that truncating subsequent to the use of a non-vetted conditioning component shall not be performed before providing output from the entropy source. 3.1.6 Additional Noise Sources In this Recommendation, it is assumed that the entropy sources have a unique primary noise source that is responsible to generate randomness. It should be noted that multiple copies of the same physical noise source are considered as a single noise source (e.g., a source with eight ring oscillators, where the sampled bits are concatenated to get an eight-bit output, or where the samples bits are XORed together). In addition to the primary noise source outputs, outputs of other noise sources may be available to the entropy source, and their outputs may be used to increase security. However, the joint entropy of these outputs may be hard to estimate, especially when there are dependencies between the sources (e.g., packet arrival times in a communication network and hard drive access times). This Recommendation allows one to concatenate the outputs of the additional noise sources to the primary noise source to generate input to the conditioning component. In such cases, vetted conditioning components shall be used. No entropy is credited from the outputs of the additional noise sources.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 18 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 3.2 Requirements for Validation Testing In this section, high-level requirements (on both submitters and testers) are presented for validation testing. 3.2.1 Requirements on the Entropy Source The intent of these requirements is to assist the developer in designing/implementing an entropy source that can provide outputs with a consistent amount of entropy and to produce the required documentation for entropy source validation. 1. The entire design of the entropy source shall be documented, including the interaction of the components specified in Section 2.2. The documentation shall justify why the entropy source can be relied upon to produce bits with entropy. 2. Documentation shall describe the operation of the entropy source, including how the entropy source works, and how to obtain data from within the entropy source for validation testing. 3. Documentation shall describe the range of operating conditions (e.g., temperature range, voltages, system activity, etc.) under which the entropy source is claimed to operate correctly. The entropy source outputs are expected to have similar entropy rates in this specified range of operating conditions. 4. The entropy source shall have a well-defined (conceptual) security boundary. This security boundary shall be documented; the documentation shall include a description of the content of the security boundary. 5. When a conditioning component is not used, the output from the entropy source is the output of the noise source, and no additional interface is required. In this case, the noise- source output is available during both validation testing and normal operation. When a conditioning component is included in the entropy source, the output from the entropy source is the output of the conditioning component, and an additional interface is required to access the noise-source output. In this case, the noise-source output shall be accessible via the interface during validation testing, but the interface may be disabled otherwise. The designer shall fully document the method used to get access to the raw noise source samples. If the noise-source interface is not disabled during normal operation, any noise-source output using this interface shall not be provided to the conditioning component for processing and eventual output as normal entropy-source output. 6. The entropy source may restrict access to raw noise source samples to special circumstances that are not available to users in the field, and the documentation shall explain why this restriction is not expected to substantially alter the behavior of the entropy source as tested during validation. 7. Documentation shall contain a description of the restarting process applied during the restart tests. 3.2.2 Requirements on the Noise Source The entropy source will have no more entropy than that provided by the noise source, and as such, the noise source requires special attention during validation testing. This is partly due to the fundamental importance of the noise source (if it does not do its job, the entropy source will notNIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 19 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B provide the expected amount of security), and partly because the probabilistic nature of its behavior requires more complicated testing. The requirements for the noise source are as follows: 1. The operation of the noise source shall be documented; this documentation shall include a description of how the noise source works, where the unpredictability comes from, and rationale for why the noise source provides acceptable entropy output, and should reference relevant, existing research and literature. 2. The behavior of the noise source shall be stationary (i.e., the probability distributions of the noise source outputs do not change when shifted in time). Documentation shall include why it is believed that the entropy rate does not change significantly during normal operation. This can be in broad terms of where the unpredictability comes from and a rough description of the behavior of the noise source (to show that it is reasonable to assume that the behavior is stationary). 3. Documentation shall provide an explicit statement of the expected entropy provided by the noise source outputs and provide a technical argument for why the noise source can support that entropy rate. To support this, documentation may include a stochastic model of the noise source outputs, and an entropy estimation based on this stochastic model may be included. 4. The noise source state shall be protected from adversarial knowledge or influence to the greatest extent possible. The methods used for this shall be documented, including a description of the (conceptual) security boundary’s role in protecting the noise source from adversarial observation or influence. 5. Although the noise source is not required to produce unbiased and independent outputs, it shall exhibit random behavior; i.e., the output shall not be definable by any known algorithmic rule. Documentation shall indicate whether the noise source produces IID data or non-IID data. This claim will be used in determining the test path followed during validation. If the submitter makes an IID claim, documentation shall include rationale for the claim. 6. The noise source shall generate fixed-length bitstrings. A description of the output space of the noise source shall be provided. Documentation shall specify the fixed symbol size (in bits) and the list (or range) of all possible outputs from each noise source. 7. If additional noise source outputs to increase security are used, a document that describes the additional noise sources shall be included. 3.2.3 Requirements on the Conditioning Component The requirements for the conditioning component are as follows: 1. The submitter shall document which conditioning component is used and the details about its implementation (e.g., the hash function and/or key size used). Documentation shall include the input and the output sizes (n and n ). in outNIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 20 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 2. If the entropy source uses a vetted conditioning component as listed in Section 3.1.5.1.1, the implementation of the component shall be tested to obtain assurance of correctness before subsequent testing of the entropy source. The submitter shall specify any keys used to test the correctness of the conditioning component implementation during validation testing. If the testing fails, validation of the entropy source fails. The submitter may retest with the corrected implementation until the conditioning component passes the validation test. 3. If the conditioning component uses cryptographic keys, the keys may be (1) fixed to a pre- determined value, (2) set using some additional input to the device, or (3) generated by using the noise source outputs. The key shall be determined before any outputs are generated from the conditioning component. 4. Any value which is used to determine the key shall not be used as any other input to the conditioning component. The input entropy to the conditioning component (h ) shall not in include any entropy provided to the key of a keyed function. 5. For entropy sources containing a conditioning component that is not listed in Section 3.1.5.1.1, a description of the conditioning component shall be provided. Documentation shall state the narrowest internal width and the size of the output blocks from the conditioning component. The submitter shall provide mathematical evidence that the component is suitable to be used to condition the noise source output, and does not significantly reduce the entropy rate of the entropy source output. The submitter shall also provide a justification about why the conditioning component does not act poorly when the noise source data is not independent. 3.2.4 Requirements on Data Collection The requirements on data collection are listed below: 1. The data collection for entropy estimation shall be performed in one of the three ways described below: • By the submitter with a witness from the testing lab, or • By the testing lab itself, or • Prepared by the submitter in advance of testing, along with the following documentation: a specification of the data generation process, and a signed document that attests that the specification was followed. 2. Data collected from the noise source for validation testing shall be raw output values. 3. The data collection process shall not require a detailed knowledge of the noise source or intrusive actions that may alter the behavior of the noise source (e.g., drilling into the device). 4. Data shall be collected from the noise source and any conditioning component that is not listed in Section 3.1.5.1.1 (if used) under normal operating conditions. 5. Data shall be collected from the entropy source under validation. Any relevant version of the hardware or software updates shall be associated with the data. 6. Documentation of the data collection method shall be provided so that a lab or submitter can perform (or replicate) the collection process at a later time, if necessary.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 21 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 7. Documentation explaining why the data collection method does not interfere with the noise source shall be provided.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 22 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 4 Health Tests Health tests are an important component of the entropy source, as they aim to detect deviations from the intended behavior of the noise source as quickly as possible and with a high probability. Noise sources can be fragile, and hence, can be affected by changes in the operating conditions of the device, such as the temperature, humidity, or electric field, which might result in unexpected behavior. The health tests take the entropy assessment as input5, and characterize the expected behavior of the noise source based on this value. Requirements on the health tests are listed in Section 4.3. 4.1 Health Test Overview The health testing of a noise source is likely to be very technology-specific. Since, in most cases, the noise source will not produce unbiased, independent binary data, traditional statistical procedures (e.g., the randomness tests described in NIST SP 800-22) that test the hypothesis of unbiased, independent bits will almost always fail, and thus are not useful for monitoring the noise source. In general, tests on the noise source need to be tailored carefully, taking into account the expected statistical behavior of the correctly operating noise source. The health testing of noise sources will typically be designed to detect failures of the noise source, based on the expected output during a failure, or to detect a deviation from the expected output during the correct operation of the noise source. Health tests are expected to raise an alarm in three cases: 1. When there is a significant decrease in the entropy of the outputs, 2. When noise source failures occur, or 3. When hardware fails, and implementations do not work correctly. 4.2 Types of Health Tests Health tests are applied to the outputs of a noise source before any conditioning is done. (It is permissible to also apply some health tests to conditioned outputs, but this is not required.) Start-up health tests are designed to be performed after powering up, or rebooting, and before the first use of the entropy source. They provide some assurance that the entropy source components are working as expected before they are used during normal operating conditions, and that nothing has failed since the last time that the start-up tests were run.6 The samples drawn from the noise source during the startup tests shall not be available for normal operations until the tests are completed; these samples may be discarded at any time, or may be used after the completion of the tests if there are no errors. 5 The submitter may claim a low entropy estimate (as described in Section 3.1.3) to reduce the false positive rates. 6 The specific conditions in which the startup tests must be run for FIPS-validated cryptographic modules are determined by the requirements of FIPS 140. This document imposes no additional requirements for the use of start-up health testing.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 23 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B Continuous health tests are run indefinitely on the outputs of the noise source7 while the noise source is operating. Continuous tests focus on the noise source behavior and aim to detect failures as the noise source produces outputs. The purpose of continuous tests is to allow the entropy source to detect many kinds of failures in its underlying noise source. These tests are run continuously on all digitized samples obtained from the noise source, and so tests must have a very low probability of raising a false alarm during the normal operation of the noise source. In many systems, a reasonable false positive probability will make it extremely unlikely that a properly functioning device will indicate a malfunction, even in a very long service life. Continuous tests are resource- constrained − this limits their ability to detect noise source failures − so they are usually designed so that only gross failures are likely to be detected. Note that continuous health tests operate over a stream of values. These sample values may be output from the entropy source as they are generated and (optionally) processed by a conditioning component; there is no need to inhibit output from the noise source or entropy source while running the test. It is important to understand that this may result in poor entropy source outputs for a time, since the error is only signaled once significant evidence has been accumulated, and these values may have already been output by the entropy source. As a result, it is important that the false positive probability be set to an acceptable level. In the following discussion, all calculations assume that a false positive probability of approximately one error in 220 samples generated by the noise source is acceptable; however, the formulas given can be adapted for different false positive probabilities selected by the submitter. On-demand health tests can be called at any time. This Recommendation does not require performing testing during operation. However, it does require that the entropy source be capable of performing on-demand health tests of the noise source output. Note that resetting, rebooting, or powering up are acceptable methods for initiating an on-demand test if the procedure results in the immediate execution of the start-up tests. Samples collected from the noise source during on- demand health tests shall not be available for use until the tests are completed, however these samples may be discarded at any time, or may be used after the completion of the tests providing that there are no errors. 4.3 Requirements for Health Tests Health tests on the noise source are a required component of an entropy source. The health tests shall include both continuous and start-up tests. 1. The continuous tests shall include either: a. The approved continuous health tests, described in Section 4.4, or b. Some developer-defined tests that meet the requirements for a substitution of those approved tests, as described in Section 4.5. If developer-defined health tests are used in place of any of the approved health tests, the tester shall verify that the implemented tests detect the failure conditions detected by the approved continuous health tests, as described in Section 4.4. The need to use the two approved continuous health tests can be avoided by providing convincing evidence that the failure being considered will be reliably 7 Entropy sources may have a warm-up phase in which the outputs are inhibited for a time immediately after startup. Continuous health testing is not required during the warm up phase.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 24 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B detected by the developer-defined continuous tests. This evidence may be a proof or the results of statistical simulations. c. The continuous tests may include additional tests defined by the developer. 2. When the health tests fail, the entropy source shall notify the consuming application (e.g., the RBG) of the error condition. The developer may have defined different types of failures (e.g., intermittent and persistent), and the application is allowed to react differently to different types of failures (e.g., by inhibiting output for a short time). The developer is allowed to define different cutoff values to detect intermittent and persistent failures. If so, these values (with corresponding false alarm probabilities) shall be specified in the submission documentation. If the entropy source detects intermittent failures and allows the noise source to return to normal functioning, the designer shall provide evidence that: a) The intermittent failures handled in this way are indeed extremely likely to be intermittent failures; and b) the tests will detect a permanent failure when one occurs, and will ultimately signal an error condition to the consuming application and cease operation. In the case where a persistent failure is detected, the entropy source shall not produce any outputs. The module may support being reset or returned to operation by the consuming application or system. (An example of a situation where this would make sense is a remote system whose cryptographic module cannot be replaced quickly, but which must continue functioning.) 3. The optimal value for the false positive probability may depend on the rate that the entropy source produces its outputs. For the approved continuous health tests, the false positive probability8 is recommended to be between 2−20 and 2−40. Lower probability values are acceptable. The submitter shall specify and document a false positive probability suitable for their application. 4. The entropy source's startup tests shall run the continuous health tests over at least 1024 consecutive samples. The startup tests may include other tests defined by the developer. The samples subjected to startup testing may be released for operational use after the startup tests have been passed, or may be discarded at any time. 5. The entropy source shall support on-demand testing. The on-demand tests shall include at least the same testing done by the start-up tests. The entropy source may support on-demand testing by restarting the entropy source and rerunning the startup tests, or by rerunning the startup tests without restarting the entropy source. The documentation shall specify the approach used for on-demand testing. The on-demand tests may include other tests defined by the developer, in addition to the testing done in the start-up tests. 6. Health tests shall be performed on the noise source samples before any conditioning is done. Additional health tests may be performed on the outputs of the conditioning function. 7. The submitter shall provide documentation that specifies all entropy source health tests and their rationale. The documentation shall include a description of the health tests, source code, the rate and conditions under which each health test is performed (e.g., at power-up, 8 Having a high false positive probability and discarding the outputs when the test raises an alarm may result in a reduction in the entropy of the outputs. For the recommended range, the loss can be considered negligible.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 25 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B continuously, or on-demand), and include rationale indicating why each test is believed to be appropriate for detecting one or more failures in the noise source. 8. The submitter shall provide documentation of any known or suspected noise source failure modes (e.g., the noise source starts producing periodic outputs like 101…01), and shall include developer-defined continuous tests to detect those failures. These should include potential failure modes that might be caused by an attack on the device. 9. Appropriate health tests that are tailored to the noise source should place special emphasis on the detection of misbehavior near the boundary between the nominal operating environment and abnormal conditions. This requires a thorough understanding of the operation of the noise source. 4.4 Approved Continuous Health Tests This recommendation provides two approved health tests: the Repetition Count test, and the Adaptive Proportion test. If these two health tests are included among the continuous health tests of the entropy source, no other tests are required. However, the developer is advised to include additional continuous health tests tailored to the noise source. Both tests are designed to require minimal resources, and to be computed on-the-fly while noise source samples are being produced, possibly conditioned, and output by the entropy source. Neither test needs to delay the availability of the noise source samples. Like all statistical tests, both of these tests have a false positive probability – the probability that a correctly functioning noise source will fail the test on a given output. In many applications, a reasonable choice for the probability of type I error is α = 2−20; this value will be used in all the calculations in the rest of this section. The developer of the entropy source shall determine a reasonable probability of type I error (and corresponding cutoff values), based the details of the entropy source and its consuming application. 4.4.1 Repetition Count Test The goal of the Repetition Count Test is to quickly detect catastrophic failures that cause the noise source to become "stuck" on a single output value for a long period of time. It can be considered as an update of the "stuck test" that was previously required for random number generators within FIPS-approved cryptographic modules. Note that this test is intended to detect a total failure of the noise source. Given the assessed min-entropy H of a noise source, the probability9 of that source generating n identical samples consecutively is at most 2−H(n-1). The test declares an error if a sample is repeated C or more times. The cutoff value C is determined by the acceptable false-positive probability α and the entropy estimate H using the following formula 9 This probability can be obtained as follows. Let a random variable take possible values with probabilities pi, for i=1,..,k, where p1≥p2≥…≥pk . Then, the probability of producing any C identical consecutive samples is ∑ piC. Since, ∑ piC is less than or equal to p1. p1C−1+ p2.p1C−1+…+ pk.p1C−1= (p1+…+pk) p1C−1 = p1C-1 = 2−H(C−1).NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 26 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B −log α C = 1+ 2 .  H  This value of C is the smallest integer satisfying the inequality α ≥2−H (C−1), which ensures that the probability of obtaining a sequence of identical values from C consecutive noise source samples is at most α. For example, for α = 2−20, an entropy source with H = 2.0 bits per sample would have a repetition count test cutoff value of 1+20/2.0 = 11. Let next() yield the next sample from the noise source. Given a continuous sequence of noise source samples, and the cutoff value C, the repetition count test is performed as follows: 1. A=next() 2. B =1 3. X=next() 4. If (X = A), B=B+1 If (B ≥ C), signal a failure. else: A=X B=1 5. Repeat Step 3. This test's cutoff value can be applied to any entropy estimate, H, including very small and very large estimates. However, it is important to note that this test is not very powerful – it is able to detect only catastrophic failures of a noise source. For example, a noise source evaluated at eight bits of min-entropy per sample has a cutoff value of six repetitions to ensure a false-positive rate of approximately once per 1012 samples generated. If that noise source somehow failed to the point that each sample had a 1/16 probability of being the same as the previous sample, so that it was providing only four bits of min-entropy per sample, it would still be expected to take about one million samples before the repetition count test would notice the problem. 4.4.2 Adaptive Proportion Test The Adaptive Proportion Test is designed to detect a large loss of entropy that might occur as a result of some physical failure or environmental change affecting the noise source. The test continuously measures the local frequency of occurrence of a sample value in a sequence of noise source samples to determine if the sample occurs too frequently. Thus, the test is able to detect when some value begins to occur much more frequently than expected, given the source's assessed entropy per sample. Note that this test is intended to detect more subtle failures of the noise source, rather than the kind of total failure detected by the Repetition Count Test. The test takes a sample from the noise source, and then counts the number of times that the same value occurs within the next W-1 samples. If that count reaches the cutoff value C, the test declares an error. The window size W is selected based on the alphabet size, and shall be assigned to 1024 if the noise source is binary (that is, the noise source produces only two distinct values) and 512 if the noise source is not binary (that is, the noise source produces more than two distinct values).NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 27 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B Let next() yield the next sample from the noise source. Given a continuous sequence of noise samples, the cutoff value C and the window size W, the adaptive proportion test is performed as follows: 1. A= next() 2. B=1. 3. For i = 1 to W–1 If (A = next()) B=B+1 If (B ≥ C) signal a failure 4. Go to Step 1. The cutoff value C is chosen such that the probability of observing C or more identical samples in a window size of W is at most α. Mathematically, C satisfies the following equation10: Pr (B ≥ C) ≤ α . For binary sources, the developer is allowed to extend the test by also checking that W-B ≥ C, which would guarantee that a binary value occurring too frequently will be caught on the first test window. For noise sources where the alphabet size is large (e.g., greater than 256), the submitter may reduce the alphabet size to a lower value, using the method described in Section 6.4. The following table gives example cutoff values for various min-entropy estimates per sample and window sizes with α = 2−20. Table 2 Example cutoff values of the Adaptive Proportion Test Binary data Non-binary data W=1024 W=512 Cutoff Cutoff Entropy Entropy Value C Value C 0.2 941 0.5 410 0.4 840 1 311 0.6 748 2 177 0.8 664 4 62 1 589 8 13 10 This probability can be computed using widely-available spreadsheet applications. In Microsoft Excel, Open Office Calc,and iWork Numbers, the calculation is done with the function =CRITBINOM(). For example, in Microsoft Excel, C would be computed as =1+CRITBINOM(W, power(2,( −H)),1−α).NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 28 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 4.5 Developer-Defined Alternatives to the Continuous Health Tests Developer-defined tests are always permitted in addition to the two approved tests listed in Section 4.4. Under some circumstances, the developer-defined tests may take the place of the two approved tests. The goal of the two approved continuous health tests specified in Section 4.4, is to detect two conditions: a. Some value is consecutively repeated many more times than expected, given the assessed entropy per sample of the source. b. Some value becomes much more common in the sequence of noise source outputs than expected, given the assessed entropy per sample of the source. The developer of the entropy source is in an excellent position to design health tests specific to the source and its known and suspected failure modes. Therefore, this Recommendation also permits developer-defined alternative health tests to be used in place of the approved tests in Section 4.4, so long as the combination of the developer-defined tests and the entropy source itself can guarantee that these two conditions will not occur without being detected by the entropy source with at least the same probability. For concreteness, these are the criteria that are required for any alternative continuous health tests: a. If a single value appears more than 100/H consecutive times in a row in the sequence of noise source samples, the test shall detect this with a probability of at least 99 %. b. Let P = 2−H. If the noise source's behavior changes so that the probability of observing a specific sample value increases to at least P* = 2−H/2, then the test shall detect this change with a probability of at least 50 % when examining 50 000 consecutive samples from this degraded source. The use of one or more of the approved continuous health test described in Section 4.4 can be avoided by providing convincing evidence that the failure being considered will be reliably detected by the developer-defined continuous tests. This evidence may be a proof or the results of statistical simulations.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 29 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 5 Testing the IID Assumption The samples from a noise source are independent and identically distributed (IID) if each sample has the same probability distribution as every other sample, and all samples are mutually independent. The IID assumption significantly simplifies the process of entropy estimation. When the IID assumption does not hold, i.e., the samples are either not identically distributed or are not independently distributed (or both), estimating entropy is more difficult and requires different methods. This section includes statistical tests that are designed to find evidence that the samples are not IID and if no evidence is found that the samples are non-IID, then it is assumed that the samples are IID (see Section 3.1.2). These tests take the sequence S = (s ,…,s ), where s ϵ A = {x ,…,x }, as 1 L i 1 k input, and test the hypothesis that the values in S are IID. If the hypothesis is rejected by any of the tests, the values in S are assumed to be non-IID. Statistical tests based on permutation testing (also known as shuffling tests) are given in Section 5.1. Five additional chi-square tests are presented in Section 5.2. 5.1 Permutation Testing Permutation testing is a way to test a statistical hypothesis in which the actual value of the test statistic is compared to a reference distribution that is inferred from the input data, rather than a standard statistical distribution. The general approach of permutation testing is to generate 10 000 permutations of the dataset, computing a test statistic for each permutation and comparing the result with a test statistic computed on the original dataset; the process is listed in Figure 4. This is repeated for each of the test statistics described in Sections 5.1.1 – 5.1.11. The shuffle algorithm of step 2.1 is provided in Figure 5. Input: S = (s ,…, s ) 1 L Output: Decision on the IID assumption 1. For each test i 1.1. Assign the counters C and C to zero. i,0 i,1 1.2. Calculate the test statistic T on S. i 2. For j = 1 to 10 000 2.1. Permute S using the Fisher-Yates shuffle algorithm. 2.2. For each test i 2.2.1. Calculate the test statistic T on the permuted data. 2.2.2. If (T > T), increment C . If (T = T), increment C . i i,0 i i,1 3. If ((C +C ≤5) or (C ≥ 9995)) for any i, reject the IID assumption; else, assume that i,0 i,1 i,0 the noise source outputs are IID. Figure 4 Generic Structure for Permutation TestingNIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 30 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B If the samples are IID, permuting the dataset is not expected to change the value of the test statistics significantly. In particular, the original dataset and permuted datasets are expected to be drawn from the same distribution; therefore, their test statistics should be similar. Unusually high or low test statistics are expected to occur infrequently. However, if the samples are not IID, then the original and permuted test statistics may be significantly different. The counters C and C are i,0 i,1 used to find the ranking of the original test statistics among the permuted test statistics (i.e., where a statistic for the original dataset fits within an ordered list of the permuted datasets). Extreme values for the counters suggest that the data samples are not IID. If the sum of C and C is less i,0 i,1 than 5, it means that the original test statistic has a very high rank; conversely, if C is greater than i,0 9995, it means that the original test statistics has a very low rank. The cutoff values for C and i,0 C are calculated using a type I error probability of 0.001. i,1 The tests described in the following subsections are intended to check the validity of the IID assumption. Some of the tests (e.g., the compression test) are effective at detecting repeated patterns of particular values (for example, strings of sample values that occur more often than would be expected by chance if the samples were IID), whereas some of the other tests (e.g., the number of directional runs test and the runs based on the median test) focus on the association between the numeric values of the successive samples in order to find an indication of a trend or some other relation, such as high sample values that are usually followed by low sample values. Input: S = (s ,…, s ) 1 L Output: Shuffled S = (s ,…, s ) 1 L 1. for i from L downto 1 do a. Generate a random integer j such that 1≤ j ≤ i. b. Swap s and s j i Figure 5 Pseudo-code of the Fisher-Yates Shuffle The tests are applicable to both binary and non-binary data, but for some of the tests, the number of distinct sample values, denoted k (the size of the set A), significantly affects the distribution of the test statistics, and thus the type I error. For such tests, one of the following conversions is applied to the input data, when the input is binary, i.e., k = 2. • Conversion I partitions the sequences into eight-bit non-overlapping blocks, and counts the number of ones in each block. Zeroes are appended when the last block has less than eight bits. For example, let the 20-bit input be (1,0,0,0,1,1,1,0,1,1,0,1,1,0,1,1,0,0,1,1). The first and the second eight-bit blocks include four and six ones, respectively. The last block, which is not complete, includes two ones. The output sequence is (4, 6, 2). • Conversion II partitions the sequences into eight-bit non-overlapping blocks, and calculates the integer value of each block. For example, let the input message be (1,0,0,0,1,1,1,0,1,1,0,1,1,0,1,1,0,0,1,1). The integer values of the first two blocks are 142, and 219. Zeroes are appended when the last block has less than eight bits. Then, the last block becomes (0,0,1,1,0,0,0,0) with an integer value of 48. The output sequence is (142, 219, 48).NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 31 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B Descriptions of the individual tests will provide guidance on when to use each of these conversions. 5.1.1 Excursion Test Statistic The excursion test statistic measures how far the running sum of sample values deviates from its average value at each point in the dataset. Given S = (s ,…, s ), the test statistic T is the largest 1 L deviation from the average and is calculated as follows: 1. Calculate the average of the sample values, i.e., = (s + s + … + s ) / L 1 2 L 2. For i = 1 to L 𝑋𝑋� Calculate d = | |. i 𝑖𝑖 3. T = max (d 1,…, d L). ∑𝑗𝑗=1𝑠𝑠𝑗𝑗 −𝑖𝑖 × 𝑋𝑋� Example 1: Let the input sequence be S = (2, 15, 4, 10, 9). The average of the sample values is 8, and d = |2–8| = 6; d =|(2+15) – (2×8)| = 1; d = |(2+15+4) – (3×8)| = 3; d = |(2+15+4+10) – 1 2 3 4 (4×8)| = 1; and d = |(2+15+4+10+9) – (5×8)| = 0. Then, T = max(6, 1, 3, 1, 0) = 6. 5 Handling binary data: The test can be applied to binary data, and no additional conversion steps are required. 5.1.2 Number of Directional Runs This test statistic determines the number of runs constructed using the relations between consecutive samples. Given S = (s , …, s ), the test statistic T is calculated as follows: 1 L 1. Construct the sequence = ( ,…, ), where ′ ′ ′ 𝑆𝑆 𝑠𝑠1 𝑠𝑠𝐿𝐿−1 ′ −1, if 𝑠𝑠𝑖𝑖 > 𝑠𝑠𝑖𝑖+1 𝑠𝑠𝑖𝑖 = � for i = 1, …, L–1. +1, if 𝑠𝑠𝑖𝑖 ≤ 𝑠𝑠𝑖𝑖+1 2. The test statistic T is the number of runs in . ′ Example 2: Let the input sequence be S = (2, 2, 2, 5, 7, 7, 9, 3, 1, 4, 4); then = (+1, +1, +1, +1, 𝑆𝑆 +1, +1, −1, −1, +1, +1). There are three runs: (+1, +1, +1, +1, +1, +1), (−1, −1)′ and (+1, +1), so T = 3. 𝑆𝑆 Handling binary data: To test binary input data, first apply Conversion I to the input sequence. 5.1.3 Length of Directional Runs This test statistic determines the length of the longest run constructed using the relations between consecutive samples. Given S = (s ,…, s ), the test statistic T is calculated as follows: 1 L 1. Construct the sequence = ( ), where ′ ′ ′ 𝑆𝑆 𝑠𝑠1,…,𝑠𝑠𝐿𝐿−1NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 32 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B ′ −1, if 𝑠𝑠𝑖𝑖 > 𝑠𝑠𝑖𝑖+1 𝑠𝑠𝑖𝑖 = � for i =1, …, L-1. +1, if 𝑠𝑠𝑖𝑖 ≤ 𝑠𝑠𝑖𝑖+1 2. The test statistic T is the length of the longest run in . ′ Example 3: Let the input sequence be S = (2, 2, 2, 5, 7, 7, 9, 3, 1, 4, 4); then = (+1, +1, +1, +1, 𝑆𝑆 +1, +1, −1, −1, +1, +1). There are three runs: (+1, +1, +1, +1, +1, +1), (−1, −1)′ and (+1, +1), so T = 6. S Handling binary data: To test binary input data, first apply Conversion I to the input sequence. 5.1.4 Number of Increases and Decreases This test statistic determines the maximum number of increases or decreases between consecutive sample values. Given S = (s ,…, s ), the test statistic T is calculated as follows: 1 L 1. Construct the sequence = ( ), where ′ ′ ′ 𝑆𝑆 𝑠𝑠1,…,𝑠𝑠𝐿𝐿−1 ′ −1, if 𝑠𝑠𝑖𝑖 > 𝑠𝑠𝑖𝑖+1 𝑠𝑠𝑖𝑖 = � for i = 1, …, L-1. +1, if 𝑠𝑠𝑖𝑖 ≤ 𝑠𝑠𝑖𝑖+1 2. Calculate the number of –1’s and +1’s in ; the test statistic T is the maximum of these numbers, i.e., T = max (number of –1’s, num′ber of +1’s). 𝑆𝑆 Example 4: Let the input sequence be S = (2, 2, 2, 5, 7, 7, 9, 3, 1, 4, 4); then = (+1, +1, +1, +1, +1, +1, −1, −1, +1, +1). There are eight +1’s and two −1’s in , so T = ma′x (number of +1s, number of −1s) = max (8, 2) = 8. ′ 𝑆𝑆 S Handling binary data: To test binary input data, first apply the Conversion I to the input sequence. 5.1.5 Number of Runs Based on the Median This test statistic determines the number of runs that are constructed with respect to the median of the input data. Given S = (s , …, s ), the test statistic T is calculated as follows: 1 L 1. Find the median of S = (s , …, s ). 1 L 2. Construct the sequence = ( ) where 𝑋𝑋� ′ ′ ′ 𝑆𝑆 𝑠𝑠1,…,𝑠𝑠𝐿𝐿 ′ −1, if 𝑠𝑠𝑖𝑖 < 𝑋𝑋� 𝑠𝑠𝑖𝑖 = � for i =1, …, L. +1, if 𝑠𝑠𝑖𝑖 ≥ 𝑋𝑋� 3. The test statistic T is the number of runs in . ′ 𝑆𝑆NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 33 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B Example 5: Let the input sequence be S = (5, 15, 12, 1, 13, 9, 4). The median of the input sequence is 9. Then, = (–1, +1, +1, –1, +1, +1, –1). The runs are (–1), (+1, +1), (–1), (+1, +1), and (–1). There are fiv′e runs, hence T = 5. 𝑆𝑆 Handling binary data: When the input data is binary, the median of the input data is assumed to be 0.5. No additional conversion steps are required. 5.1.6 Length of Runs Based on Median This test statistic determines the length of the longest run that is constructed with respect to the median of the input data and is calculated as follows: 1. Find the median of S = (s , …, s ). 1 L 2. Construct a temporary sequence = ( ) from the input sequence S = (s , …, s ), as 𝑋𝑋� 1 L ′ ′ ′ 𝑆𝑆 𝑠𝑠1,…,𝑠𝑠𝐿𝐿 ′ −1, if 𝑠𝑠𝑖𝑖 < 𝑋𝑋� 𝑠𝑠𝑖𝑖 = � for i = 1, …, L. +1, if 𝑠𝑠𝑖𝑖 ≥ 𝑋𝑋� 3. The test statistic T is the length of the longest run in . ′ Example 6: Let the input sequence be S = (5, 15, 12, 1, 13, 9, 4). The median for this data subset 𝑆𝑆 is 9. Then, S ' = (–1, +1, +1, –1, +1, +1, –1). The runs are (–1), (+1, +1), (–1), (+1, +1), and (–1). The longest run has a length of 2; hence, T =2. Handling binary data: When the input data is binary, the median of the input data is assumed to be 0.5. No additional conversion steps are required. 5.1.7 Average Collision Test Statistic The average collision test statistic counts the number of successive sample values until a duplicate is found. The average collision test statistic is calculated as follows: 1. Let C be a list of the number of the samples observed to find two occurrences of the same value in the input sequence S = (s , …, s ). C is initially empty. 1 L 2. Let i = 1. 3. While i < L a. Find the smallest j such that (s, …, s ) contains two identical values. If no such j i i+j-1 exists, break out of the while loop. b. Add j to the list C. c. i = i + j. 4. The test statistic T is the average of all values in the list C. Example 7: Let the input sequence be S = (2, 1, 1, 2, 0, 1, 0, 1, 1, 2). The first collision occurs for j = 3, since the second and third values are the same. 3 is added to the list C. Then, the first threeNIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 34 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B samples are discarded, and the next sequence to be examined is (2, 0, 1, 0, 1, 1, 2). The collision occurs for j = 4. The third sequence to be examined is (1,1,2), and the collision occurs for j = 2. There are no collisions in the final sequence (2). Hence, C = [3,4,2]. The average of the values in C is T = 3. Handling binary data: To test binary input data, first apply Conversion II to the input sequence. 5.1.8 Maximum Collision Test Statistic The maximum collision test statistic counts the number of successive sample values until a duplicate is found. The maximum collision test statistic is calculated as follows: 1. Let C be a list of the number of samples observed to find two occurrences of the same value in the input sequence S = (s , …, s ). C is initially empty. 1 L 2. Let i = 1. 3. While i < L a. Find the smallest j such that (s,…, s ) contains two identical values. If no such j i i+j-1 exists, break out of the while loop. b. Add j to the list C. c. i=i+j. 4. The test statistic T is the maximum value in the list C. Example 8: Let the input data be (2, 1, 1, 2, 0, 1, 0, 1, 1, 2). C = [3,4,2] is computed as in Example 7. T = max(3,4,2) = 4. Handling binary data: To test binary input data, first apply Conversion II to the input sequence. 5.1.9 Periodicity Test Statistic The periodicity test aims to determine the number of periodic structures in the data. The test takes a lag parameter p as input, where p < L, and the test statistic T is calculated as follows: 1. Initialize T to zero. 2. For i = 1 to L − p If (s = s ), increment T by one. i i+p Example 9: Let the input data be (2, 1, 2, 1, 0, 1, 0, 1, 1, 2), and let p = 2. Since s = s for five i i+p values of i (1, 2, 4, 5 and 6), T = 5. Handling binary data: To test binary input data, first apply Conversion I to the input sequence. The test is repeated for five different values of p: 1, 2, 8, 16, and 32.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 35 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 5.1.10 Covariance Test Statistic The covariance test measures the strength of the lagged correlation. The test takes a lag value p < L as input. The test statistic is calculated as follows: 1. Initialize T to zero. 2. For i = 1 to L – p T=T+(s ×s ). i i+p Example 10: Let the input data be (5, 2, 6, 10, 12, 3, 1), and let p be 2. T is calculated as (5 6) + (2 10) + (6 12) + (10 3) + (12 1) = 164. × Ha×ndling bi×nary data: ×To test bin×ary input data, first apply Conversion I to the input sequence. The test is repeated for five different values of p: 1, 2, 8, 16, and 32. 5.1.11 Compression Test Statistic General-purpose compression algorithms are well adapted for removing redundancy in a character string, particularly involving commonly recurring subsequences of characters. The compression test statistic for the input data is the length of that data subset after the samples are encoded into a character string and processed by a general-purpose compression algorithm. The compression test statistic is computed as follows: 1. Encode the input data as a character string containing a list of values separated by a single space, e.g., “S = (144, 21, 139, 0, 0, 15)” becomes “144 21 139 0 0 15”. 2. Compress the character string with the bzip2 compression algorithm provided in [BZ2]. 3. T is the length of the compressed string, in bytes. Handling binary data: The test can be applied directly to binary data, with no conversion required. 5.2 Additional Chi-square Statistical Tests This section includes additional chi-square statistical procedures to test independence and goodness-of-fit. The independence tests attempt to discover dependencies in the probabilities between successive samples in the (entire) sequence submitted for testing (see Section 5.2.1 for non-binary data and Section 5.2.3 for binary data); the goodness-of-fit tests attempt to discover a failure to follow the same distribution in ten data subsets produced from the (entire) input sequence submitted for testing (see Section 5.2.2 for non-binary data and Section 5.2.4 for binary data). The length of the longest repeated substring test is provided in Section 5.2.5. 5.2.1 Testing Independence for Non-Binary Data Given the input S = (s , …, s ), where s ϵ A = {x , …, x }, the following steps are initially performed 1 L i 1 k to determine the number of bins n needed for the chi-square tests. binNIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 36 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 1. Find the proportion of each x in S, i.e., Calculate the expected number i number of 𝑚𝑚𝑖𝑖 in 𝑆𝑆 of occurrences of each possible pair in S, as = . 𝑝𝑝𝑖𝑖 𝑝𝑝𝑖𝑖 = 𝐿𝐿 . 2. Allocate the possible pairs, (s 𝑧𝑧t𝑖𝑖a ,r 𝑧𝑧t𝑗𝑗i )ng from 𝑒𝑒th𝑖𝑖,e𝑗𝑗 sm 𝑝𝑝𝑖𝑖a 𝑝𝑝l𝑗𝑗le 𝐿𝐿s /t 2 , into bins such that the expected value of each bin is at least five. The expected value of a bin is equal to the sum of (𝑧𝑧𝑖𝑖,𝑧𝑧𝑗𝑗) 𝑒𝑒𝑖𝑖,𝑗𝑗 the values of the pairs that are included in the bin. After allocating all pairs, if the expected value of the last bin is less than five, merge the last two bins. Let n be the number of bins bin 𝑒𝑒𝑖𝑖,𝑗𝑗 constructed using this procedure. After constructing the bins, the Chi-square test is executed as follows: 1. Let o be a list of n counts, each initialized to 0. For j=1 to L-1: bin a. If the pair (s, s ) is in bin i, increment o by 1. j j+1 i b. Let j = j+2. 2. The test statistic is calculated as T = . The test fails if T is greater than the 2 𝑖𝑖𝑏𝑏𝑖𝑖𝑖𝑖 (𝑜𝑜𝑖𝑖 − 𝐸𝐸(𝐵𝐵𝑖𝑖𝑖𝑖𝑖𝑖 ) ) critical value of the Chi-square test statistic with (n –1) – (k–1) = n –k degrees of freedom ∑𝑖𝑖=1 𝐸𝐸(𝐵𝐵𝑖𝑖𝑖𝑖b 𝑖𝑖i )n bin when the probability of type I error is chosen as 0.001. If the value of degrees of freedom is less than one, do not apply the test. Example 11: Let S be (2, 2, 3, 1, 3, 2, 3, 2, 1, 3, 1, 1, 2, 3, 1, 1, 2, 2, 2, 3, 3, 2, 3, 2, 3, 1, 2, 2, 3, 3, 2, 2, 2, 1, 3, 3, 3, 2, 3, 2, 1, 3, 2, 3, 1, 2, 2, 3, 1, 1, 3, 2, 3, 2, 3, 1, 2, 2, 3, 3, 2, 2, 2, 1, 3, 3, 3, 2, 3, 2, 1, 2, 2, 3, 3, 3, 2, 3, 2, 1, 2, 2, 2, 1, 3, 3, 3, 2, 3, 2, 1, 3, 2, 3, 1, 2, 2, 3, 1, 1). The alphabet consists of k = 3 values {1, 2, 3}; and p , p , and p are 0.21, 0.41 and 0.38, respectively. With L = 100, the 1 2 3 sorted expected values are calculated as: (z z) (1,1) (1,3) (3,1) (1,2) (2,1) (3,3) (2,3) (3,2) (2,2) i, j 2.21 3.99 3.99 4.31 4.31 7.22 7.79 7.79 8.41 The pairs can be allocated into n = 6 bins. 𝑒𝑒𝑖𝑖,𝑗𝑗 bin Bin Pairs 1 (1,1), (1,3) 6.2 𝐸𝐸(𝐵𝐵𝑖𝑖𝑛𝑛𝑖𝑖 ) 2 (3,1), (1,2) 8.3 3 (2,1), (3,3) 11.53 4 (2,3) 7.79 5 (3,2) 7.79 6 (2,2) 8.41 The frequencies for the bins are calculated as 7, 6, 10, 8, 12, and 7 respectively, and the test statistic is calculated as 3.46. The value of the degrees of freedom is 3 (= 6-3). The hypothesis is not rejected, since the test statistic is less than the critical value 16.266.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 37 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 5.2.2 Testing Goodness-of-fit for Non-Binary Data The test checks whether the distribution of samples is identical for different parts of the input. Given the input S = (s , …, s ), where s ϵ A = {x , …, x }, perform the following steps to calculate 1 L i 1 k the number of bins n for the test. bin 1. Let be the number of occurrences of x in the entire dataset S, and let , for 1 i i k. Note that c is divided by ten because S will be partitioned into ten data subsets. i 𝑐𝑐𝑖𝑖 𝑒𝑒𝑖𝑖 = 𝑐𝑐𝑖𝑖/10 ≤ 2. Let List[i] be the sample value with the ith smallest (e.g., List has the smallest value for ≤ has the next smallest value, etc.) 𝑒𝑒𝑖𝑖 3. Starting from List, allocate the sample values into bins. Assign consecutive List[i] values to 𝑒𝑒𝑖𝑖; 𝐿𝐿𝑖𝑖𝑠𝑠𝐿𝐿 a bin until the sum of the for those binned items is at least five, then begin assigning the following List[i] value(s) to the next bin. If the expected value of the last bin is less than five, merge the last two bins. Le𝑒𝑒t 𝑖𝑖 n be the number of bins constructed after this procedure. bin 4. Let E be the expected number of sample values in Bin i; E is the sum of the e for the listed i i i items in that bin. For example, if Bin1 contains (x , x and x ), then E = e + e + e . 1 10 50 1 1 10 50 Example 12: Let the number of distinct sample values k be 4; and let c = 43, c = 55, c = 52 and 1 2 3 c =10. After partitioning the entire input sequence into 10 parts, the expected value of each sample 4 becomes e = 4.3, e = 5.5, e = 5.2 and e = 1. The sample list starting with the smallest expected 1 2 3 4 value is formed as List = [4, 1, 3, 2]. The first bin contains sample 4 and 1, and the expected value of Bin 1 becomes 5.3 (= e +e ). The second bin contains sample 3, and the last bin contains sample 4 1 2. Since the expected value of the last bin is greater than five, no additional merging is necessary. Given n , E and list of samples for each bin, the chi-square goodness-of-fit test is executed as bin i follows: 1. Partition S into ten non-overlapping sequences of length , where 𝐿𝐿 for d = 0,…, 9. If L is not a multiple of 10, the remaining samples �10� 𝑆𝑆𝑑𝑑 = are not used. (𝑠𝑠𝑑𝑑⌊𝐿𝐿/10⌋+1,…,𝑠𝑠(𝑑𝑑+1)⌊𝐿𝐿/10⌋) 2. T = 0. 3. For d = 0 to 9 3.1. For i = 1 to n bin 3.1.1. Let o be the total number of times the samples in Bin i appear in . i 3.1.2. T = T + . 𝑆𝑆𝑑𝑑 2 (𝑜𝑜𝑖𝑖− 𝐸𝐸𝑖𝑖 ) The test fails if the test sta𝐸𝐸t𝑖𝑖i stic T is greater than the critical value of chi-square with 9(n bin -1) degrees of freedom when the type I error is chosen as 0.001. 5.2.3 Testing Independence for Binary Data This test checks the independence assumption for binary data. A chi-square test for independence between adjacent bits could be used, but its power is limited, due to the small output space (i.e., the use of binary inputs). A more powerful check can be achieved by comparing the frequenciesNIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 38 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B of m-bit tuples to their expected values that are calculated by multiplying the probabilities of each successive bit, i.e., assuming that the samples are independent. If nearby bits are not independent, then the expected probabilities of m-bit tuples derived from their bit probabilities will be biased for the whole dataset, and a chi-square test statistic will be much larger than expected. Given the input binary data S = (s , …, s ), the length of the tuples, m, is determined as follows: 1 L 1. Let p and p be the proportion of zeroes and ones in S, respectively, i.e., , 0 1 # 0′s in 𝑆𝑆 and . 𝑝𝑝0 = 𝐿𝐿 # 1′s in 𝑆𝑆 2. Find𝑝𝑝 t1 he= m axi𝐿𝐿mum integer m such that . If m is greater than 11, set m = 11. If m is 1, the test fails. For example, for p =𝑚𝑚 0.1𝐿𝐿 4, p = 0.86, and L = 1000, m = 2. min(𝑝𝑝0,𝑝𝑝10 ) �𝑚𝑚� ≥1 5 The test is applied if m ≥ 2. 1. Initialize T to 0. 2. Partition S into non-overlapping m-bit blocks, denoted as ,…, . If L is not a multiple of m, discard the remaining bits. 𝐵𝐵 = (𝐵𝐵1 𝐵𝐵�𝑚𝑚𝐿𝐿 �) 3. For each possible m-bit tuple (a , a , …, a ) 1 2 m a. Let o be the number of times that the pattern (a , a , …, a ) occurs in the input B. 1 2 m b. Let w be the number of ones in (a , a , …, a ). 1 2 m c. Let e = 𝑙𝑙 𝑚𝑚−𝑙𝑙 𝐿𝐿 d. T = T + 𝑝𝑝1 (𝑝𝑝0 .) �𝑚𝑚�. 2 (𝑜𝑜−𝑒𝑒) The test fails if the test statistic T is greater than the critical value of chi-square with 2m–2 degrees 𝑒𝑒 of freedom, when the type I error is chosen as 0.001. 5.2.4 Testing Goodness-of-fit for Binary Data This test checks the distribution of the number of ones in non-overlapping intervals of the input data to determine whether the distribution of the ones remains the same throughout the sequence. Given the input binary data S = (s , …, s ), the test description is as follows: 1 L 1. Let p be the proportion of ones in the entire sequence S, i.e., p = (the number of ones in S)/ L. 2. Partition S into ten non-overlapping subsets of length , where = 𝐿𝐿 ( ) for d = 0, …, 9. If L is not a multiple of 10, the remaining bits �10� 𝑆𝑆𝑑𝑑 are discarded. 𝑠𝑠𝑑𝑑⌊𝐿𝐿/10⌋+1,…,𝑠𝑠(𝑑𝑑+1)⌊𝐿𝐿/10⌋ 3. Initialize T to 0. 4. Let the expected number of zeros and ones in each sub-sequence S be dNIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 39 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 𝐿𝐿 𝑒𝑒0 = (1−𝑝𝑝)� �, 10 𝐿𝐿 respectively. 𝑒𝑒1 = 𝑝𝑝� �, 10 5. For d = 0 to 9 a. Let and be the number of zeros and ones in , respectively. b. T = 𝑜𝑜T0 + 𝑜𝑜1 . 𝑆𝑆𝑑𝑑 2 2 (𝑜𝑜0−𝑒𝑒0 ) (𝑜𝑜1−𝑒𝑒1 ) T is a chi-square random v𝑒𝑒0ariab+le wi𝑒𝑒t1h nine degrees of freedom. The test fails if T is larger than the critical value at 0.001, which is 27.887. 5.2.5 Length of the Longest Repeated Substring Test This test checks the IID assumption using the length of the longest repeated substring. If this length is significantly longer than the expected value, then the test invalidates the IID assumption. The test can be applied to binary and non-binary inputs. Given the input S = (s , …, s ), where s ϵ A = {x , …, x }, 1 L i 1 k 1. Find the proportion of each possible input value x in S, i.e., i number of 𝑚𝑚𝑖𝑖 in 𝑆𝑆 2. Calculate the collisio𝑝𝑝n𝑖𝑖 probability as p col = . 𝑝𝑝𝑖𝑖 = 𝐿𝐿 . 𝑘𝑘 2 3. Find the length of the longest repeated substring W, i.e., find the largest W such that, for at ∑𝑖𝑖=1𝑝𝑝𝑖𝑖 least one i ≠ j, s = s, s = s , ... , s = s . i j i+1 j+1 i+W-1 j+W-1 4. The number of overlapping subsequences of length W in S is L–W+1, and the number of pairs of overlapping subsequences is 𝐿𝐿−𝑊𝑊 +1 � �. 5. Let X be a binomially distributed random variable with parameters N= and a 2 probability of success p colW. Calculate the probability that X is greater than𝐿𝐿 o−r e𝑊𝑊qu+al 1to 1, i.e., � � Pr (X ≥ 1) = 1− Pr (X = 0) = 1− (1− p colW)N. 2 The test fails if Pr (X ≥ 1) is less than 0.001.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 40 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 6 Estimating Min-Entropy One of the essential requirements of an entropy source is the ability to reliably create random outputs. To ensure that sufficient entropy is input to an RBG construction in SP 800-90C, the amount of entropy produced per noise source sample must be determined. This section describes generic estimation methods that will be used to test the noise source and also the conditioning component, when non-vetted conditioning components are used. It should be noted that the entropy estimation methods described in this section rely on some statistical assumptions that may not hold for all types of noise sources. The methods should not replace in-depth analysis of noise sources, but should be used to support the initial entropy estimate of the submitter (see Requirement 3 in Section 3.2.2). An example noise source analysis is provided in [HaFis15]. Each estimator takes a sequence S = (s , …, s ) as its input, where each s comes from an output 1 L i space A = {x , …, x } that is specified by the submitter. The estimators presented in this 1 k Recommendation follow a variety of strategies, which cover a range of assumptions about the data. For further information about the theory and origins of these estimators, see Appendix G. The estimators that are to be applied to a sequence depend on whether the data has been determined to be IID or non-IID. For IID data, the min-entropy estimation is determined as specified in Section 6.1, whereas for non-IID data, the procedures in Section 6.2 are used. The estimators presented in this section work well when the entropy-per-sample is greater than 0.1. For alphabet sizes greater than 256, some of the estimators are not very efficient. Therefore, for efficiency purposes, the method described in Section 6.4 can be used to reduce the alphabet space of the outputs. 6.1 IID Track: Entropy Estimation for IID Data For sources with IID outputs, the min-entropy estimation is determined using the most common value estimate described in Section 6.3.1. It is important to note that this estimate typically provides an overestimation when the samples from the source are not IID11. 6.2 Non-IID Track: Entropy Estimation for Non-IID Data Many viable noise sources fail to produce IID outputs. Moreover, some sources may have dependencies that are beyond the ability of the tester to address. To derive any utility out of such sources, a diverse and conservative set of entropy tests are required. Testing sequences with dependent values may result in overestimates of entropy. However, a large, diverse battery of estimates minimizes the probability that such a source’s entropy is greatly overestimated. 11 However, it is possible for this estimate to slightly underestimate the true min-entropy. It is believed that this underestimation is likely to not exceed one bit because of the relationship between min-entropy and the expected guessing work derived in Appendix D. Of course, such an underestimate would not indicate that a guessing attack that ignores dependencies could be less costly than one that takes the dependencies into account. As an example, consider a data sample consisting of pairs of bytes generated from the joint distribution on two bytes X and Y, each having possible values A and B, where Pr(X=A, Y=A)=0.104, Pr(X=A,Y=B)=0.332, Pr(X=B,Y=A)=0.239, and Pr(X=B,Y=B)=0.325. The min-entropy according to the MCV estimator is 0.712, while the true min-entropy is 0.795.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 41 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B For non-IID data, the following estimators shall be calculated on the outputs of the noise source and outputs of any conditioning component that is not listed in Section 3.1.5.1.1, and the minimum of all the estimates is taken as the entropy assessment of the entropy source for this Recommendation: • The Most Common Value Estimate (Section 6.3.1), • The Collision Estimate (Section 6.3.2), • The Markov Estimate (Section 6.3.3), • The Compression Estimate (Section 6.3.4), • The t-Tuple Estimate (Section 6.3.5), • The Longest Repeated Substring (LRS) Estimate (Section 6.3.6), • The Multi Most Common in Window Prediction Estimate (Section 6.3.7), • The Lag Prediction Estimate (Section 6.3.8), • The MultiMMC Prediction Estimate (Section 6.3.9), and • The LZ78Y Prediction Estimate (Section 6.3.10). The Collision, Markov and Compression estimates are only applied to binary inputs. 6.3 Estimators 6.3.1 The Most Common Value Estimate This method first finds the proportion of the most common value in the input dataset, and then constructs a confidence interval for this proportion. The upper bound of the confidence interval is used to estimate the min-entropy per sa𝑝𝑝m̂ ple of the source. Given the input S = (s , …, s ), where s ϵ A = {x , …, x }, 1 L i 1 k 1. Find the proportion of the most common value in the dataset, i.e., = 𝑝𝑝̂ #{𝑚𝑚𝑖𝑖 in 𝑆𝑆} 2. Calculate an upper bound on the pro𝑝𝑝 b� a bi m lit𝑖𝑖a yx of th𝐿𝐿 e mo. st common value as 𝑝𝑝𝑜𝑜 𝑝𝑝̂ (1−𝑝𝑝̂) 𝑝𝑝𝑜𝑜 = min�1,𝑝𝑝̂ +2.576� �, 𝐿𝐿−1 where 2.576 corresponds to the value. 3. The estimated min-entropy is –lo 𝑍𝑍g 2((1−0).0.0 5) 𝑝𝑝𝑜𝑜NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 42 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B Example: If the dataset is S = (0, 1, 1, 2, 0, 1, 2, 2, 0, 1, 0, 1, 1, 0, 2, 2, 1, 0, 2, 1), with L = 20, the most common value is 1, with = 0.4. The min-entropy estimate is −log (0.6822) = 0.5363. 2 𝑝𝑝̂ 𝑝𝑝𝑜𝑜 = 0.4+2.576√0.012 = 0.6895. 6.3.2 The Collision Estimate The collision estimate, proposed by Hagerty and Draper [HD12], measures the mean number of samples to the first collision in a dataset, where a collision is any repeated value. The goal of the method is to estimate the probability of the most-likely output value, based on the collision times. The method will produce a low entropy estimate for noise sources that have considerable bias toward a particular output or value (i.e., the mean time until a collision is relatively short), while producing a higher entropy estimate for a longer mean time to collision. This entropy estimation method is only applied to binary inputs. Given the input S = (s , …, s ), where s ϵ A = {0,1}, 1 L i 1. Set v = 0, index =1. 2. Beginning with s , step through the input until any observed value is repeated; i.e., find index the smallest j such that s = s, for some i with index . i j 3. Set v = v + 1, t = j – index + 1 and index = j + 1. v ≤ 𝑖𝑖 < 𝑗𝑗 4. Repeat steps 2-3 until the end of the dataset is reached. 5. Calculate the sample mean , and the sample standard deviation , of t as i 𝑋𝑋� , . 𝜎𝜎� 1 𝑣𝑣 1 𝑣𝑣 2 𝑋𝑋� = 𝑣𝑣∑𝑖𝑖=1𝐿𝐿𝑖𝑖 𝜎𝜎� = �𝑣𝑣−1∑𝑖𝑖=1(𝑜𝑜𝑖𝑖− 𝑋𝑋�) 6. Compute the lower-bound of the confidence interval for the mean, based on a normal distribution with a confidence level of 99 %, 𝜎𝜎� 𝑋𝑋� ′ = 𝑋𝑋� −2.576 . 7. Using a binary search, solve for the parameter p, √su𝑣𝑣ch that −2 1 −1 −1 −11 −1 −1 𝑋𝑋� ′ = 𝑝𝑝𝑞𝑞 �1+ (𝑝𝑝 −𝑞𝑞 )�𝐹𝐹(𝑞𝑞)−𝑝𝑝𝑞𝑞 (𝑝𝑝 −𝑞𝑞 ). where 2 2 𝑞𝑞 = 1−𝑝𝑝 , 𝑝𝑝 ≥ 𝑞𝑞, −3 𝑧𝑧 𝐹𝐹(1⁄𝑧𝑧) = Γ(3,𝑧𝑧)𝑧𝑧 𝑒𝑒 ,NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 43 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B and (a,b) is the incomplete Gamma function defined as An efficient implementation of F(1/z) is provided in Appendix G.1.1. The b∞oun𝑚𝑚d−s1 o−f 𝑜𝑜the binary search Γ ∫𝑏𝑏 𝐿𝐿 𝑒𝑒 𝑑𝑑𝐿𝐿. should be 1/2 and 1. 8. If the binary search yields a solution, then the min-entropy estimation is the negative logarithm of the parameter, p: min-entropy = –log (p). 2 If the search does not yield a solution, then the min-entropy estimation is: min-entropy = log (2)=1. 2 Example: Suppose that S = (1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0). The collisions of the sequence are (1, 0, 0), (0, 1, 1), (1, 0, 0), (1, 0, 1), (0, 1, 0), (1, 1), (1, 0, 0), (1, 1), (0, 0), (0, 1, 1), (1, 0, 0), (1, 0, 1), (0, 1, 0), (1, 1). After step 5, v=14, and the sequence (t , … t ) is (3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2). Then 1 v . The solution to the equation is p = 0.7329, giving an estimated min- entropy of 0.44′83. 𝑋𝑋� = 2.7143,𝜎𝜎� = 0.4688, and 𝑋𝑋� = 2.3915 6.3.3 The Markov Estimate In a first-order Markov process, the next sample value depends only on the latest observed sample value; in an nth-order Markov process, the next sample value depends only on the previous n observed values. Therefore, a Markov model can be used as a template for testing sources with dependencies. The Markov estimate provides a min-entropy estimate by measuring the dependencies between consecutive values from the input dataset. The min-entropy estimate is based on the entropy present in any subsequence (i.e., chain) of outputs, instead of an estimate of the min-entropy per output. Samples are collected from the noise source, and specified as d-long chains of samples. From this data, probabilities are determined for both the initial state and transitions between any two states. These probabilities are used to determine the highest probability of any particular d-long chain of samples. The corresponding maximum probability is used to determine the min-entropy present in all such chains generated by the noise source. This min-entropy value is particular to d-long chains and cannot be extrapolated linearly; i.e., chains of length wd will not necessarily have w times as much min-entropy present as a d-long chain. It may not be possible to know what a typical output length will be at the time of testing. Therefore, although not mathematically correct, in practice, calculating an entropy estimate per sample (extrapolated from that of the d-long chain) provides estimates that are close. This entropy estimation method is only applied to binary inputs. Given the input S = (s , …, s ), where s ϵ A = {0,1}, 1 L i 1. Estimate the initial probabilities for each output value, and . #{0 in 𝑆𝑆} 2. Let T be the transition matrix of the form 𝑃𝑃0 = 𝐿𝐿 𝑃𝑃1 = 1− 𝑃𝑃0 2×2NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 44 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 0 1 0 1 𝑃𝑃0,0 𝑃𝑃0,1 𝑃𝑃1,0 𝑃𝑃1,1 where the probabilities are calculated as , , #{00 in 𝑆𝑆} #{01 in 𝑆𝑆} 𝑃𝑃0,0 = #{00 in 𝑆𝑆}+#{01 in 𝑆𝑆} 𝑃𝑃0,1 = #{00 in 𝑆𝑆}+#{01 in 𝑆𝑆} and . #{10 in 𝑆𝑆} #{11 in 𝑆𝑆} 𝑃𝑃1,0 = #{10 in 𝑆𝑆}+#{11 in 𝑆𝑆}, 𝑃𝑃1,1 = #{10 in 𝑆𝑆}+#{11 in 𝑆𝑆} 3. Find the probability of the most likely sequence of outputs of length 128, as calculated below. Sequence Probability 00…0 127 0101…01 𝑃𝑃0×𝑃𝑃0,0 64 63 011…1 𝑃𝑃0×𝑃𝑃0,1 ×𝑃𝑃1,0 126 100…0 𝑃𝑃0×𝑃𝑃0,1×𝑃𝑃1,1 126 1010…10 𝑃𝑃1×𝑃𝑃1,0×𝑃𝑃0,0 64 63 11…1 𝑃𝑃1×𝑃𝑃1,0 ×𝑃𝑃0, 1 127 𝑃𝑃1×𝑃𝑃1,1 4. Let be the maximum of the probabilities in the table given above. The min-entropy max estimate is the negative logarithm of the probability of the most likely sequence of outputs, : max 𝑝𝑝̂ min-entropy = min(–log ( )/128,1) 2 max 𝑝𝑝̂ Example: For the purpose of this example12, suppose that, L = 40 and S = (1, 0, 0, 0, 1, 1, 1, 0, 0, 𝑝𝑝̂ 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0) 0.475 and 0.525. The transition matrix is calculated as . 𝑃𝑃0 = 𝑃𝑃1 = 0 1 0 0.389 0.611 1 0.571 0.429 The probabilities of the possible sequences are 12 The test is designed for long sequences (i.e., L ≈ 1 000 000), for the purpose of the example, a very small value of L is used.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 45 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B Sequence Probability 00…0 3.9837 10-53 0101…01 4.4813×10-30 011…1 1.4202×10-47 10…0 6.4631×10-53 1010…10 4.6288×10-30 11…1 1.1021×10-47 × The resulting entropy estimate is min(– log (4.6288 10-30)/128,1) = min(0.761,1) = 0.761. 2 6.3.4 The Compression Estimate × The compression estimate, proposed by Hagerty and Draper [HD12], computes the entropy rate of a dataset, based on how much the dataset can be compressed. This estimator is based on the Maurer Universal Statistic [Mau92]. The estimate is computed by generating a dictionary of values, and then computing the average number of samples required to produce an output, based on the dictionary. One advantage of using the Maurer statistic is that there is no assumption of independence. When sequences with dependencies is tested with this statistic, the compression rate is affected (and therefore the entropy), but an entropy estimate is still obtained. A calculation of the Maurer statistic is efficient, as it requires only one pass through the dataset to provide an entropy estimate. Given a dataset from the noise source, the samples are first partitioned into two disjoint groups. The first group serves as the dictionary for the compression algorithm; the second group is used as the test group. The compression values are calculated over the test group to determine the mean, which is the Maurer statistic. Using the same method as the collision estimate, the probability distribution that has the minimum possible entropy for the calculated Maurer statistic is determined. For this distribution, the entropy per sample is calculated as the lower bound on the entropy that is present. This entropy estimation method is only applied to binary inputs. Given the input S = (s , …, s ), where s ϵ A = {0,1}, 1 L i 1. Let b = 6. Create a new sequence, , by dividing S into non-overlapping b-bit blocks. If L is not a multiple of′ b, disc′ard th′e extra data. 𝑆𝑆 = (𝑠𝑠1,…,𝑠𝑠⌊𝐿𝐿/𝑏𝑏⌋)NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 46 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 2. Partition the dataset, , into two disjoint groups. These two groups will form the dictionary and the test data. ′ 𝑆𝑆 a. Create the dictionary from the first d = 1000 elements of , . ′ ′ ′ b. Use the remaining observations, 𝑆𝑆 (𝑠𝑠1,… fo ,r 𝑠𝑠 𝑑𝑑te )sting. ′ ′ 3. Initialize the dictionary dic 𝑣𝑣t t =o a ⌊n 𝐿𝐿 /a 𝑏𝑏ll ⌋ z −ero 𝑑𝑑 array of size 2b (. 𝑠𝑠 F𝑑𝑑+o1r ,i … fr ,o 𝑠𝑠m⌊𝐿𝐿 /1𝑏𝑏 ⌋t )o , d, let dict[ ] = i. The value of dict[ ] is the index of the last occurrence of each in the dictionary. ′ 𝑠𝑠𝑖𝑖 ′ ′ 4. Run the test data against the dictionary created in Step 2. 𝑠𝑠𝑖𝑖 𝑠𝑠𝑖𝑖 a. Let D be a list of length v. b. For i from to : i. If dict[ ] is non-zero, then D = i – dict[ ]. Update the dictionary with the 𝑑𝑑+1 ⌊𝐿𝐿/𝑏𝑏⌋ i-d index of′ the most recent observation, dict[′ ]=i. 𝑠𝑠𝑖𝑖 𝑠𝑠𝑖𝑖 ′ ii. If dict[s] is zero, add that value to the dictionary, i.e., dict[ ] =i. Let D = i 𝑠𝑠𝑖𝑖 i-d i. ′ 𝑠𝑠𝑖𝑖 5. Calculate the sample mean, , and sample standard deviation13, , of (log (D ), …, 2 1 log (D )). 2 v 𝑋𝑋� 𝜎𝜎� 𝑣𝑣 ∑𝑖𝑖=1log2𝐷𝐷𝑖𝑖 𝑋𝑋� = , 𝑣𝑣 and 𝑐𝑐 = 0.5907 𝑣𝑣 2 ∑𝑖𝑖=1(log2𝐷𝐷𝑖𝑖) 2 𝜎𝜎� = 𝑐𝑐� − 𝑋𝑋� . 𝑣𝑣 −1 6. Compute the lower-bound of the confidence interval for the mean, based on a normal distribution using 2.576𝜎𝜎� 𝑋𝑋� ′ = 𝑋𝑋� − . 7. Using a binary search, solve for the parameter p,√ s𝑣𝑣uch that the following equation is true: 𝑏𝑏 where 𝑋𝑋� ′ = 𝐺𝐺(𝑝𝑝)+(2 −1)𝐺𝐺(𝑞𝑞), 13 Note that a correction factor is applied to the standard deviation, as described in [Mau92] and computed with higher accuracy in [CoNa98]. This correction factor reduces the standard deviation to account for dependencies in the Di values.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 47 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B , 1 𝐿𝐿 𝑜𝑜 𝐺𝐺(𝑧𝑧) = 𝑣𝑣∑𝑜𝑜=𝑑𝑑+1∑𝑜𝑜=1log2(𝑢𝑢)𝐹𝐹(𝑧𝑧,𝐿𝐿,𝑢𝑢) , 2 𝑜𝑜−1 𝑧𝑧 (1−𝑧𝑧) 𝑖𝑖𝑖𝑖 𝑢𝑢 < 𝐿𝐿 𝐹𝐹(𝑧𝑧,𝐿𝐿,𝑢𝑢) = � 𝑜𝑜−1 and 𝑧𝑧(1−𝑧𝑧) 𝑖𝑖𝑖𝑖 𝑢𝑢 = 𝐿𝐿 1−𝑝𝑝 𝑞𝑞 = 𝑏𝑏 . The bounds of the binary search should b2e −1and 1. 8. If the binary search yields a solution, then the− 𝑏𝑏min-entropy is the negative logarithm of the parameter, p: 2 min-entropy = –log (p)/b. 2 If the search does not yield a solution, then the min-entropy estimation is: min-entropy = 1. Example: For illustrative purposes, suppose that d = 4 (instead of 1000), L = 48 and S = (1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1 ,1, 1, 0, 0, 0, 1, 1). After step 1, the new blocked sequence is (100011, 100101, 010111, 001100, 011100, 101010, 111011, 100011). The dictionary seq′uence is (100011, 100101, 010111, 001100), and the testing sequence is (011100, 101010, 11𝑆𝑆101=1, 100011). v = 4. After the dictionary is initialized in step 3, it has the following values (only non-zero values are shown): i 1 2 3 4 100011 100101 010111 001100 ′ dic 𝑠𝑠t[𝑖𝑖 ] 1 2 3 4 ′ 𝑠𝑠𝑖𝑖 After Step 4, the resulting D = 5, D = 6, D = 7, and D = 7. The values computed in step 5 are 1 2 3 4 and , and the value for step 6 is . The value of p that solves the equation in step 7 is 0.5715, and the min-entropy estimate is 0.1345. 𝑋𝑋� = 2.6304 𝜎𝜎� = 0.9074 𝑋𝑋� ′ = 1.4617 6.3.5 t-Tuple Estimate This method examines the frequency of t-tuples (pairs, triples, etc.) that appears in the input dataset and produces an estimate of the entropy per sample, based on the frequency of those t-tuples. The frequency of the t-tuple (r , r , …, r) in S = (s , …, s ) is the number of i’s such that s = r , s = 1 2 t 1 L i 1 i+1 r , …, s = r. It should be noted that the tuples can overlap. 2 i+t-1 t Given the input S = (s , …, s ), where s ϵ A = {x , …, x }, 1 L i 1 kNIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 48 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 1. Find the largest t such that the number of occurrences of the most common t-tuple in S is at least 35. 2. Let Q[i] store the number of occurrences of the most common i-tuple in S for i = 1, ..., t. For example, in S=(2, 2, 0, 1, 0, 2, 0, 1, 2, 1, 2, 0, 1, 2, 1, 0, 0, 1, 0, 0, 0), Q = max(#0’s,#1’s,#2’s) = #0’s = 9, and Q = 4 is obtained by the number of the tuple 01 in S. 3. For i = 1 to t, let P[i] = Q[i] / (L-i+1), and compute an estimate on the maximum individual sample value probability as . Let = max ( ). max 1/𝑖𝑖 4. Calculate an upper bound on𝑃𝑃 t𝑚𝑚 he𝑚𝑚 𝑚𝑚 p[r𝑖𝑖o]b=ab𝑃𝑃il[i𝑖𝑖ty] of the m𝑝𝑝̂ost commo𝑃𝑃n𝑚𝑚 v𝑚𝑚 a𝑚𝑚 l[u1e] ,…a,s𝑃𝑃 𝑚𝑚 𝑚𝑚𝑚𝑚[𝐿𝐿] 𝑝𝑝𝑜𝑜 𝑝𝑝̂𝑚𝑚𝑚𝑚𝑚𝑚 (1−𝑝𝑝̂𝑚𝑚𝑚𝑚𝑚𝑚) 𝑝𝑝𝑜𝑜 = min�1,𝑝𝑝̂𝑚𝑚𝑚𝑚𝑚𝑚 +2.576� �, 𝐿𝐿−1 5. The entropy estimate is calculated as –log ( 2 Example: For the purpose of this example, suppose that the cutoff is 3 instead of 35 in step one. 𝑝𝑝𝑜𝑜). Suppose that S = (2, 2, 0, 1, 0, 2, 0, 1, 2, 1, 2, 0, 1, 2, 1, 0, 0, 1, 0, 0, 0), and L = 21. The number of occurrences of the most common 4-tuple is 2, which falls below the threshold, and therefore t = 3. In step 2, Q = 9, Q = 4, and Q = 3. P = 0.4286, P = 0.2, P = 0.1579. P  max = 0.4286, P  = 0.4472, P  = 0.5405, and =0.5405. The upper bound of a 99 % max max max confidence interval is 0.8276. The min-entropy estimate is −log (0.8276) = 0.273. 2 𝑝𝑝̂ 6.3.6 Longest Repeated Substring (LRS) Estimate This method estimates the collision entropy (sampling without replacement) of the source, based on the number of repeated substrings (tuples) within the input dataset. Although this method estimates collision entropy (an upper bound on min-entropy), this estimate handles tuple sizes that are too large for the t-tuple estimate, and is therefore a complementary estimate. Given the input S = (s , …, s ), where s ϵ A = {x , …, x }, 1 L i 1 k 1. Find the smallest u such that the number of occurrences of the most common u-tuple in S is less than 35. 2. Find the largest v such that the number of occurrences of the most common v-tuple in S is at least 2, and the most common (v+1)-tuple in S occurs once. In other words, v is the largest length that a tuple repeat occurs. If v < u, this estimate cannot be computed. 3. For W = u to v, compute the estimated W-tuple collision probability 𝐶𝐶𝑖𝑖 ∑𝑖𝑖�2� 𝑃𝑃𝑊𝑊 = 𝐿𝐿−𝑊𝑊+1 , where C is the number of occurrences of the ith unique W-tuple. Compute the estimated i � 2 � average collision probability per string symbol as . Let 1/𝑊𝑊 𝑃𝑃𝑚𝑚𝑚𝑚𝑚𝑚,𝑊𝑊 = 𝑃𝑃𝑊𝑊 𝑝𝑝̂ = max(𝑃𝑃𝑚𝑚𝑚𝑚𝑚𝑚,𝑜𝑜 ,…,𝑃𝑃𝑚𝑚𝑚𝑚𝑚𝑚,𝑣𝑣 ).NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 49 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 4. Calculate an upper bound on the probability of the most common value as 𝑝𝑝𝑜𝑜 𝑝𝑝̂ (1−𝑝𝑝̂) 𝑝𝑝𝑜𝑜 = min�1,𝑝𝑝̂ +2.576� �, 𝐿𝐿−1 5. The entropy estimate is calculated as –log ( 2 Example: For the purpose of this example, suppose that the cutoff is 3 instead of 35 in step 1. 𝑝𝑝𝑜𝑜). Suppose that S = (2, 2, 0, 1, 0, 2, 0, 1, 2, 1, 2, 0, 1, 2, 1, 0, 0, 1, 0, 0, 0), and L = 21. In step 1, u is calculated as 4, as the frequency of the most common 4-tuple is 2. In step 2, v is calculated as 5. After step 3, P = 0.0131, P = 0.0074, P = 0.3381, P = 0.3744, and 4 5 max,4 max,5 After step 4, = 0.6531. The min-entropy estimate is −log 2 (0.6531) = 0.6146. 𝑝𝑝̂ = max(0.3381,0.3744) = 0.3744. 𝑝𝑝𝑜𝑜 6.3.7 Multi Most Common in Window Prediction Estimate The Multi Most Common in Window (MultiMCW) predictor contains several subpredictors, each of which aims to guess the next output, based on the last w outputs. Each subpredictor predicts the value that occurs most often in that window of w previous outputs. The MultiMCW predictor keeps a scoreboard that records the number of times that each subpredictor was correct, and uses the subpredictor with the most correct predictions to predict the next value. In the event of a tie, the most common sample value that has appeared most recently is predicted. This predictor was designed for cases where the most common value changes over time, but still remains relatively stationary over reasonable lengths of the sequence. Given the input S = (s , …, s ), where s ϵ A = {x , …, x }, 1 L i 1 k 1. Let window sizes be w =63, w =255, w =1023, w =4095, and N = L − w . Let correct be 1 2 3 4 1 an array of N Boolean values, each initialized to 0. 2. Let scoreboard be a list of four counters, each initialized to 0. Let frequent be a list of four values, each initialized to Null. Let winner = 1. 3. For i = w + 1 to L: 1 a. For j =1 to 4, i. If i > w, let frequent be the most frequent value in (s , s , …, s ). If j j i-wj i-wj+1 i-1 there is a tie, then the most frequent value that has appeared most recently is assigned to frequent. j ii. Else, let frequent = Null. j b. Let prediction = frequent . winner c. If (prediction = s) let correct = 1. i , i- w1 d. Update the scoreboard. For j =1 to 4, i. If (frequent = s) j i 1. Let scoreboard = scoreboard +1 j jNIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 50 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 2. If scoreboard ≥ scoreboard , let winner =j j winner 4. Let C be the number of ones in correct. 5. Calculate the predictor’s global performance as . The upper bound of the 99 % 𝐶𝐶 confidence interval on , denoted is calculated as: 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 = 𝑁𝑁 ′ 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 1 𝑁𝑁 1−0.01 , if 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 = 0, ′ 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 = � 𝑃𝑃𝑖𝑖𝑔𝑔𝑔𝑔𝑏𝑏𝑚𝑚𝑔𝑔 �1−𝑃𝑃𝑖𝑖𝑔𝑔𝑔𝑔𝑏𝑏𝑚𝑚𝑔𝑔� min (1,𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 + 2.576� 𝑁𝑁−1 ), otherwise where 2.576 corresponds to the value. 6. Calculate the predictor’s local pe 𝑍𝑍r(f1o−rm0.0a0n5c)e, based on the longest run of correct predictions. Let r be one greater than the length of the longest run of ones in correct. Use a binary search to solve the following for : 𝑃𝑃𝑙𝑙𝑜𝑜𝑐𝑐𝑚𝑚𝑙𝑙 1−𝑃𝑃𝑙𝑙𝑜𝑜𝑐𝑐𝑚𝑚𝑙𝑙𝑥𝑥 1 0.99 = × 𝑁𝑁+1 , where and x = x , de(r𝑟𝑟iv+ed1 b−y i𝑟𝑟t𝑥𝑥er)a𝑞𝑞ting 𝑥𝑥the recurrence relation 10 𝑞𝑞 = 1−𝑃𝑃𝑙𝑙𝑜𝑜𝑐𝑐𝑚𝑚𝑙𝑙 𝑟𝑟 𝑟𝑟+1 for j from 1 to 10, and x = 1. N𝑥𝑥o𝑗𝑗 te= th1at+ so𝑞𝑞l𝑃𝑃v𝑙𝑙 i𝑜𝑜 n𝑐𝑐 g𝑚𝑚 𝑙𝑙 f𝑥𝑥o𝑗𝑗− r 1 using the logarithm of these 0 equations is robust against overflows. Table 3 given in Appendix G.2 provides some pre- calculated values of . 𝑃𝑃𝑙𝑙𝑜𝑜𝑐𝑐𝑚𝑚𝑙𝑙 7. The min-entropy is t𝑃𝑃he𝑙𝑙𝑜𝑜 𝑐𝑐n𝑚𝑚e𝑙𝑙gative logarithm of the greater performance metric min-entropy . ′ 1 Example: Suppose that S = (1, 2, 1, 0, 2= , 1 ,− 1l ,o 2g , 2 2( ,m 0a , x 0( , 𝑃𝑃 0𝑖𝑖 )𝑙𝑙 ,𝑜𝑜 s𝑏𝑏 o𝑚𝑚 𝑙𝑙 t, h𝑃𝑃 a𝑙𝑙 t𝑜𝑜 𝑐𝑐 L𝑚𝑚 𝑙𝑙 =, 𝑘𝑘1) 2) . For the purpose of this example, suppose that w = 3, w = 5, w = 7, w = 9 (instead of w = 63, w = 255, w = 1023, w 1 2 3 4 1 2 3 4 = 4095). Then N = 9. In step 3, the values are as follows: i frequent scoreboard Winner prediction s correct scoreboard i i-w1 (step 3b) (step 3b) (step 3d) 4 (1, --, --, --) (0, 0, 0, 0) 1 1 0 0 (0, 0, 0, 0) 5 (0, --, --, --) (0, 0, 0, 0) 1 0 2 0 (0, 0, 0, 0) 6 (2, 2, --, --) (0, 0, 0, 0) 1 2 1 0 (0, 0, 0, 0) 7 (1, 1, --, --) (0, 0, 0, 0) 1 1 1 1 (1, 1, 0, 0) 8 (1, 1, 1, --) (1, 1, 0, 0) 2 1 2 0 (1, 1, 0, 0) 9 (1, 2, 2, --) (1, 1, 0, 0) 2 2 2 1 (1, 2, 1, 0) 10 (2, 2, 2, 2) (1, 2, 1, 0) 2 2 0 0 (1, 2, 1, 0) 11 (2, 2, 2,2) (1, 2, 1, 0) 2 2 0 0 (1, 2, 1, 0) 12 (0, 0, 2, 0) (1, 2, 1, 0) 2 0 0 1 (2, 3, 1, 1)NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 51 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B After all of the predictions are made, correct = (0, 0, 0, 1, 0, 1, 0, 0, 1). Then, , , , and the resulting min-entropy estimate is 0.3908. 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 = 0.3333 ′ 𝑃𝑃 6𝑖𝑖 .3𝑙𝑙𝑜𝑜 .8𝑏𝑏 𝑚𝑚𝑙𝑙 T=h0e. 7L6a2g7 P𝑃𝑃r𝑙𝑙 e𝑜𝑜 d𝑐𝑐𝑚𝑚 i𝑙𝑙 c=tio0n.0 E3s6timate The lag predictor contains several subpredictors, each of which predicts the next output, based on a specified lag. The lag predictor keeps a scoreboard that records the number of times that each subpredictor was correct, and uses the subpredictor with the most correct predictions to predict the next value. Given the input S = (s , …, s ), where s ϵ A = {x , …, x }, 1 L i 1 k 1. Let D = 128, and N = L −1. Let lag be a list of D values, each initialized to Null. Let correct be a list of N Boolean values, each initialized to 0. 2. Let scoreboard be a list of D counters, each initialized to 0. Let winner = 1. 3. For i = 2 to L: a. For d = 1 to D: i. If (d < i), lag d = s i−d, ii. Else lag = Null. d b. Let prediction = lag . winner c. If (prediction = s )let correct = 1. i, i-1 d. Update the scoreboard. For d = 1 to D: i. If (lag = s) d i 1. Let scoreboard = scoreboard +1. d d 2. If scoreboard ≥ scoreboard , let winner = d. d winner 4. Let C be the number of ones in correct. 5. Calculate the predictor’s global performance as . The upper bound of the 99 % 𝐶𝐶 confidence interval on , denoted is calculated as: 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 = 𝑁𝑁 ′ 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 1 𝑁𝑁 1−0.01 , if 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 = 0, ′ 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 = � 𝑃𝑃𝑖𝑖𝑔𝑔𝑔𝑔𝑏𝑏𝑚𝑚𝑔𝑔 �1−𝑃𝑃𝑖𝑖𝑔𝑔𝑔𝑔𝑏𝑏𝑚𝑚𝑔𝑔� min (1,𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 + 2.576� 𝑁𝑁−1 ), otherwise where 2.576 corresponds to the value. 6. Calculate the predictor’s local pe 𝑍𝑍rf(1o−rm0.0a0n5c)e, based on the longest run of correct predictions. Let r be one greater than the length of the longest run of ones in correct. Use a binary search to solve the following for : 𝑃𝑃𝑙𝑙𝑜𝑜𝑐𝑐𝑚𝑚𝑙𝑙 1−𝑃𝑃𝑙𝑙𝑜𝑜𝑐𝑐𝑚𝑚𝑙𝑙𝑥𝑥 1 0.99 = × 𝑁𝑁+1 , (𝑟𝑟+1−𝑟𝑟𝑥𝑥)𝑞𝑞 𝑥𝑥NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 52 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B where and x = x , derived by iterating the re𝑞𝑞cu=rre1n−ce𝑃𝑃 r𝑙𝑙e𝑜𝑜l𝑐𝑐a𝑚𝑚t𝑙𝑙ion 10 𝑟𝑟 𝑟𝑟+1 for j from 1 to 10, and x =1. No𝑥𝑥t𝑗𝑗 e =tha1t +so𝑞𝑞lv𝑃𝑃i𝑙𝑙 n𝑜𝑜𝑐𝑐 g𝑚𝑚 𝑙𝑙 f𝑥𝑥o𝑗𝑗 r− 1 using the logarithm of these 0 equations is robust against overflows. Table 3 in Appendix G.2 provides some pre- calculated values of . 𝑃𝑃𝑙𝑙𝑜𝑜𝑐𝑐𝑚𝑚𝑙𝑙 7. The min-entropy is th𝑃𝑃e𝑙𝑙𝑜𝑜 𝑐𝑐n𝑚𝑚e𝑙𝑙gative logarithm of the greater performance metric min-entropy ′ 1 Example: Suppose that S = (2, 1, 3, 2, 1,= 3, − 1,l o 3,g 12( , m 2)a , x so(𝑃𝑃 t𝑖𝑖 h𝑙𝑙 a𝑜𝑜 t𝑏𝑏 𝑚𝑚 L𝑙𝑙 , =𝑃𝑃 1𝑙𝑙𝑜𝑜 0𝑐𝑐 𝑚𝑚 a𝑙𝑙 n,𝑘𝑘d) N). = 9. For the purpose of this example, suppose that D = 3 (instead of 128). The following table shows the values in step 3. i lag Winner prediction s correct scoreboard i i-1 (step 3b) (step 3d) 2 (2, --, --) 1 2 1 0 (0, 0, 0) 3 (1, 2, --) 1 1 3 0 (0, 0, 0) 4 (3, 1, 2) 1 3 2 0 (0, 0, 1) 5 (2, 3, 1) 3 1 1 1 (0, 0, 2) 6 (1, 2, 3) 3 3 3 1 (0, 0, 3) 7 (3, 1, 2) 3 2 1 0 (0, 1, 3) 8 (1, 3, 1) 3 1 3 0 (0, 2, 3) 9 (3, 1, 3) 3 3 1 0 (0, 3, 3) 10 (1, 3, 1) 2 3 2 0 (0, 3, 3) After all of the predictions are made, correct = (0, 0, 0, 1, 1, 0, 0, 0, 0). Then, , , , and the resulting min-entropy estimate is 0.735. 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 = 0.2222 ′ 𝑃𝑃 6𝑖𝑖 .3𝑙𝑙𝑜𝑜 .9𝑏𝑏 𝑚𝑚𝑙𝑙 T=h0e. 6M0u08ltiM𝑃𝑃𝑙𝑙 M𝑜𝑜𝑐𝑐 C𝑚𝑚𝑙𝑙 P=re0d.1i1c6ti7on Estimate The MultiMMC predictor is composed of multiple Markov Model with Counting (MMC) subpredictors. Each MMC predictor records the observed frequencies for transitions from one output to a subsequent output (rather than the probability of a transition, as in a typical Markov model), and makes a prediction, based on the most frequently observed transition from the current output. MultiMMC contains D MMC subpredictors running in parallel, one for each depth from 1 to D. For example, the MMC with depth 1 creates a first-order model, while the MMC with depth D creates a Dth-order model. MultiMMC keeps a scoreboard that records the number of times that each MMC subpredictor was correct, and uses the subpredictor with the most correct predictions to predict the next value.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 53 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B Given the input S = (s , …, s ), where s ϵ A = {x , …, x }, 1 L i 1 k 1. Let D = 16, and N = L − 2. Let subpredict be a list of D values, each initialized to Null. Let correct be an array of N values, each initialized to 0. Let entries be an array of D values, each initialized to 0, and let maxEntries = 100 000. 2. For d = 1 to D, let M be a set of counters, where M [x, y] denotes the number of observed d d transitions from output x to output y for the d th-order MMC. 3. Let scoreboard be a list of D counters, each initialized to 0. Let winner = 1. 4. For i = 3 to L: a. For d = 1 to D: i. If d < i−1: 1. If [(s i-d-1, …,s i-2), s i-1] is in M d, increment M d[(s i-d−1, …,s i−2), s i−1] by 1. 2. Else if entries d < maxEntries, add a counter for [(s i−d−1, …,s i−2), s i− 1] to the set, let M d[(s i−d−1,…,s i−2), s i− 1] = 1 and increment entries d by 1. b. For d = 1 to D: i. If d < i, find the y value that corresponds to the highest M [(s ,…,s ), y] d i-d i-1 value, and denote that y as ymax. If there is a tie, let ymax be the greatest y in the tie. Let subpredict = ymax. If all possible values of M [(s ,…,s ), d d i-d i-1 y] are 0, then let subpredict = Null. d c. Let prediction = subpredict . winner d. If (prediction = s), let correct = 1. i i-2 e. Update the scoreboard. For d = 1 to D: i. If (subpredict = s) d i 1. Let scoreboard = scoreboard +1. d d 2. If scoreboard ≥ scoreboard , let winner = d. d winner 5. Let C be the number of ones in correct. 6. Calculate the predictor’s global performance as . The upper bound of the 99 % 𝐶𝐶 confidence interval on , denoted is calculated as: 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 = 𝑁𝑁 ′ 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 1 𝑁𝑁 1−0.01 , if 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 = 0 ′ 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 = � 𝑃𝑃𝑖𝑖𝑔𝑔𝑔𝑔𝑏𝑏𝑚𝑚𝑔𝑔 �1−𝑃𝑃𝑖𝑖𝑔𝑔𝑔𝑔𝑏𝑏𝑚𝑚𝑔𝑔� min (1,𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 + 2.576� 𝑁𝑁−1 ), otherwise where 2.576 corresponds to the value. 𝑍𝑍(1−0.005)NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 54 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 7. Calculate the predictor’s local performance, based on the longest run of correct predictions. Let r be one greater than the length of the longest run of ones in correct. Use a binary search to solve the following for : 𝑃𝑃𝑙𝑙𝑜𝑜𝑐𝑐𝑚𝑚𝑙𝑙 1−𝑃𝑃𝑙𝑙𝑜𝑜𝑐𝑐𝑚𝑚𝑙𝑙𝑥𝑥 1 0.99 = × 𝑁𝑁+1 , where (𝑟𝑟+1−𝑟𝑟𝑥𝑥)𝑞𝑞 𝑥𝑥 and x = x , derived by iterating the re𝑞𝑞cu=rre1n−ce𝑃𝑃 r𝑙𝑙e𝑜𝑜l𝑐𝑐a𝑚𝑚t𝑙𝑙ion 10 𝑟𝑟 𝑟𝑟+1 for j from 1 to 10, and x =1. No𝑥𝑥t𝑗𝑗 e =tha1t +so𝑞𝑞lv𝑃𝑃i𝑙𝑙 n𝑜𝑜𝑐𝑐 g𝑚𝑚 𝑙𝑙 f𝑥𝑥o𝑗𝑗 r− 1 using the logarithm of these 0 equations is robust against overflows. Table 3 in Appendix G.2 provides some pre- calculated values of . 𝑃𝑃𝑙𝑙𝑜𝑜𝑐𝑐𝑚𝑚𝑙𝑙 8. The min-entropy is th𝑃𝑃e𝑙𝑙𝑜𝑜 𝑐𝑐n𝑚𝑚e𝑙𝑙gative logarithm of the greater performance metric min-entropy . ′ 1 Example: Suppose that S = (2, 1, 3, 2, 1,= 3, − 1,l 3o ,g 12 )( ,m soa x th(𝑃𝑃 at𝑖𝑖 𝑙𝑙 L𝑜𝑜 𝑏𝑏 =𝑚𝑚 𝑙𝑙 9, 𝑃𝑃 a𝑙𝑙 n𝑜𝑜 d𝑐𝑐𝑚𝑚 N𝑙𝑙, 𝑘𝑘=) 7) . For the purpose of this example, further suppose that D = 3 (instead of 16). After each iteration of step 4 is completed, the values are: i subpredict scoreboard Winner prediction s correct scoreboard i i-2 (step 4c) (step 4c) (step 4e) 3 (Null, Null, Null) (0, 0, 0) 1 Null 3 0 (0, 0, 0) 4 (Null, Null, Null) (0, 0, 0) 1 Null 2 0 (0, 0, 0) 5 (1, Null, Null) (0, 0, 0) 1 1 1 1 (1, 0, 0) 6 (3, 3, Null) (1, 0, 0) 1 3 3 1 (2, 1, 0) 7 (2, 2, 2) (2, 1, 0) 1 2 1 0 (2, 1, 0) 8 (3, Null, Null) (2, 1, 0) 1 3 3 1 (3, 1, 0) 9 (2, 2, Null) (3, 1, 0) 1 2 1 0 (3, 1, 0) Let {x→y:c} denote a nonzero count c for the transition from x to y. Models M , M , and M are 1 2 3 shown below after step 4a (the model update step) for each value of i.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 55 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B i M M M 1 2 3 3 {2→1:1} -- -- 4 {1→3:1}, {(2, 1)→3:1} -- {2→1:1} 5 {1→3:1}, {(1, 3)→2:1}, {(2, 1, 3)→2:1} {2→1:1}, {(2, 1)→3:1} {3→2:1} 6 {1→3:1}, {(1, 3)→2:1}, {(1, 3, 2)→1:1}, {2→1:2}, {(2, 1)→3:1}, {(2, 1, 3)→2:1} {3→2:1} {(3, 2)→1:1} 7 {1→3:2}, {(1, 3)→2:1}, {(1, 3, 2)→1:1}, {2→1:2}, {(2, 1)→3:2}, {(2, 1, 3)→2:1}, {3→2:1} {(3, 2)→1:1} {(3, 2, 1)→3:1} 8 {1→3:2}, {(1, 3)→1:1}, {(1, 3, 2)→1:1}, {2→1:2}, {(1, 3)→2:1}, {(2, 1, 3)→1:1}, {3→1:1}, {(2, 1)→3:2}, {(2, 1, 3)→2:1}, {3→2:1} {(3, 2)→1:1} {(3, 2, 1)→3:1} 9 {1→3:3}, {(1, 3)→1:1}, {(1, 3, 1)→3:1}, {2→1:2}, {(1, 3)→2:1}, {(1, 3, 2)→1:1}, {3→1:1}, {(2, 1)→3:2}, {(2, 1, 3)→1:1}, {3→2:1} {(3, 1)→3:1}, {(2, 1, 3)→2:1}, {(3, 2)→1:1} {(3, 2, 1)→3:1} After the predictions are all made, correct = (0, 0, 1, 1, 0, 1, 0). Then, , , , and the resulting min-entropy estimate is 0.0755. ′ 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 = 0.4286 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 = 06..93.41900 T𝑃𝑃h𝑙𝑙 e𝑜𝑜𝑐𝑐 𝑚𝑚 L𝑙𝑙 Z=780Y.1 P30re7diction Estimate The LZ78Y predictor is loosely based on LZ78 encoding with Bernstein's Yabba scheme [Sal07] for adding strings to the dictionary. The predictor keeps a dictionary of strings that have been added to the dictionary so far, and continues adding new strings to the dictionary until the dictionary has reached its maximum capacity. Each time that a sample is processed, every substring in the most recent B samples updates the dictionary or is added to the dictionary. Given the input S = (s , …, s ), where s ϵ A = {x , …, x }, 1 L i 1 k 1. Let B = 16, and N = L – B – 1. Let correct be an array of N Boolean values, each initialized to 0. Let maxDictionarySize = 65 536. 2. Let D be an empty dictionary. Let dictionarySize = 0. 3. For i = B+2 to L: a. For j=B down to 1: i. If (s , …, s ) is not in D, and dictionarySize < maxDictionarySize: i-j-1 i-2NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 56 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 1. Let D[s , …, s ] be added to the dictionary. i-j-1 i-2 2. Let D[s , …, s ][s ] = 0. i-j-1 i-2 i-1 3. dictionarySize = dictionarySize + 1 ii. If (s , … , s ) is in D, i-j-1 i-2 1. Let D[s , …, s ][s ] = D[s , … ,s ][s ] + 1. i-j-1 i-2 i-1 i-j-1 i-2 i-1 b. Use the dictionary to predict the next value, s. Let prediction = Null, and let i maxcount = 0. For j = B down to 1: i. Let prev = (s i−j, … s i−1). ii. If prev is in the dictionary, find the y ϵ {x , …,x } that has the highest 1 k D[prev][y] value. In the event of a tie, let the y be the symbol with the higher byte value. For example, if D[prev] and D[prev] both have the highest value, then y= 5. iii. If D[prev][y] > maxcount: 1. prediction = y. 2. maxcount = D[prev][y]. c. If (prediction = s i), let correct i−B−1 = 1. 4. Calculate the predictor’s global performance as . The upper bound of the 99 % 𝐶𝐶 confidence interval on , denoted is calculated as: 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 = 𝑁𝑁 ′ 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 1 𝑁𝑁 1−0.01 , if 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 = 0, ′ 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 = � 𝑃𝑃𝑖𝑖𝑔𝑔𝑔𝑔𝑏𝑏𝑚𝑚𝑔𝑔 �1−𝑃𝑃𝑖𝑖𝑔𝑔𝑔𝑔𝑏𝑏𝑚𝑚𝑔𝑔� min (1,𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 + 2.576� 𝑁𝑁−1 ), otherwise where 2.576 corresponds to the value. 5. Calculate the predictor’s local pe 𝑍𝑍rf(1o−rm0.0a0n5c)e, based on the longest run of correct predictions. Let r be one greater than the length of the longest run of ones in correct. Use a binary search to solve the following for : 𝑃𝑃𝑙𝑙𝑜𝑜𝑐𝑐𝑚𝑚𝑙𝑙 1−𝑃𝑃𝑙𝑙𝑜𝑜𝑐𝑐𝑚𝑚𝑙𝑙𝑥𝑥 1 0.99 = × 𝑁𝑁+1 , where and x = x , de(r𝑟𝑟iv+ed1 b−y i𝑟𝑟te𝑥𝑥r)a𝑞𝑞ting t𝑥𝑥he re currence relation 10 𝑞𝑞 = 1−𝑃𝑃𝑙𝑙𝑜𝑜𝑐𝑐𝑚𝑚𝑙𝑙 𝑟𝑟 𝑟𝑟+1 for j from 1 to 10, and x = 1. N𝑥𝑥o𝑗𝑗 te= th1at+ so𝑞𝑞l𝑃𝑃v𝑙𝑙 i𝑜𝑜 n𝑐𝑐 g𝑚𝑚 𝑙𝑙 f𝑥𝑥o𝑗𝑗− r 1 using the logarithm of these 0 equations is robust against overflows. Table 3 given in Appendix G.2 provides some pre- calculated values of . 𝑃𝑃𝑙𝑙𝑜𝑜𝑐𝑐𝑚𝑚𝑙𝑙 6. The min-entropy is th𝑃𝑃e𝑙𝑙𝑜𝑜 𝑐𝑐n𝑚𝑚e𝑙𝑙gative logarithm of the greater performance metricNIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 57 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B min-entropy ′ 1 = −log2�max�𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙,𝑃𝑃𝑙𝑙𝑜𝑜𝑐𝑐𝑚𝑚𝑙𝑙,𝑘𝑘��. Example: Suppose that S = (2, 1, 3, 2, 1, 3, 1, 3, 1, 2, 1, 3, 2), and L = 13. For the purpose of this example, suppose that B = 4 (instead of 16), then N = 8. i Add to D prev Max D[prev] entry prediction s correct i i-B-1 6 D[2, 1, 3, 2] (1, 3, 2, 1) Null Null 3 0 D[1, 3, 2] (3, 2, 1) Null D[3, 2] (2, 1) Null D (1) Null 7 D[1, 3, 2, 1] (3, 2, 1, 3) Null Null 1 0 D[3, 2, 1] (2, 1, 3) Null D[2, 1] (1, 3) Null D (3) Null 8 D[3, 2, 1, 3] (2, 1, 3, 1) Null 3 3 1 D[2, 1, 3] (1, 3, 1) Null D[1, 3] (3, 1) Null D (1) 3 9 D[2, 1, 3, 1] (1, 3, 1, 3) Null 1 1 1 D[1, 3, 1] (3, 1, 3) Null D[3, 1] (1, 3) 1 D (3) 1 10 D[1, 3, 1, 3] (3, 1, 3, 1) Null 3 2 0 D[3, 1, 3] (1, 3, 1) 3 D[1, 3] (3, 1) 3 D (1) 3 11 D[3, 1, 3, 1] (1, 3, 1, 2) Null 1 1 1 D[1, 3, 1] (3, 1, 2) Null D[3, 1] (1, 2) Null D (2) 1 12 D[1, 3, 1, 2] (3, 1, 2, 1) Null 3 3 1 D[3, 1, 2] (1, 2, 1) Null D[1, 2] (2, 1) 3 D (1) 3 13 D[3, 1, 2, 1] (1, 2, 1, 3) Null 1 2 0 D[1, 2, 1] (2, 1, 3) 1 D[2, 1] (1, 3) 1 D (3) 1 After the predictions are all made, correct = (0, 0, 1, 1, 0, 1, 1, 0). Then, , , , and the resulting min-entropy estimate is 0.0191. ′ 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 = 0.5 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 = 0.9868 𝑃𝑃𝑙𝑙𝑜𝑜𝑐𝑐𝑚𝑚𝑙𝑙 = 0.1229NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 58 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 6.4 Reducing the Symbol Space It is often the case that the data requirements for a test on noise source samples depends on the number of possible different symbols from the noise source (i.e., the size of the alphabet A, denoted k). For example, consider two different noise sources. The first source outputs 4-bit samples, and thus has a possible total of 24 = 16 different symbols, and the second source outputs 32-bit samples, for a possible total of 232 different symbols. In many cases, the variability in the output that contributes to the entropy in a sample may be concentrated among some portion of the bits in the sample. For example, consider a noise source that outputs 32-bit high-precision clock samples that represent the time it takes to perform some system process. Suppose that the bits in a sample are ordered in the conventional way, so that the lower-order bits of the sample correspond to the higher resolution measurements of the clock. It is easy to imagine that in this case, the low-order bits would contain most of the variability. In fact, it would seem likely that some of the high-order bits may be constantly 0. For this example, it would be reasonable to truncate the 32-bit sample to a 4-bit string by taking the lower 4 bits, and perform the tests on the 4-bit strings. Of course, it must be noted that in this case, only a maximum of 4 bits of min-entropy per sample could be credited to the noise source. The algorithm given below provides an example of a method for mapping the n-bit samples, collected as specified in Section 3.1.1, to m-bit samples, where n ≥ m. The resulting strings can be used as input to tests that may have infeasible data requirements if the mapping were not performed. Note that after the mapping is performed, the maximum amount of entropy possible per n-bit sample is m bits. Given a noise source that produces n-bit samples, where n exceeds the bit-length that can be handled by the test, the submitter may provide the tester with an ordered ranking of the bits in the n-bit samples (see Section 3.2.2). The rank of ‘1’ corresponds to the bit assumed to be contributing the most entropy to the sample, and the rank of n corresponds to the bit contributing the least amount. If multiple bits contribute the same amount of entropy, the ranks can be assigned arbitrarily among those bits. The following algorithm, or its equivalent, is used to assign ranks. Input: A noise source and corresponding statistical model with samples of the form X = a a …a , 1 2 n where each a is a bit. i Output: An ordered ranking of the bits a through a , based on the amount of entropy that each 1 n bit is assumed to contribute to the noise source outputs. 1. Set M = {a , a , …, a }. 1 2 n 2. For i = 1 to n: a. Choose an output bit a from M such that no other bit in M is assumed to contribute more entropy to the noise source samples than a. b. Set the ranking of a to i. c. Remove a from M. Given the ranking, n-bit samples are mapped to m-bit samples by simply taking the m-bits of greatest rank in order (i.e., bit 1 of the m-bit string is the bit from an n-bit sample with rank 1, bitNIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 59 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 2 of the m-bit string is the bit from an n-bit sample with rank 2, … and bit m of the m-bit string is the bit from an n-bit sample with rank m). Note that for the estimators in Section 6, a reference to a sample in the dataset will be interpreted as a reference to the m-bit subsets of the sample when the test necessitates processing the dataset as specified in this section. The submitter is allowed to use an alternative method to reduce symbol size. The submitter shall provide a description of the alternative method they use and an argument as to why this method is more suitable for the noise source shall be provided.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 60 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B Acronyms Selected acronyms and abbreviations used in this paper are defined below. AES Advanced Encryption Standard API Application Programming Interface ANS American National Standard CAVP Cryptographic Algorithm Validation Program CBC-MAC Cipher Block Chaining Message Authentication Code CMVP Cryptographic Module Validation Program DRBG Deterministic Random Bit Generator FIPS Federal Information Processing Standard HMAC Hash-based Message Authentication Code IID Independent and Identically Distributed LRS Longest Repeated Substring NIST National Institute of Standards and Technology NRBG Non-deterministic Random Bit Generator NVLAP National Voluntary Laboratory Accreditation Program RAM Random Access Memory RBG Random Bit Generator SP NIST Special PublicationNIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 61 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B Glossary Alphabet A finite set of two or more symbols. Alphabet size The number of distinct symbols that the noise source produces. A clearly specified mathematical process for computation; a set Algorithm of rules that, if followed, will give a prescribed result. Approved FIPS-approved or NIST-Recommended. A fixed-length data structure that stores a collection of elements, Array where each element is identified by its integer index. An evaluation of the amount of entropy provided by a (digitized) Assessment (of entropy) noise source and/or the entropy source that employs it. A value that is chosen from an alphabet space is said to be biased Biased if one value is more likely to be chosen than another value. (Contrast with unbiased.) Binary data (from a Digitized output from a noise source that consists of a single bit; noise source) that is, each sampled output value is represented as either 0 or 1. An ordered sequence of 0’s and 1’s. The leftmost bit is the most Bitstring significant bit. Collision An instance of duplicate sample values occurring in a dataset. A method of processing the raw data to reduce bias and/or ensure Conditioning (of noise that the entropy rate of the conditioned output is no less than some source output) specified amount. An interval estimate [low, high] of a population parameter. If the population is repeatedly sampled, and confidence intervals are Confidence interval computed for each sample with significance level α, approximately 100(1− α) % of the intervals are expected to contain the true population parameter. A type of health test performed within an entropy source on the output of its noise source in order to gain some level of assurance Continuous test that the noise source is working correctly, prior to producing each output from the entropy source. Consuming application An application that uses the output from an approved random bit (for an RBG) generator. Dataset A sequence of sample values. (See Sample.)NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 62 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B An RBG that includes a DRBG mechanism and (at least initially) has access to a source of entropy input. The DRBG produces a Deterministic Random sequence of bits from a secret initial value called a seed, along Bit Generator (DRBG) with other possible inputs. A DRBG is often called a Pseudorandom Number (or Bit) Generator. The party that develops the entire entropy source or the noise Developer source. A dynamic-length data structure that stores a collection of Dictionary elements or values, where a unique label identifies each element. The label can be any data type. Digitization The process of generating bits from the noise source. The portion of an RBG that includes the functions necessary to instantiate and uninstantiate the RBG, generate pseudorandom DRBG mechanism bits, (optionally) reseed the RBG and test the health of the DRBG mechanism. Approved DRBG mechanisms are specified in SP 800-90A. A measure of the disorder, randomness or variability in a closed Entropy system. Min-entropy is the measure used in this Recommendation. The rate at which a digitized noise source (or entropy source) provides entropy; it is computed as the assessed amount of entropy provided by a bitstring output from the source, divided by Entropy rate the total number of bits in the bitstring (yielding the assessed bits of entropy per output bit). This will be a value between zero (no entropy) and one. The combination of a noise source, health tests, and an optional Entropy source conditioning component that produce random bitstrings to be used by an RBG. The estimated value of a parameter, as computed using an Estimate estimator. Estimator A technique for estimating the value of a parameter. An erroneous acceptance of the hypothesis that a statistically significant event has been observed. This is also referred to as a type 1 error. When “health-testing” the components of a device, False positive it often refers to a declaration that a component has malfunctioned – based on some statistical test(s) – despite the fact that the component was actually working correctly.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 63 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B Global performance For a predictor, the number of accurate predictions over a long metric period. Testing within an implementation immediately prior to or during Health testing normal operation to determine that the implementation continues to perform as implemented and as validated. Two random variables X and Y are independent if they do not convey information about each other. Receiving information Independent about X does not change the assessment of the probability distribution of Y (and vice versa). Independent and A quality of a sequence of random variables for which each Identically Distributed element of the sequence has the same probability distribution as (IID) the other values, and all values are mutually independent. A dynamic-length data structure that stores a sequence of values, List where each value is identified by its integer index. Local performance For a predictor, the length of the longest run of correct predictions metric A model for a probability distribution where the probability that the ith element of a sequence has a given value depends only on Markov model the values of the previous n elements of the sequence. The model is called an nth order Markov model. The min-entropy (in bits) of a random variable X is the largest value m having the property that each observation of X provides at least m bits of information (i.e., the min-entropy of X is the greatest lower bound for the information content of potential Min-entropy observations of X). The min-entropy of a random variable is a lower bound on its entropy. The precise formulation for min- entropy is (log max p) for a discrete distribution having 2 i probabilities p ,...,p . Min-entropy is often used as a worst-case 1 k measure of the unpredictability of a random variable. The maximum amount of information from the input that can affect the output. For example, if f(x) = SHA-1(x) || 01, and x Narrowest internal consists of a string of 1000 binary bits, then the narrowest internal width width of f(x) is 160 bits (the SHA-1 output length), and the output width of f(x) is 162 bits (the 160 bits from the SHA-1 operation, concatenated by 01). The component of an entropy source that contains the non- Noise source deterministic, entropy-producing activity (e.g., thermal noise or hard drive seek times).NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 64 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B An RBG that always has access to an entropy source and (when Non-deterministic working properly) produces outputs that have full entropy (see SP Random Bit Generator 800-90C). Also called a true random bit (or number) generator (NRBG) (Contrast with a DRBG). Non-physical non- An entropy source that does not use dedicated hardware but uses deterministic random system resources (RAM content, thread number etc.) or the bit generator interaction of the user (time between keystrokes etc.). A type of health test that is available to be run whenever a user or On-demand test a relying component requests it. The set of all possible distinct bitstrings that may be obtained as Output space samples from a digitized noise source. The probability that the chosen test statistic will assume values P-value that are equal to or more extreme than the observed test statistic value, assuming that the null hypothesis is true. A function that predicts the next value in a sequence, based on Predictor previously observed values in the sequence. A function that assigns a probability to each measurable subset of Probability distribution the possible outcomes of a random variable. Probability model A mathematical representation of a random phenomenon. A deterministic process (or data produced by such a process) whose output values are effectively indistinguishable from those of a random process as long as the internal states and internal Pseudorandom actions of the process are unknown. For cryptographic purposes, “effectively indistinguishable” means “not within the computational limits established by the intended security strength.” A device or algorithm that outputs a random sequence that is Random Bit Generator effectively indistinguishable from statistically independent and (RBG) unbiased bits. An RBG is classified as either a DRBG or an NRBG. Raw data Digitized output of the noise source. Physical non- An entropy source that uses dedicated hardware or uses a physical deterministic random experiment (noisy diode(s), oscillators, event sampling like bit generator radioactive decay, etc.)NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 65 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B Run (of output A sequence of identical values. sequences) An observation of the raw data output by a noise source. Common examples of output values obtained by sampling are single bits, Sample single bytes, etc. (The term “sample” is often extended to denote a sequence of such observations; this Recommendation will refrain from that practice.) A conceptual boundary that is used to assess the amount of entropy provided by the values output from an entropy source. Security boundary The entropy assessment is performed under the assumption that any observer (including any adversary) is outside of that boundary. Sequence An ordered list of quantities. The term used to indicate a requirement that needs to be fulfilled Shall to claim conformance to this Recommendation. Note that shall may be coupled with not to become shall not. The term used to indicate an important recommendation. Ignoring Should the recommendation could result in undesirable results. Note that should may be coupled with not to become should not. A suite of health tests that are performed every time the entropy source is initialized or powered up. These tests are carried out on Start-up testing the noise source before any output is released from the entropy source. A stochastic model is a mathematical description (of the relevant properties) of an entropy source using random variables. A stochastic model used for an entropy source analysis is used to support the estimation of the entropy of the digitized data and finally of the raw data. In particular, the model is intended to Stochastic model provide a family of distributions, which contains the true (but unknown) distribution of the noise source outputs. Moreover, the stochastic model should allow an understanding of the factors that may affect the entropy. The distribution of the entropy source needs to remain in the family of distributions, even if the quality of the digitized data goes down. The party that submits the entire entropy source and output from its components for validation. The submitter can be any entity that Submitter can provide validation information as required by this Recommendation (e.g., developer, designer, vendor or any organization).NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 66 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B Symbol The value of the noise source output (i.e., sample value). Testing laboratory An accredited cryptographic security testing laboratory. Type I error Incorrectly rejection of a true null hypothesis. A value that is chosen from a sample space is said to be unbiased Unbiased if all potential values have the same probability of being chosen. (Contrast with biased.)NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 67 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B References [BZ2] BZIP2 Compression Algorithm. http://www.bzip.org/. [Cac97] C. Cachin, Entropy Measures and Unconditional Security in Cryptography, PhD Thesis, Reprint as vol.1 of ETH Series in Information Security and Cryptography, ISBN 3-89649-185-7, Hartung-Gorre Verlag, Konstanz, ETH Zurich, 1997. [CoNa98] J.S. Coron, D. Naccache, An Accurate Evaluation of Maurer's Universal Test, Selected Areas in Cryptography 1998: 57-71 [Fel50] W. Feller, An Introduction to Probability Theory and its Applications, volume one, chapter 13, John Wiley and Sons, Inc., 1950. [FIPS140] Federal Information Processing Standard 140-2, Security Requirements for Cryptographic Modules, May 25, 2001. https://doi.org/10.6028/NIST.FIPS.140-2. [FIPS180] Federal Information Processing Standard 180-4, Secure Hash Standard (SHS), August 2015. https://doi.org/10.6028/NIST.FIPS.180-4. [FIPS197] Federal Information Processing Standard 197, Specification for the Advanced Encryption Standard (AES), November 2001. https://doi.org/10.6028/NIST.FIPS.197. [FIPS198] Federal Information Processing Standard 198-1, The Keyed-Hash Message Authentication Code (HMAC), July 2008. https://doi.org/10.6028/NIST.FIPS.198-1. [FIPS202] Federal Information Processing Standard 202, SHA-3 Standard: Permutation- Based Hash and Extendable-Output Functions, August 2015. https://doi.org/10.6028/NIST.FIPS.202. [HaFis15] P. Haddad, V. Fischer, F. Bernard, Jean Nicolai, A Physical Approach for Stochastic Modeling of TERO-Based TRNG. CHES 2015: 357-372 [HD12] P. Hagerty and T. Draper, Entropy Bounds and Statistical Tests, NIST Random Bit Generation Workshop, December 2012, https://csrc.nist.gov/csrc/media/events/random-bit-generation-workshop- 2012/documents/hagerty_entropy_paper.pdf. (Presentation available at https://csrc.nist.gov/csrc/media/events/random-bit-generation-workshop- 2012/documents/hagerty_entropy_paper.pdf.) [IG140-2] National Institute of Standards and Technology, Communications Security Establishment Canada, Implementation Guidance for FIPS PUB 140-2 andNIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 68 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B the Cryptographic Module Validation Program, (last updated) September 11, 2017, https://csrc.nist.gov/CSRC/media/Projects/Cryptographic-Module- Validation-Program/documents/fips140-2/FIPS1402IG.pdf. [Kel15] J. Kelsey, Kerry A. McKay, M. Sonmez Turan, Predictive Models for Min- Entropy Estimation, Proceedings of the Workshop on Cryptographic Hardware and Embedded Systems 2015 (CHES 2015), France. https://doi.org/10.1007/978-3-662-48324-4_19. [Mau92] U. Maurer, A Universal Statistical Test for Random Bit Generators, Journal of Cryptology, Vol. 5, No. 2, 1992, pp. 89-105. [RaSt98] M. Raab, A. Steger: Balls into Bins - A Simple and Tight Analysis. RANDOM 1998: 159-170. [Sal07] D. Salomon, Data Compression: The Complete Reference, Chapter 3, Springer, 2007 [Shan51] C.E Shannon, Prediction and Entropy of Printed English, Bell System Technical Journal, volume 30, pp. 50-64, January 1951, https://archive.org/details/bstj30-1-50. [SP800-38B] National Institute of Standards and Technology Special Publication (SP) 800- 38B Recommendations for Block Cipher Modes of Operation: The CMAC Mode for Authentication, May 2005 (updated October 6, 2016), https://doi.org/10.6028/NIST.SP.800-38B. [SP800-57] National Institute of Standards and Technology Special Publication (SP) 800- 57 Part 1 Revision 4, Recommendation for Key Management – Part 1: General, January 2016. https://doi.org/10.6028/NIST.SP.800-57pt1r4. [SP800-90A] National Institute of Standards and Technology Special Publication (SP) 800- 90A Revision 1, Recommendation for Random Number Generation Using Deterministic Random Bit Generators, June 2015. https://doi.org/10.6028/NIST.SP.800-90Ar1. [SP800-90C] National Institute of Standards and Technology Special Publication (SP) 800-90C (Draft), Recommendations for Random Bit Generator (RBG) Constructions, April 2016. https://csrc.nist.gov/publications/detail/sp/800- 90c/draft. [SP800-107] National Institute of Standards and Technology Special Publication (SP) 800- 107 Revision 1, Recommendations for Applications using Approved Hash Algorithms, August 2012. https://doi.org/10.6028/NIST.SP.800-107r1.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 69 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B Min-Entropy and Optimum Guessing Attack Cost Suppose that an adversary wants to determine at least one of several secret values, where each secret value is independently chosen from a set of M possibilities, with probability distribution P = {p , p , …, p }. Assume that these probabilities are sorted so that p ≥ p ≥ …≥ p . Consider a 1 2 M 1 2 M guessing strategy aimed at successfully guessing as many secret values as possible. The adversary's goal would be to minimize the expected number of guesses per successful recovery. Such a strategy would consist of guessing a maximum of k possibilities for a given secret value, moving on to a new secret value when either a guess is correct, or k incorrect guesses for the current value have been made. In general, the optimum value of k can be anywhere in the range 1 ≤ k ≤ M, depending on the probability distribution P. Note that when k = M, the Mth guess is considered a valid (though trivial) guess. Regardless of the value of k chosen, it is clear that the k guesses selected for a given secret value should be the k most likely possible values, in decreasing order of probability. The expected work per success can be computed for this attack as follows. For 1 ≤ j ≤ k – 1, the attacker will make exactly j guesses if the secret value is the jth most likely value, an event having probability p. The attacker will make exactly k guesses if the secret value is not any of the k – 1 j most likely values, an event having probability Thus, the expected number of guesses for the attack is given by the following: 𝑘𝑘−1 1−∑𝑗𝑗=1𝑝𝑝𝑗𝑗. 𝑘𝑘−1 𝑝𝑝1 +2𝑝𝑝2 +⋯+(𝑘𝑘−1)𝑝𝑝𝑘𝑘−1 +𝑘𝑘 �1−�𝑝𝑝𝑗𝑗�. 𝑗𝑗=1 Since this attack will be successful if and only if the secret value is one of the k most likely possibilities, which is the case with probability , the expected number of times the attack must be performed until the first success is the 𝑘𝑘reciprocal of this probability. Multiplying this ∑𝑗𝑗=1𝑝𝑝𝑗𝑗 reciprocal by the expected number of guesses per attack gives the following as the expected work per success: 𝑘𝑘−1 𝑝𝑝1 +2𝑝𝑝2 +⋯+(𝑘𝑘−1)𝑝𝑝𝑘𝑘−1 +𝑘𝑘 �1−∑𝑗𝑗=1 𝑝𝑝𝑗𝑗� 𝑊𝑊𝑘𝑘(𝑃𝑃) = 𝑘𝑘 . It is not critical to determine the value k* that min∑im𝑗𝑗= iz1 e𝑝𝑝s 𝑗𝑗 , since the min-entropy of P leads to an accurate approximation (and sometimes the exact value) of . Stated more precisely, 𝑊𝑊𝑘𝑘(𝑃𝑃) is an upper bound of , and it can be shown that∗ for all k 𝑊𝑊𝑘𝑘 (𝑃𝑃) 1 1 1 such that 1 ≤ k ≤ M. Since the min-en∗tropy of P is , these two bounds imply that the 𝑊𝑊1(𝑃𝑃) = 𝑝𝑝1 𝑊𝑊𝑘𝑘 (𝑃𝑃) 𝑊𝑊𝑘𝑘(𝑃𝑃) ≥ 2𝑝𝑝1+2 error between the min-entropy of P and can be bounded as follows: − log2(𝑝𝑝1) ∗ log2(𝑊𝑊𝑘𝑘 (𝑃𝑃)) ∗ 0 ≤ −log2𝑝𝑝1−log2�𝑊𝑊𝑘𝑘 (𝑃𝑃)� ≤ 1− log2(𝑝𝑝1 +1). Notice that since , the upper bound on the error approaches 0 as , and 1 alternatively, this bound approaches 1 as and . In other words, the min-entropy of 𝑀𝑀 ≤ 𝑝𝑝1 ≤ 1 𝑝𝑝1 → 1 1 𝑀𝑀 → ∞ 𝑝𝑝1 → 𝑀𝑀NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 70 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B P either corresponds to the exact expected work, measured in bits, needed to perform the optimum guessing attack or over-estimates this work by at most one bit. In order to prove the claim that , for 1 ≤ k ≤ M, rewrite the expected work per 1 1 success as 𝑊𝑊𝑘𝑘(𝑃𝑃) ≥ 2𝑝𝑝1+2 1+(1−𝑝𝑝1)+(1−𝑝𝑝1 −𝑝𝑝2)+⋯+(1−𝑝𝑝1 −𝑝𝑝2 −⋯−𝑝𝑝𝑘𝑘−1) 𝑊𝑊𝑘𝑘(𝑃𝑃) = . Consider an alternative probability dis𝑝𝑝t1ri+bu𝑝𝑝ti2on+ ⋯on+ 𝑝𝑝a𝑘𝑘 set of M possibilities where occurs times and . It is straightforward′ to 𝑃𝑃 = 1 see that , since each term in the numerator of is at least as large as the {𝑝𝑝1,𝑝𝑝1,…,𝑝𝑝1,𝑟𝑟,0,…,0}, 𝑝𝑝1 𝐿𝐿 = �𝑝𝑝1� 𝑟𝑟 = 1−𝐿𝐿𝑝𝑝1 corresponding term in ′ , and the denominator of is at least as large as the denomin 𝑊𝑊at𝑘𝑘 o(r𝑃𝑃 o)f ≥ 𝑊𝑊𝑘𝑘(.𝑃𝑃 ) ′ 𝑊𝑊 ′𝑘𝑘(𝑃𝑃) 𝑊𝑊𝑘𝑘(𝑃𝑃 ) 𝑊𝑊𝑘𝑘(𝑃𝑃 ) Now to show th 𝑊𝑊at𝑘𝑘(𝑃𝑃) . Based on the above formula for , for 1 ≤ k ≤ t + 1, 1 1 the numerator of can be written as 𝑊𝑊𝑘𝑘(𝑃𝑃′) ≥ 2𝑝𝑝1 +2 𝑊𝑊𝑘𝑘(𝑃𝑃) ′ 𝑊𝑊𝑘𝑘(𝑃𝑃 ) 𝑘𝑘−1 𝑘𝑘(𝑘𝑘−1) 1 𝑘𝑘−1 � (1−𝑖𝑖𝑝𝑝1) = 𝑘𝑘− 𝑝𝑝1 = 𝑘𝑘𝑝𝑝1� − �. Consider the following 𝑖𝑖t=w0o cases where 1 ≤ k ≤ 2t and k = t + 1. T𝑝𝑝h1ese are2 the only cases to check, since if M > t + 1, then = for k > t + 1, because the remaining probabilities are all zero. Furthermore, r = 0 if′ and only if′ is an integer, and when this happens, only the first 𝑊𝑊𝑘𝑘(𝑃𝑃 ) 𝑊𝑊𝑜𝑜+1(𝑃𝑃 ) 1 case needs to be addressed since . 𝑝𝑝1 ′ ′ For 1 ≤ k ≤ t, the denominator of 𝑊𝑊𝑜𝑜+1(𝑃𝑃 ) = 𝑊𝑊𝑜𝑜. (T𝑃𝑃he)n, ′ 𝑊𝑊𝑘𝑘(𝑃𝑃 ) = 𝑘𝑘𝑝𝑝1 1 𝑘𝑘−1 ′ 𝑘𝑘𝑝𝑝1�𝑝𝑝1 − 2 � 1 𝑘𝑘−1 𝑊𝑊𝑘𝑘(𝑃𝑃 ) = = − , 𝑘𝑘𝑝𝑝1 𝑝𝑝1 2 1 1 1 ≥ − �� �−1�, 𝑝𝑝1 2 𝑝𝑝1 1 1 1 ≥ − � −1�, 𝑝𝑝1 2 𝑝𝑝 1 1 1 ≥ + . 2𝑝𝑝1 2 For k = t +1, the denominator of is tp +r =1. Let x = so 0 ≤ x < 1. This implies 1 ′ 1 1 𝑊𝑊𝑘𝑘(𝑃𝑃 ) 𝑝𝑝1− �𝑝𝑝1�, ′ 1 𝑘𝑘 −1 1 1 1 1 𝑊𝑊𝑘𝑘(𝑃𝑃 ) = 𝑘𝑘𝑝𝑝1� − � = �� �+1�𝑝𝑝1� − � ��, 𝑝𝑝1 2 𝑝𝑝1 𝑝𝑝1 2 𝑝𝑝1NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 71 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 1 1 𝑝𝑝1𝑥𝑥 = � −𝑥𝑥+1�� + �, 𝑝𝑝1 2 2 1 1 𝑝𝑝1𝑥𝑥(1−𝑥𝑥) = + + , 2𝑝𝑝1 2 2 1 1 ≥ + . Therefore, it has been shown that 2𝑝𝑝1 2 for 1 ≤ k ≤ M. Note that this lower bound is sharp, since achieves this value wh′en P i1 s a un1 iform distribution. 𝑊𝑊𝑘𝑘(𝑃𝑃) ≥ 𝑊𝑊𝑘𝑘(𝑃𝑃 ) ≥ 2𝑝𝑝1+2 𝑊𝑊𝑘𝑘(𝑃𝑃)NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 72 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B The Narrowest Internal Width The narrowest internal width of a conditioning component is the minimum number of bits of the state that is dependent on the input to the functions, and influences the output of the function (across all steps of making up the conditioning function). It can also be considered as the logarithm of an upper bound on the number of distinct outputs, based on the size of the internal state. Example: Let F(X) be a function defined as follows: 1. Let h be the output of SHA-256(X) truncated to 64 bits. 1 2. Return SHA-256(h || h ) truncated to 128 bits. 1 1 This function takes an arbitrarily-long input X and will yield 128-bit output value, but its internal width is only 64 bits, because the value of the output only depends on the value of 64-bit h . 1NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 73 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B CBC-MAC Specification CBC-MAC using a 128-bit approved block-cipher algorithm is one of the vetted conditioning components. This CBC-MAC construction shall not be used for any other purpose than as the algorithm for a conditioning component, as specified in Section 3.1.5.1.1. The following notation is used for the construction. Let E(Key, input_string) represent the approved encryption algorithm, with a Key and an input_string as input parameters. The length of the input_string shall be an integer multiple of the output length n of the block-cipher algorithm and shall always be the same length (i.e., variable length strings shall not be used as input). Let n be the length (in bits) of the output block of the approved block cipher algorithm, and let w be the number of n-bit blocks in the input_string. Let output_string be the n-bit output of CBC-MAC. CBC-MAC: Input: bitstring Key, input_string. Output: bitstring output_string. Process: 1. Let be the sequence of blocks formed by dividing input string into n-bit blocks; i.e., each consists of n bits. 𝑠𝑠0,𝑠𝑠1,…𝑠𝑠𝑙𝑙−1 2. V = 0. 𝑠𝑠𝑖𝑖 3. For i = 0 to w 1 V = E(Key, V ⊕ ). 4. Output V as th−e CBC-MAC output. 𝑠𝑠𝑖𝑖NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 74 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B Different Strategies for Entropy Estimation Each of the estimation methods presented in Section 6 follows one of two approaches to estimating min-entropy. The first approach is based on entropic statistics, first described for IID data in [HD12], and later applied to non-IID data [HD12]. The second approach is based on predictors, first described in [Kel15]. G.1 Entropic Statistics The entropic statistics presented in [HD12], each designed to compute a different statistic on the samples, provide information about the structure of the data: collision, compression, and Markov. While the estimators (except for the Markov) were originally designed for application to independent outputs, the tests have performed well when applied to data with dependencies. The estimators assume that a probability distribution describes the output of a random noise source, but that the probability distribution is unknown. The goal of each estimator is to reveal information about the unknown distribution, based on a statistical measurement. The collision and compression estimators in Section 6 each solve an equation for an unknown parameter, where the equation is different for each estimator. These equations come from the target statistic’s expected value using a near-uniform distribution, which provides a lower bound for min- entropy. A near-uniform distribution is an instance of a one-parameter family of probability distributions parameterized by p, P : p 𝑝𝑝, if 𝑖𝑖 = 0 𝑃𝑃𝑝𝑝(𝑖𝑖) = �1−𝑝𝑝 , otherwise where k is the number of states in the outp𝑘𝑘ut− sp1ace, and , which is the case when 1−𝑝𝑝 1 In other words, one output state has the maximum probability, and the remaining output states are 𝑝𝑝 ≥ 𝑘𝑘−1 𝑝𝑝 ≥ 𝑘𝑘. equally likely. For more information, see [HD12]. G.1.1 Approximation of The function F(1/z), used by t𝐅𝐅he( 𝟏𝟏c/o𝐳𝐳ll)ision estimate (Section 6.3.2), can be approximated by the following continued fraction:14 14 Derived from Equation 8.9.2 at http://dlmf.nist.gov/8.9.NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 75 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B 1 −𝑘𝑘 1 𝑧𝑧 + 1+ 1−𝑘𝑘 𝑧𝑧+ 2 1+ 2−𝑘𝑘 𝑧𝑧+ 3 1+ G.2 Predictors … Shannon first published the relationship between the entropy and predictability of a sequence in 1951 [Shan51]. Predictors construct models from previous observations, which are used to predict the next value in a sequence. The prediction-based estimation methods in this Recommendation work in a similar way, but attempt to find bounds on the min-entropy of integer sequences generated by an unknown process (rather than the N-gram entropy of English text, as in [Shan51]). The predictor approach uses two metrics to produce an estimate. The first metric is based on the global performance of the predictor, called accuracy in machine-learning literature. Essentially, a predictor captures the proportion of guesses that were correct. This approximates how well one can expect a predictor to guess the next output from a noise source, based on the results over a long sequence of guesses. The second metric is based on the greatest number of correct predictions in a row, which is called the local performance metric. This metric is useful for detecting cases where a noise source falls into a highly predictable state for some time, but the predictor may not perform well on long sequences. The calculations for the local entropy estimate come from the probability theory of runs and recurrent events [Fel50]. For more information about min-entropy estimation using predictors, see [Kel15]. In order to make the predictor estimates lean toward a conservative underestimate of min-entropy, is replaced by , the proportion corresponding to the 99th percentile of the number of correct predictions base′d on the observed number of correct predictions. Note that the order in 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 which correct predictions occur does not influence the min-entropy estimate based on . For example, a predictor could always be correct for the first half of the outputs in a data set, and 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 always incorrect for the second half of the outputs. The min-entropy estimate of this sequence, based on , is half the data length in bits. On the other hand, for another sequence, the predictor could have a 50 % chance of being correct for every sample in this sequence. The min- 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 entropy estimate of this second sequence, based on , is the same as that of the first sequence. However, the typical successful prediction run lengths are very different for these two sequences. 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 Therefore, the approach takes the local prediction performance into account in order to conservatively decrease the min-entropy estimate if the observed local prediction behavior is statistically significant, given the global prediction success rate. The predictor estimates accomplish this by basing the min-entropy estimate on , where is the successful prediction proportion for which the observed longes′t run of correct predictions is the 99th percentile. This is effectively a one-tail hypothesis tem st a tx h ( a𝑃𝑃 t 𝑖𝑖 re𝑙𝑙𝑜𝑜 j𝑏𝑏 e𝑚𝑚 c𝑙𝑙 t, s𝑃𝑃 𝑙𝑙𝑜𝑜𝑐𝑐𝑚𝑚𝑙𝑙) in favo𝑃𝑃 r 𝑙𝑙 o𝑜𝑜 f𝑐𝑐 𝑚𝑚𝑙𝑙 if the observed longest run, given a success probability of , is beyon′d the 99th percentile. 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙 𝑃𝑃𝑙𝑙𝑜𝑜𝑐𝑐𝑚𝑚𝑙𝑙 ′ 𝑃𝑃𝑖𝑖𝑙𝑙𝑜𝑜𝑏𝑏𝑚𝑚𝑙𝑙NIST SP 800-90B RECOMMENDATION FOR THE ENTROPY SOURCES USED FOR RANDOM BIT GENERATION 76 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B The following table provides pre-calculated values for P for different r (length of the longest local run of ones +1) values when the length of the input sequence is 1 000 000. Table 3 values for different r values when L=1 000 000. r 𝑷𝑷𝑷𝑷 𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷 r r r 1 0.0000 36 0.6157 160 0.9045 370 0.9597 𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷 𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷 𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷 𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷𝑷 2 0.0001 37 0.6242 165 0.9074 380 0.9609 3 0.0022 38 0.6324 170 0.9101 390 0.9619 4 0.0100 39 0.6402 175 0.9127 400 0.9629 5 0.0253 40 0.6477 180 0.9152 410 0.9639 6 0.0468 41 0.6549 185 0.9175 420 0.9648 7 0.0728 42 0.6619 190 0.9198 430 0.9656 8 0.1014 43 0.6686 195 0.9219 440 0.9664 9 0.1313 44 0.6750 200 0.9239 450 0.9672 10 0.1614 45 0.6812 205 0.9258 460 0.9680 11 0.1911 46 0.6872 210 0.9276 470 0.9687 12 0.2200 47 0.6930 215 0.9293 480 0.9694 13 0.2479 48 0.6986 220 0.9309 490 0.9700 14 0.2746 49 0.7040 225 0.9325 500 0.9707 15 0.3000 55 0.7092 230 0.9340 550 0.9735 16 0.3242 60 0.7328 235 0.9355 600 0.9758 17 0.3471 65 0.7531 240 0.9369 650 0.9778 18 0.3688 70 0.7705 245 0.9382 700 0.9795 19 0.3893 75 0.7858 250 0.9395 750 0.9809 20 0.4088 80 0.7992 255 0.9407 800 0.9822 21 0.4272 85 0.8111 260 0.9419 850 0.9833 22 0.4447 90 0.8217 265 0.9430 900 0.9843 23 0.4613 95 0.8312 270 0.9441 950 0.9852 24 0.4770 100 0.8398 275 0.9452 1000 0.9860 25 0.4919 105 0.8476 280 0.9462 1500 0.9909 26 0.5060 110 0.8547 285 0.9471 2000 0.9933 27 0.5195 115 0.8612 290 0.9481 2500 0.9947 28 0.5323 120 0.8671 295 0.9490 3000 0.9957 29 0.5445 125 0.8726 300 0.9499 4000 0.9968 30 0.5561 130 0.8776 310 0.9516 5000 0.9975 31 0.5672 135 0.8823 320 0.9531 10000 0.9988 32 0.5778 140 0.8867 330 0.9546 33 0.5879 145 0.8907 340 0.9560 34 0.5976 150 0.8945 350 0.9573 35 0.6068 155 0.8980 360 0.9586