[
  {
    "title": "Executive Summary",
    "subsections": [
      {
        "content": "The World Wide Web (WWW) is a system for exchanging information over the Internet. At the most basic level, the Web can be divided into two principal components: Web servers, which are applications that make information available over the Internet (in essence, publish information), and Web browsers (clients), which are used to access and display the information stored on the Web servers. This document focuses on the security issues of Web servers."
      },
      {
        "content": "Unfortunately, Web servers are often the most targeted and attacked hosts on organizations' networks. As a result, it is essential to secure Web servers and the network infrastructure that supports them. The following are examples of specific security threats to Web servers:"
      },
      {
        "content": "Malicious entities may exploit software bugs in the Web server, underlying operating system, or active content to gain unauthorized access to the Web server. Examples of this unauthorized access include gaining access to files or folders that were not meant to be publicly accessible (e.g., directory traversal attacks) and being able to execute commands and/or install software on the Web server.\nDenial of service (DoS) attacks may be directed to the Web server or its supporting network infrastructure, denying or hindering valid users from making use of its services.\nSensitive information on the Web server may be read or modified without authorization.\nSensitive information on backend databases that are used to support interactive elements of a Web application may be compromised through command injection attacks (e.g., Structured Query Language [SQL] injection, Lightweight Directory Access Protocol (LDAP) injection, cross-site scripting [XSS]).\nSensitive information transmitted unencrypted between the Web server and the browser may be intercepted.\nInformation on the Web server may be changed for malicious purposes. Web site defacement is a commonly reported example of this threat.\nMalicious entities may gain unauthorized access to resources elsewhere in the organization's network via a successful attack on the Web server.\nMalicious entities may attack external entities after compromising a Web server host. These attacks can be launched directly (e.g., from the compromised host against an external server) or indirectly (e.g., placing malicious content on the compromised Web server that attempts to exploit vulnerabilities in the Web browsers of users visiting the site).\nThe server may be used as a distribution point for attack tools, pornography, or illegally copied software."
      },
      {
        "content": "Web servers may also face indirect attacks to gain information from their users. In these attacks, the user is persuaded or automatically directed to visit a malicious Web site that appears to be legitimate. The identifying information that is harvested may be used to access the Web site itself or form the basis for identity theft. Successful attacks can compromise confidential Web site resources or harm an organization's image. These indirect attacks occur in two forms:# CURRENT_PAGE_RAW_OCR_TEXT"
      },
      {
        "content": "Phishing, where attackers use social engineering to trick users into logging into a fake site\nPharming, where Domain Name System (DNS) servers or users' host files are compromised to redirect users to a malicious site in place of the legitimate site."
      },
      {
        "content": "This document is intended to assist organizations in installing, configuring, and maintaining secure public Web servers. More specifically, this document describes, in detail, the following practices to apply:"
      },
      {
        "content": "Securing, installing, and configuring the underlying operating system\nSecuring, installing, and configuring Web server software\nDeploying appropriate network protection mechanisms, such as firewalls, routers, switches, and intrusion detection and intrusion prevention systems\nMaintaining the secure configuration through application of appropriate patches and upgrades, security testing, monitoring of logs, and backups of data and operating system files\nUsing, publicizing, and protecting information and data in a careful and systematic manner."
      },
      {
        "title": "Key Guidelines",
        "content": [
          {
            "text": "The following key guidelines are recommended to Federal departments and agencies for maintaining a secure Web presence. Organizations should carefully plan and address the security aspects of the deployment of a public Web server. Because it is much more difficult to address security once deployment and implementation have occurred, security should be considered from the initial planning stage. Organizations are more likely to make decisions about configuring computers appropriately and consistently when they develop and use a detailed, well-designed deployment plan. Developing such a plan will support Web server administrators in making the inevitable tradeoff decisions between usability, performance, and risk."
          },
          {
            "text": "Organizations often fail to consider the human resource requirements for both deployment and operational phases of the Web server and supporting infrastructure. Organizations should address the following points in a deployment plan:"
          },
          {
            "text": "Types of personnel required (e.g., system and Web server administrators, Webmasters, network administrators, information systems security officers [ISSO])\nSkills and training required by assigned personnel\nIndividual (i.e., level of effort required of specific personnel types) and collective staffing (i.e., overall level of effort) requirements."
          }
        ]
      },
      {
        "title": "Security Management Practices",
        "content": [
          {
            "text": "Organizations should implement appropriate security management practices and controls when maintaining and operating a secure Web server. Appropriate management practices are essential to operating and maintaining a secure Web server. Security practices entail the identification of an organization's information system assets and the development, documentation, and implementation of policies, standards, procedures, and guidelines that help to ensure the confidentiality, integrity, and availability of information system resources. To ensure the security of a Web server and the supporting network infrastructure, the following practices should be implemented:"
          },
          {
            "text": "Organization-wide information system security policy\n\nConfiguration/change control and management# Risk Assessment and Management\n\n\nRisk assessment and management\n\nStandardized software configurations that satisfy the information system security policy\nSecurity awareness and training\nContingency planning, continuity of operations, and disaster recovery planning\nCertification and accreditation."
          },
          {
            "text": "Configuration/change control and management# Risk Assessment and Management"
          },
          {
            "text": "Risk assessment and management"
          },
          {
            "text": "Organizations should ensure that Web server operating systems are deployed, configured, and managed to meet the security requirements of the organization. The first step in securing a Web server is securing the underlying operating system. Most commonly available Web servers operate on a general-purpose operating system. Many security issues can be avoided if the operating systems underlying Web servers are configured appropriately. Default hardware and software configurations are typically set by manufacturers to emphasize features, functions, and ease of use, at the expense of security. Because manufacturers are not aware of each organization's security needs, each Web server administrator must configure new servers to reflect their organization's security requirements and reconfigure them as those requirements change. Using security configuration guides or checklists can assist administrators in securing systems consistently and efficiently. Securing an operating system initially would generally include the following steps:"
          },
          {
            "text": "Patch and upgrade the operating system\nRemove or disable unnecessary services and applications\nConfigure operating system user authentication\nConfigure resource controls\nInstall and configure additional security controls\nPerform security testing of the operating system."
          },
          {
            "text": "Organizations should ensure that the Web server application is deployed, configured, and managed to meet the security requirements of the organization. In many respects, the secure installation and configuration of the Web server application will mirror the operating system process discussed above. The overarching principle is to install the minimal amount of Web server services required and eliminate any known vulnerabilities through patches or upgrades. If the installation program installs any unnecessary applications, services, or scripts, they should be removed immediately after the installation process concludes. Securing the Web server application would generally include the following steps:"
          },
          {
            "text": "Patch and upgrade the Web server application\nRemove or disable unnecessary services, applications, and sample content\nConfigure Web server user authentication and access controls\nConfigure Web server resource controls\nTest the security of the Web server application and Web content."
          },
          {
            "text": "Organizations should take steps to ensure that only appropriate content is published on a Web site. Many agencies lack a Web publishing process or policy that determines what type of information to publish openly, what information to publish with restricted access, and what information should not be published to any publicly accessible repository. This is unfortunate because Web sites are often one of the first places that malicious entities search for valuable information. Some generally accepted examples of what should not be published or at least should be carefully examined and reviewed before publication on a public Web site include\u2014# CURRENT_PAGE_RAW_OCR_TEXT"
          },
          {
            "text": "Classified or proprietary information\nInformation on the composition or preparation of hazardous materials or toxins\nSensitive information relating to homeland security\nMedical records\nAn organization's detailed physical and information security safeguards\nDetails about an organization's network and information system infrastructure (e.g., address ranges, naming conventions, access numbers)\nInformation that specifies or implies physical security vulnerabilities\nDetailed plans, maps, diagrams, aerial photographs, and architectural drawings of organizational buildings, properties, or installations\nAny sensitive information about individuals, such as personally identifiable information (PII), that might be subject to either Federal, state or, in some instances, international privacy laws."
          },
          {
            "text": "Organizations should ensure appropriate steps are taken to protect Web content from unauthorized access or modification. Although information on public Web sites is content that is intended to be public, assuming a credible review process and policy is in place, it is still important to ensure that information cannot be modified without authorization. Users of this information rely upon the integrity of such information even if the information is not confidential. Because of the public accessibility, content on publicly accessible Web servers is inherently more vulnerable than information that is inaccessible from the Internet. This vulnerability means that organizations need to protect public Web content through the appropriate configuration of Web server resource controls."
          }
        ]
      },
      {
        "title": "Examples of resource control practices include\u2014",
        "content": [
          {
            "text": "Install or enable only necessary services.\nInstall Web content on a dedicated hard drive or logical partition.\nLimit uploads to directories that are not readable by the Web server.\nDefine a single directory for all external scripts or programs executed as part of Web content.\nDisable the use of hard or symbolic links.\nDefine a complete Web content access matrix that identifies which folders and files within the Web server document directory are restricted and which are accessible (and by whom).\nDisable directory listings.\nUse user authentication, digital signatures, and other cryptographic mechanisms as appropriate.\nUse host-based intrusion detection systems (IDS), intrusion prevention systems (IPS), and/or file integrity checkers to detect intrusions and verify Web content.\nProtect each backend server (e.g., database server, directory server) from command injection attacks at both the Web server and the backend server."
          },
          {
            "text": "Organizations should use active content judiciously after balancing the benefits gained against the associated risks. Most early Web sites presented static information residing on the server, typically in the form of text-based documents. Soon thereafter, interactive elements were introduced to offer users new ways to interact with a Web site. Unfortunately, these same interactive elements introduced new Web-related vulnerabilities because they involve dynamically executing code on either the Web server or client using a large number of inputs, from Universal Resource Locator (URL) parameters to Hypertext Transfer Protocol (HTTP) POST content and, more recently, Extensible Markup Language.# CURRENT PAGE RAW OCR TEXT"
          }
        ]
      },
      {
        "title": "(XML) content in the form of Web service messages.",
        "content": [
          {
            "text": "Different active content technologies have different associated vulnerabilities, and their risks should be weighed against their benefits. Although most Web sites use some form of active content generators, many also deliver some or all of their content in a non-active form."
          },
          {
            "text": "Organizations must use authentication and cryptographic technologies as appropriate to protect certain types of sensitive data. Public Web servers often support a range of technologies for identifying and authenticating users with differing privileges for accessing information. Some of these technologies are based on cryptographic functions that can provide an encrypted channel between a Web browser client and a Web server that supports encryption. Web servers may be configured to use different cryptographic algorithms, providing varying levels of security and performance."
          },
          {
            "text": "Without proper user authentication in place, organizations cannot selectively restrict access to specific information. All information that resides on a public Web server is then accessible by anyone with access to the server. In addition, without some process to authenticate the server, users of the public Web server will not be able to determine whether the server is the \"authentic\" Web server or a counterfeit version operated by a malicious entity."
          },
          {
            "text": "Even with an encrypted channel and an authentication mechanism, it is possible that attackers may attempt to access the site via a brute force attack. Improper authentication techniques can allow attackers to gather valid usernames or potentially gain access to the Web site. Strong authentication mechanisms can also protect against phishing and pharming attacks. Therefore, an appropriate level of authentication should be implemented based on the sensitivity of the Web server's users and content."
          },
          {
            "text": "Organizations should employ their network infrastructure to help protect their public Web servers. The network infrastructure (e.g., firewalls, routers, IDSs) that supports the Web server plays a critical role in the security of the Web server. In most configurations, the network infrastructure will be the first line of defense between a public Web server and the Internet. Network design alone, however, cannot protect a Web server. The frequency, sophistication, and variety of Web server attacks perpetrated today support the idea that Web server security must be implemented through layered and diverse protection mechanisms (i.e., defense-in-depth)."
          },
          {
            "text": "Organizations should commit to the ongoing process of maintaining the security of public Web servers to ensure continued security. Maintaining a secure Web server requires constant effort, resources, and vigilance from an organization. Securely administering a Web server on a daily basis is an essential aspect of Web server security. Maintaining the security of a Web server will usually involve the following steps:"
          },
          {
            "text": "Configuring, protecting, and analyzing log files\nBacking up critical information frequently\n\nMaintaining a protected authoritative copy of the organization's Web content# CURRENT_PAGE_RAW_OCR_TEXT\n\n\nEstablishing and following procedures for recovering from compromise\n\nTesting and applying patches in a timely manner\nTesting security periodically."
          },
          {
            "text": "Maintaining a protected authoritative copy of the organization's Web content# CURRENT_PAGE_RAW_OCR_TEXT"
          },
          {
            "text": "Establishing and following procedures for recovering from compromise"
          }
        ]
      },
      {
        "title": "1. Introduction",
        "content": [
          {
            "subsection": "1.1 Authority",
            "text": []
          },
          {
            "text": "The National Institute of Standards and Technology (NIST) developed this document in furtherance of its statutory responsibilities under the Federal Information Security Management Act (FISMA) of 2002, Public Law 107-347. NIST is responsible for developing standards and guidelines, including minimum requirements, for providing adequate information security for all agency operations and assets; but such standards and guidelines shall not apply to national security systems. This guideline is consistent with the requirements of the Office of Management and Budget (OMB) Circular A-130, Section 8b(3), \"Securing Agency Information Systems,\" as analyzed in A-130, Appendix IV: Analysis of Key Sections. Supplemental information is provided in A-130, Appendix III."
          },
          {
            "text": "This guideline has been prepared for use by Federal agencies. It may be used by nongovernmental organizations on a voluntary basis and is not subject to copyright, although attribution is desired. Nothing in this document should be taken to contradict standards and guidelines made mandatory and binding on Federal agencies by the Secretary of Commerce under statutory authority, nor should these guidelines be interpreted as altering or superseding the existing authorities of the Secretary of Commerce, the Director of the OMB, or any other Federal official."
          },
          {
            "subsection": "1.2 Purpose and Scope",
            "text": []
          },
          {
            "text": "The purpose of the Guidelines on Securing Public Web Servers is to recommend security practices for designing, implementing, and operating publicly accessible Web servers, including related network infrastructure issues. Some Federal organizations might need to go beyond these recommendations or adapt them in other ways to meet their unique requirements. While intended as recommendations for Federal departments and agencies, it may be used in the private sector on a voluntary basis."
          },
          {
            "text": "This document may be used by organizations interested in enhancing security on existing and future Web server systems to reduce the number and frequency of Web-related security incidents. This document presents generic principles that apply to all systems."
          },
          {
            "text": "This guideline does not cover the following aspects relating to securing a Web server:"
          },
          {
            "text": "Securing other types of network servers\nFirewalls and routers used to protect Web servers beyond a basic discussion in Section 8\nSecurity considerations related to Web client (browser) software\nSpecial considerations for high-traffic Web sites with multiple hosts\nSecuring back-end servers that may support the Web server (e.g., database servers, file servers)\nServices other than Hypertext Transfer Protocol (HTTP) and Hypertext Transfer Protocol Secure (HTTPS)\nSOAP-style Web Services\nProtection of intellectual property."
          },
          {
            "subsection": "1.3 Audience and Assumptions",
            "text": []
          },
          {
            "text": "This document, while technical in nature, provides the background information to# CURRENT_PAGE_RAW_OCR_TEXT"
          }
        ]
      },
      {
        "title": "Help Readers Understand",
        "content": [
          {
            "text": "The intended audience for this document includes the following:"
          },
          {
            "text": "System engineers and architects, when designing and implementing Web servers\nWeb and system administrators, when administering, patching, securing, or upgrading Web servers\nWebmasters, when creating and managing Web content\nSecurity consultants, when performing security audits to determine information system (IS) security postures\nProgram managers and information technology (IT) security officers, to ensure that adequate security measures have been considered for all phases of the system's life cycle."
          },
          {
            "text": "This document assumes that readers have some minimal operating system, networking, and Web server expertise. Because of the constantly changing nature of Web server threats and vulnerabilities, readers are expected to take advantage of other resources (including those listed in this document) for more current and detailed information."
          },
          {
            "text": "The practices recommended in this document are designed to help mitigate the risks associated with Web servers. They build on and assume the implementation of practices described in other NIST guidelines listed in Appendix E."
          }
        ]
      },
      {
        "title": "1.4 Document Structure",
        "content": [
          {
            "text": "The remainder of this document is organized into the following eight major sections:"
          },
          {
            "text": "Section 2 discusses Web server security problems and presents an overview.\nSection 3 discusses the planning and management of Web servers.\nSection 4 presents an overview of securing the underlying operating system for a Web server.\nSection 5 discusses securely installing and configuring a Web server.\nSection 6 examines the security of Web content.\nSection 7 examines popular Web authentication and encryption technologies.\nSection 8 discusses protecting a Web server through the supporting network infrastructure.\nSection 9 discusses the basics of securely administering a Web server on a daily basis."
          },
          {
            "text": "The document also contains several appendices with supporting material."
          },
          {
            "text": "Appendix A provides a variety of online Web security resources.\nAppendix B defines terms used in this document.\nAppendix C provides a list of commonly used Web server security tools and applications.\nAppendix D lists references used in this document.\nAppendix E provides a Web server security checklist.\nAppendix F contains an acronym list.\nAppendix G contains an index for the publication."
          }
        ]
      },
      {
        "title": "2. Background",
        "content": [
          {
            "text": "The World Wide Web is one of the most important ways for an organization to publish information, interact with Internet users, and establish an e-commerce/e-government presence. However, if an organization is not rigorous in configuring and operating its public Web site, it may be vulnerable to a variety of security threats. Although the threats in cyberspace remain largely the same as in the physical world (e.g., fraud, theft, vandalism, and terrorism), they are far more dangerous as a result of three important developments: increased efficiency, action at a distance, and rapid technique propagation [Schn00]."
          },
          {
            "text": "Increased Efficiency\u2014Automation makes attacks, even those with minimal opportunity for success,# Current Page Raw OCR Text"
          },
          {
            "text": "Efficient and extremely profitable. For example, in the physical world, an attack that would succeed once in 10,000 attempts would be ineffectual because of the time and effort required, on average, for a single success. The time invested in achieving a single success would be outweighed by the time invested in the 9,999 failures. On the Internet, automation enables the same attack to be a stunning success. Computing power and bandwidth are becoming less expensive daily, while the number of hosts that can be targeted is growing rapidly. This synergy means that almost any attack, no matter how low its success rate, will likely find many systems to exploit."
          }
        ]
      },
      {
        "title": "Action at a Distance",
        "content": [
          {
            "text": "The Internet allows action at a distance. The Internet has no borders, and every point on the Internet is potentially reachable from every other point. This means that an attacker in one country can target a remote Web site in another country as easily as one close to home."
          }
        ]
      },
      {
        "title": "Rapid Technique Propagation",
        "content": [
          {
            "text": "The Internet allows for easier and more rapid technique propagation. Before the Internet, techniques for attack were developed that would take years, if ever, to propagate, allowing time to develop effective countermeasures. Today, a new technique can be propagated within hours or days. It is now more difficult to develop effective countermeasures in a timely manner."
          },
          {
            "text": "Compromised Web sites have served as an entry point for intrusions into many organizations' internal networks. Organizations can face monetary losses, damage to reputation, or legal action if an intruder successfully violates the confidentiality of their data. Denial of service (DoS) attacks can make it difficult, if not impossible, for users to access an organization's Web site. These attacks may cost the organization significant time and money. DoS attacks are easy for attackers to attempt because of the number of possible attack vectors, the variety of automated tools available, and the low skill level needed to use the tools. DoS attacks, as well as threats of initiating DoS attacks, are also increasingly being used to blackmail organizations. In addition, an organization can find itself in an embarrassing situation resulting from malicious intruders changing the content of the organization's Web pages."
          },
          {
            "text": "Kossakowski and Allen identified three main security issues related to the operation of a publicly accessible Web site [Koss00]:"
          },
          {
            "text": "Misconfiguration or other improper operation of the Web server, which may result, for example, in the disclosure or alteration of proprietary or sensitive information. This information can include items such as the following:\nAssets of the organization\nConfiguration of the server or network that could be exploited for subsequent attacks\nInformation regarding the users or administrator(s) of the Web server, including their passwords.\nVulnerabilities within the Web server that might allow, for example, attackers to compromise the security of the server and other hosts on the organization's network by taking actions such as the# Security Considerations for Web Servers"
          }
        ]
      },
      {
        "title": "Risks Associated with Web Servers",
        "content": [
          {
            "text": "Defacing the Web site or otherwise affect information integrity\nExecuting unauthorized commands or programs on the host operating system, including ones that the intruder has installed\nGaining unauthorized access to resources elsewhere in the organization's computer network\nLaunching attacks on external sites from the Web server, thus concealing the intruders' identities, and perhaps making the organization liable for damages\nUsing the server as a distribution point for illegally copied software, attack tools, or pornography, perhaps making the organization liable for damages\nUsing the server to deliver attacks against vulnerable Web clients to compromise them.\nInadequate or unavailable defense mechanisms for the Web server to prevent certain classes of attacks, such as DoS attacks, which disrupt the availability of the Web server and prevent authorized users from accessing the Web site when required."
          },
          {
            "text": "In recent years, as the security of networks and server installations have improved, poorly written software applications and scripts that allow attackers to compromise the security of the Web server or collect data from backend databases have become the targets of attacks. Many dynamic Web applications do not perform sufficient validation of user input, allowing attackers to submit commands that are run on the server. Common examples of this form of attack are structured query language (SQL) injection, where an attacker submits input that will be passed to a database and processed, and cross-site scripting, where an attacker manipulates the application to store scripting language commands that are activated when another user accesses the Web page."
          }
        ]
      },
      {
        "title": "Steps to Ensure Security of Public Web Servers",
        "content": [
          {
            "text": "A number of steps are required to ensure the security of any public Web server. As a prerequisite for taking any step, however, it is essential that the organization have a security policy in place. Taking the following steps within the context of the organization's security policy should prove effective:"
          },
          {
            "text": "Step 1: Installing, configuring, and securing the underlying operating system (OS)\nStep 2: Installing, configuring, and securing Web server software\nStep 3: Employing appropriate network protection mechanisms (e.g., firewall, packet filtering router, and proxy)\nStep 4: Ensuring that any applications developed specifically for the Web server are coded following secure programming practices\nStep 5: Maintaining the secure configuration through application of appropriate patches and upgrades, security testing, monitoring of logs, and backups of data and OS\nStep 6: Using, publicizing, and protecting information and data in a careful and systemic manner\nStep 7: Employing secure administration and maintenance processes (including server/application updating and log reviews)\nStep 8: Conducting initial and periodic vulnerability scans of each public Web server and supporting network infrastructure (e.g., firewalls, routers)."
          },
          {
            "text": "The practices recommended in this document are designed to help mitigate the risks associated with public Web servers. They build on and assume the implementation of practices.# Security Principles for Web Server Security"
          },
          {
            "text": "described in the NIST publications on system and network security listed in Appendix A. When addressing Web server security issues, it is an excellent idea to keep in mind the following general information security principles [Curt01 and Salt75]:"
          }
        ]
      },
      {
        "title": "Simplicity",
        "content": [
          {
            "text": "Security mechanisms (and information systems in general) should be as simple as possible. Complexity is at the root of many security issues."
          }
        ]
      },
      {
        "title": "Fail-Safe",
        "content": [
          {
            "text": "If a failure occurs, the system should fail in a secure manner, i.e., security controls and settings remain in effect and are enforced. It is usually better to lose functionality rather than security."
          }
        ]
      },
      {
        "title": "Complete Mediation",
        "content": [
          {
            "text": "Rather than providing direct access to information, mediators that enforce access policy should be employed. Common examples of mediators include file system permissions, proxies, firewalls, and mail gateways."
          }
        ]
      },
      {
        "title": "Open Design",
        "content": [
          {
            "text": "System security should not depend on the secrecy of the implementation or its components. \"Security through obscurity\" is not reliable."
          }
        ]
      },
      {
        "title": "Separation of Privilege",
        "content": [
          {
            "text": "Functions, to the degree possible, should be separate and provide as much granularity as possible. The concept can apply to both systems and operators and users. In the case of systems, functions such as read, edit, write, and execute should be separate. In the case of system operators and users, roles should be as separate as possible. For example, if resources allow, the role of system administrator should be separate from that of the security administrator."
          }
        ]
      },
      {
        "title": "Least Privilege",
        "content": [
          {
            "text": "This principle dictates that each task, process, or user is granted the minimum rights required to perform its job. By applying this principle consistently, if a task, process, or user is compromised, the scope of damage is constrained to the limited resources available to the compromised entity."
          }
        ]
      },
      {
        "title": "Psychological Acceptability",
        "content": [
          {
            "text": "Users should understand the necessity of security. This can be provided through training and education. In addition, the security mechanisms in place should present users with sensible options that give them the usability they require on a daily basis. If users find the security mechanisms too cumbersome, they may devise ways to work around or compromise them. The objective is not to weaken security so it is understandable and acceptable, but to train and educate users and to design security mechanisms and policies that are usable and effective."
          }
        ]
      },
      {
        "title": "Least Common Mechanism",
        "content": [
          {
            "text": "When providing a feature for the system, it is best to have a single process or service gain some function without granting that same function to other parts of the system. The ability for the Web server process to access a back-end database, for instance, should not also enable other applications on the system to access the back-end database."
          }
        ]
      }
    ]
  },
  {
    "title": "CURRENT_PAGE_RAW_OCR_TEXT",
    "subsections": [
      {
        "content": "Work Factor\u2014Organizations should understand what it would take to break the system or network's security features. The amount of work necessary for an attacker to break the system or network should exceed the value that the attacker would gain from a successful compromise.\n\n\nCompromise Recording\u2014Records and logs should be maintained so that if a compromise does occur, evidence of the attack is available to the organization. This information can assist in securing the network and host after the compromise and aid in identifying the methods and exploits used by the attacker. This information can be used to better secure the host or network in the future. In addition, these records and logs can assist organizations in identifying and prosecuting attackers."
      },
      {
        "content": "Work Factor\u2014Organizations should understand what it would take to break the system or network's security features. The amount of work necessary for an attacker to break the system or network should exceed the value that the attacker would gain from a successful compromise."
      },
      {
        "content": "Compromise Recording\u2014Records and logs should be maintained so that if a compromise does occur, evidence of the attack is available to the organization. This information can assist in securing the network and host after the compromise and aid in identifying the methods and exploits used by the attacker. This information can be used to better secure the host or network in the future. In addition, these records and logs can assist organizations in identifying and prosecuting attackers."
      },
      {
        "title": "3. Planning and Managing Web Servers",
        "content": [
          {
            "text": "The most critical aspect of deploying a secure Web server is careful planning prior to installation, configuration, and deployment. Careful planning will ensure that the Web server is as secure as possible and in compliance with all relevant organizational policies. Many Web server security and performance problems can be traced to a lack of planning or management controls. The importance of management controls cannot be overstated. In many organizations, the IT support structure is highly fragmented. This fragmentation leads to inconsistencies, and these inconsistencies can lead to security vulnerabilities and other issues."
          },
          {
            "subsection": "3.1 Installation and Deployment Planning",
            "text": []
          },
          {
            "text": "Security should be considered from the initial planning stage at the beginning of the systems development life cycle to maximize security and minimize costs. It is much more difficult and expensive to address security after deployment and implementation. Organizations are more likely to make decisions about configuring hosts appropriately and consistently if they begin by developing and using a detailed, well-designed deployment plan. Developing such a plan enables organizations to make informed tradeoff decisions between usability and performance, and risk. A deployment plan allows organizations to maintain secure configurations and aids in identifying security vulnerabilities, which often manifest themselves as deviations from the plan."
          },
          {
            "text": "In the planning stages of a Web server, the following items should be considered [Alle00]:"
          },
          {
            "text": "Identify the purpose(s) of the Web server.\nWhat information categories will be stored on the Web server?\nWhat information categories will be processed on or transmitted through the Web server?\nWhat are the security requirements for this information?\nWill any information be retrieved from or stored on another host (e.g., back-end database, mail server)?\nWhat are the security requirements for any other hosts involved (e.g., back-end database, directory server, mail server, proxy server)?\nWhat other service(s) will be provided by the Web server (in general, dedicating the host to being only a Web server is the most secure option)?\nWhat are the security requirements for these additional services?\nWhat are the requirements for continuity of services provided by Web servers, such as those\n```# Current Page Raw OCR Text"
          }
        ]
      },
      {
        "title": "Specified in continuity of operations plans and disaster recovery plans?",
        "content": [
          {
            "text": "Where on the network will the Web server be located (see Section 8)?\nIdentify the network services that will be provided on the Web server, such as those supplied through the following protocols:\nHTTP\nHTTPS\nInternet Caching Protocol (ICP)\nHyper Text Caching Protocol (HTCP)\nWeb Cache Coordination Protocol (WCCP)\nSOCKS\nDatabase services (e.g., Open Database Connectivity [ODBC]).\nIdentify any network service software, both client and server, to be installed on the Web server and any other support servers.\nIdentify the users or categories of users of the Web server and any support hosts.\nDetermine the privileges that each category of user will have on the Web server and support hosts.\nDetermine how the Web server will be managed (e.g., locally, remotely from the internal network, remotely from external networks).\nDecide if and how users will be authenticated and how authentication data will be protected.\nDetermine how appropriate access to information resources will be enforced.\nDetermine which Web server applications meet the organization's requirements. Consider servers that may offer greater security, albeit with less functionality in some instances. Some issues to consider include\u2014\nCost\nCompatibility with existing infrastructure\nKnowledge of existing employees\nExisting manufacturer relationship\nPast vulnerability history\nFunctionality.\nWork closely with manufacturer(s) in the planning stage."
          },
          {
            "text": "The choice of Web server application may determine the choice of OS. However, to the degree possible, Web server administrators should choose an OS that provides the following [Alle00]:"
          },
          {
            "text": "Ability to restrict administrative or root level activities to authorized users only\nAbility to control access to data on the server\nAbility to disable unnecessary network services that may be built into the OS or server software\nAbility to control access to various forms of executable programs, such as Common Gateway Interface (CGI) scripts and server plug-ins in the case of Web servers\nAbility to log appropriate server activities to detect intrusions and attempted intrusions\nProvision of a host-based firewall capability."
          },
          {
            "text": "In addition, organizations should consider the availability of trained, experienced staff to administer the server and server products. Many organizations have learned the difficult lesson that a capable and experienced administrator for one type of operating environment is not automatically as effective for another."
          },
          {
            "text": "Although many Web servers do not host sensitive information, most Web servers should be considered sensitive because of the damage to the organization's reputation that could occur if the servers' integrity is compromised. In such cases, it is critical that the Web servers are located.# CURRENT_PAGE_RAW_OCR_TEXT"
          }
        ]
      },
      {
        "title": "Secure Physical Environments",
        "content": [
          {
            "text": "In areas that provide secure physical environments, when planning the location of a Web server, the following issues should be considered:"
          },
          {
            "text": "Are the appropriate physical security protection mechanisms in place? Examples include\u2014\nLocks\nCard reader access\nSecurity guards\nPhysical IDSs (e.g., motion sensors, cameras).\nAre there appropriate environmental controls so that the necessary humidity and temperature are maintained?\nIs there a backup power source? For how long will it provide power?\nIf high availability is required, are there redundant Internet connections from at least two different Internet service providers (ISP)?\nIf the location is subject to known natural disasters, is it hardened against those disasters and/or is there a contingency site outside the potential disaster area?"
          }
        ]
      },
      {
        "title": "3.2 Security Management Staff",
        "content": [
          {
            "text": "Because Web server security is tightly intertwined with the organization's general information system security posture, a number of IT and system security staff may be interested in Web server planning, implementation, and administration. This section provides a list of generic roles and identifies their responsibilities as they relate to Web server security. These roles are for the purpose of discussion and may vary by organization."
          },
          {
            "subsection": "3.2.1 Senior IT Management/Chief Information Officer",
            "text": []
          },
          {
            "text": "The Senior IT Management/Chief Information Officer (CIO) ensures that the organization's security posture is adequate. The Senior IT Management provides direction and advisory services for the protection of information systems for the entire organization. The Senior IT Management/CIO is responsible for the following activities associated with Web servers:"
          },
          {
            "text": "Coordinating the development and maintenance of the organization's information security policies, standards, and procedures\nCoordinating the development and maintenance of the organization's change control and management procedures\nEnsuring the establishment of, and compliance with, consistent IT security policies for departments throughout the organization\nCoordinating with upper management, public affairs, and other relevant personnel to produce a formal policy and process for publishing information to Web sites and ensuring this policy is enforced."
          },
          {
            "subsection": "3.2.2 Information Systems Security Program Managers",
            "text": []
          },
          {
            "text": "The Information Systems Security Program Managers (ISSPM) oversee the implementation of and compliance with the standards, rules, and regulations specified in the organization's security policy. The ISSPMs are responsible for the following activities associated with Web servers:"
          },
          {
            "text": "Ensuring that security procedures are developed and implemented\nEnsuring that security policies, standards, and requirements are followed\nEnsuring that all critical systems are identified and that contingency planning, disaster recovery plans, and continuity of operations plans exist for these critical systems\nEnsuring that critical systems are identified and scheduled for periodic# Security Testing According to the Security Policy Requirements"
          }
        ]
      },
      {
        "title": "3.2.3 Information Systems Security Officers",
        "content": [
          {
            "text": "Information Systems Security Officers (ISSO) are responsible for overseeing all aspects of information security within a specific organizational entity. They ensure that the organization's information security practices comply with organizational and departmental policies, standards, and procedures. ISSOs are responsible for the following activities associated with Web servers:"
          },
          {
            "text": "Developing internal security standards and procedures for the Web server(s) and supporting network infrastructure\nCooperating in the development and implementation of security tools, mechanisms, and mitigation techniques\nMaintaining standard configuration profiles of the Web servers and supporting network infrastructure controlled by the organization, including, but not limited to, OSs, firewalls, routers, and Web server applications\nMaintaining operational integrity of systems by conducting security tests and ensuring that designated IT professionals are conducting scheduled testing on critical systems."
          }
        ]
      },
      {
        "title": "3.2.4 Web Server and Network Administrators",
        "content": [
          {
            "text": "Web server administrators are system architects responsible for the overall design, implementation, and maintenance of a Web server. Network administrators are responsible for the overall design, implementation, and maintenance of a network. On a daily basis, Web server and network administrators contend with the security requirements of the specific system(s) for which they are responsible. Security issues and solutions can originate from either outside (e.g., security patches and fixes from the manufacturer or computer security incident response teams) or within the organization (e.g., the security office). The administrators are responsible for the following activities associated with Web servers:"
          },
          {
            "text": "Installing and configuring systems in compliance with the organizational security policies and standard system and network configurations\nMaintaining systems in a secure manner, including frequent backups and timely application of patches\nMonitoring system integrity, protection levels, and security-related events\nFollowing up on detected security anomalies associated with their information system resources\nConducting security tests as required."
          }
        ]
      }
    ]
  },
  {
    "title": "CURRENT_PAGE_RAW_OCR_TEXT",
    "subsections": [
      {
        "content": "Performs input validation so that the application's security mechanisms cannot be bypassed when a malicious user tampers with data he or she sends to the application, including HTTP requests, headers, query strings, cookies, form fields, and hidden fields.\nProcesses errors in a secure manner so as not to lead to exposure of sensitive implementation information.\nProtects sensitive information processed and/or stored by the application. Inadequate protection can allow data tampering and access to confidential information such as usernames, passwords, and credit card numbers.\nMaintains its own application-specific logs. In many instances, Web server logging is not sufficient to track what a user does at the application level, requiring the application to maintain its own logs. Insufficient logging details can lead to a lack of knowledge about possible intrusions and an inability to verify a user's actions (both legitimate and malicious).\nIs \"hardened\" against application-level DoS attacks. Although DoS attacks are most frequently targeted at the network and transport layers, the application itself can be a target. If a malicious user can monopolize a required application or system resource, legitimate users can be prevented from using the system."
      },
      {
        "title": "3.3 Management Practices",
        "content": [
          {
            "text": "Appropriate management practices are critical to operating and maintaining a secure Web server. Security practices entail the identification of an organization's information system assets and the development, documentation, and implementation of policies, standards, procedures, and guidelines that ensure confidentiality, integrity, and availability of information system resources."
          },
          {
            "text": "To ensure the security of a Web server and the supporting network infrastructure, organizations should implement the following practices:"
          },
          {
            "text": "Organizational Information System Security Policy\u2014A security policy should specify the basic information system security tenets and rules, and their intended internal purpose. The policy should also outline who in the organization is responsible for particular areas of information security (e.g., implementation, enforcement, audit, review). The policy must be enforced consistently throughout the organization to be effective. Generally, the CIO and senior management are responsible for drafting the organization's security policy.\nConfiguration/Change Control and Management\u2014The process of controlling modification to a system's design, hardware, firmware, and software provides sufficient assurance that the system is protected against the introduction of an improper modification before, during, and after system implementation. Configuration control leads to consistency with the organization's information system security policy. Configuration control is traditionally overseen by a configuration control board that is the final authority on all proposed changes to an information system. If resources allow, consider the use of development, quality assurance, and/or test environments so that changes can be.\n```# CURRENT_PAGE_RAW_OCR_TEXT"
          },
          {
            "text": "vetted and tested before deployment in production."
          }
        ]
      },
      {
        "title": "Risk Assessment and Management",
        "content": [
          {
            "text": "Risk assessment is the process of analyzing and interpreting risk. It involves determining an assessment's scope and methodology, collecting and analyzing risk-related data, and interpreting the risk analysis results. Collecting and analyzing risk data requires identifying assets, threats, vulnerabilities, safeguards, consequences, and the probability of a successful attack. Risk management is the process of selecting and implementing controls to reduce risk to a level acceptable to the organization."
          }
        ]
      },
      {
        "title": "Standardized Configurations",
        "content": [
          {
            "text": "Organizations should develop standardized secure configurations for widely used OSs and applications. This will provide recommendations to Web server and network administrators on how to configure their systems securely and ensure consistency and compliance with the organizational security policy. Because it only takes one insecurely configured host to compromise a network, organizations with a significant number of hosts are especially encouraged to apply this recommendation."
          }
        ]
      },
      {
        "title": "Secure Programming Practices",
        "content": [
          {
            "text": "Organizations should adopt secure application development guidelines to ensure that they develop their Web applications in a sufficiently secure manner."
          }
        ]
      },
      {
        "title": "Security Awareness and Training",
        "content": [
          {
            "text": "A security training program is critical to the overall security posture of an organization. Making users and administrators aware of their security responsibilities and teaching the correct practices helps them change their behavior to conform to security best practices. Training also supports individual accountability, which is an important method for improving information system security. If the user community includes members of the general public, providing security awareness specifically targeting them might also be appropriate."
          }
        ]
      },
      {
        "title": "Contingency, Continuity of Operations, and Disaster Recovery Planning",
        "content": [
          {
            "text": "Contingency plans, continuity of operations plans, and disaster recovery plans are established in advance to allow an organization or facility to maintain operations in the event of a disruption."
          }
        ]
      },
      {
        "title": "Certification and Accreditation",
        "content": [
          {
            "text": "Certification in the context of information systems security means that a system has been analyzed to determine how well it meets all of the security requirements of the organization. Accreditation occurs when the organization's management accepts that the system meets the organization's security requirements."
          }
        ]
      },
      {
        "title": "3.4 System Security Plan",
        "content": [
          {
            "text": "The objective of system security planning is to improve protection of information system resources. Plans that adequately protect information assets require managers and information owners\u2014directly affected by and interested in the information and/or processing capabilities\u2014to be convinced that their information assets are adequately protected from loss, misuse, unauthorized access or modification, unavailability, and undetected activities. The purpose of the system security plan is to provide an overview of the security and privacy requirements of the system and describe the controls in place or planned for meeting those requirements.# System Security Plan"
          },
          {
            "text": "The system security plan also delineates responsibilities and expected behavior of all individuals who access the system. The system security plan should be viewed as documentation of the structured process of planning adequate, cost-effective security protection for a system. It should reflect input from various managers with responsibilities concerning the system, including information owners, the system owner, and the ISSPM."
          },
          {
            "text": "For Federal agencies, all information systems must be covered by a system security plan. Other organizations should strongly consider the completion of a system security plan for each of their systems as well. The information system owner is generally the party responsible for ensuring that the security plan is developed and maintained and that the system is deployed and operated according to the agreed-upon security requirements."
          }
        ]
      },
      {
        "title": "Effective System Security Plan",
        "content": [
          {
            "text": "In general, an effective system security plan should include the following:"
          },
          {
            "subsection": "System Identification",
            "text": []
          },
          {
            "text": "The first sections of the system security plan provide basic identifying information about the system. They contain general information such as the key points of contact for the system, the purpose of the system, the sensitivity level of the system, and the environment in which the system is deployed."
          },
          {
            "subsection": "Controls",
            "text": []
          },
          {
            "text": "This section of the plan describes the control measures (in place or planned) that are intended to meet the protection requirements of the information system. Controls fall into three general categories:"
          },
          {
            "text": "Management controls, which focus on the management of the computer security system and the management of risk for a system.\nOperational controls, which are primarily implemented and executed by people (rather than systems). They often require technical or specialized expertise, and often rely upon management activities as well as technical controls.\nTechnical controls, which are security mechanisms that the computer system employs. The controls can provide automated protection from unauthorized access or misuse, facilitate detection of security violations, and support security requirements for applications and data. The implementation of technical controls, however, always requires significant operational considerations and should be consistent with the management of security within the organization [NIST06a]."
          }
        ]
      },
      {
        "title": "Human Resources Requirements",
        "content": [
          {
            "text": "The greatest challenge and expense in developing and securely maintaining a public Web server is providing the necessary human resources to adequately perform the required functions. Many organizations fail to fully recognize the amount of expense and skills required to field a secure public Web server. This failure often results in overworked employees and insecure systems. From the initial planning stages, organizations need to determine the necessary human resource requirements. Appropriate and sufficient human resources are the single most important aspect of effective Web server security. Organizations should also consider the fact that, in general, technical solutions are not a# Substitute for Skilled and Experienced Personnel"
          },
          {
            "text": "When considering the human resource implications of developing and deploying a Web server, organizations should consider the following:"
          }
        ]
      },
      {
        "title": "Required Personnel",
        "content": [
          {
            "text": "What types of personnel are required? This would include such positions as system and Web server administrators, Webmasters, network administrators, and ISSOs."
          }
        ]
      },
      {
        "title": "Required Skills",
        "content": [
          {
            "text": "What are the required skills to adequately plan, develop, and maintain the Web server in a secure manner? Examples include OS administration, network administration, active content expertise, and programming."
          }
        ]
      }
    ]
  },
  {
    "title": "3.6 Alternative Web Server Platforms",
    "subsections": [
      {
        "content": "Although many organizations manage Web servers that operate over general-purpose OSs, there are instances in which an organization may wish to use one of the alternatives discussed below. Although these technologies are relatively new to the area of Web servers, they are based on sound technologies and have started to see broader use in the Web server environment."
      },
      {
        "title": "3.6.1 Trusted Operating Systems",
        "content": [
          {
            "text": "Trusted operating systems (TOS) are security-modified or -enhanced OSs that include additional security mechanisms not found in most general-purpose OSs. They were originally created to meet the need of the Federal government for high security mandatory access control (MAC) systems. TOSs provide a very secure system-wide control policy, a finely defined set of access privileges, and extensive logging and auditing capabilities. Many TOSs are independently verified to ensure that they meet the requirements set forth in their design documentation."
          },
          {
            "text": "TOSs are generally used in applications for which security is paramount. TOSs can securely control all aspects of a computing environment, including networking resources, users, processes, and memory. Specifically, TOSs can limit access to system resources in a manner that is not likely to be interfered with or compromised.# Using a TOS"
          },
          {
            "text": "Using a TOS will generally produce a very secure Web server; however, some difficulties exist in using TOSs. A major drawback is that configuring and administering a TOS requires knowledge of each protected subsystem and its access needs. It may also require significant planning and administrative overhead to design and support a complex Web site on a TOS. However, even with these limitations, organizations that have very high security requirements should consider using a TOS on their Web servers."
          },
          {
            "text": "Some manufacturers have begun bundling their OS offerings with most or all of the functionality of traditional TOSs for use in server or workstation environments. Organizations may benefit from such systems because much of the overhead in designing and configuring the Web server to run in a TOS environment has been performed by the manufacturer. Web servers benefit from the TOS' MAC in that the system can explicitly deny the Web server process access to sensitive portions of the system even if an attacker has managed to take control of the process."
          }
        ]
      },
      {
        "title": "Issues to Consider",
        "content": [
          {
            "text": "The following are some issues to keep in mind when considering a Web platform:"
          },
          {
            "text": "What is the underlying OS and how has it fared in security testing?\nDoes the organization have the necessary expertise in administering a TOS?\nAre the additional costs of purchasing and supporting a TOS outweighed by the benefits?\nIs the TOS compatible with the organization's existing Web applications and scripts?\nIs the TOS compatible with the organization's other applications and servers with which it will be interoperating?"
          }
        ]
      },
      {
        "title": "3.6.2 Web Server Appliances",
        "content": [
          {
            "text": "A Web server appliance is a software/hardware combination that is designed to be a \"plug-and-play\" Web server. These appliances employ the use of a simplified OS that is optimized to support a Web server. The simplified OS improves security by minimizing unnecessary features, services, and options. The Web server application on these systems is often pre-hardened and pre-configured for security."
          },
          {
            "text": "These systems offer other benefits in addition to security. Performance is often enhanced because the system (i.e., OS, Web server application, and hardware) is designed and built specifically to operate as a Web server. Cost is often reduced because hardware and software not specifically required by a Web server are not included. These systems can be an excellent option for small- to medium-size organizations that cannot afford a full-time Web administrator."
          },
          {
            "text": "The greatest weakness in these systems is that they may not be suitable for large, complex, and multi-layered Web sites. They may be limited in what types of active content they support (e.g., J2EE, .NET, PHP Hypertext Preprocessor [PHP]), potentially reducing the options available to an organization. An appliance may host the back-end database as well as the front-end Web interface, potentially preventing organizations from having separate servers for each. Finally, it may be difficult to configure appliances from different manufacturers to work together. Nevertheless, because they offer a secure environment and an easy-to-configure interface, small- to medium-size organizations may find appliances an attractive option requiring less administrative effort. Web server appliances are available.# Web Server Appliances and Security"
          },
          {
            "text": "From most major hardware manufacturers and from various specialized manufacturers that concentrate solely on Web server appliances."
          },
          {
            "text": "In addition to Web server appliances, there are also a growing number of security appliances available for Web servers. These systems augment the security mechanisms on the Web server itself. In some cases, these systems can prevent attacks from reaching a Web server, which is especially helpful if the server has known vulnerabilities (e.g., new patches have not yet been applied). The most common types of security appliances are\u2014"
          },
          {
            "text": "Secure Sockets Layer (SSL) accelerators, which off-load the computationally expensive processing required for initiating SSL/Transport Layer Security (TLS) connections\nSecurity gateways, which monitor HTTP traffic to and from the Web server for potential attacks and take action as necessary\nContent filters, which can monitor traffic to and from the Web server for potentially sensitive or inappropriate data and take action as necessary\nAuthentication gateways, which authenticate users via a variety of authentication mechanisms and control access to Universal Resource Locators (URL) hosted on the Web server itself."
          },
          {
            "text": "In many instances, most or all of the above-mentioned functionality is combined in a single device, which is frequently referred to as a reverse proxy."
          },
          {
            "text": "In organizations requiring complicated dynamic Web sites, the security appliance configuration may be complex, which could cause configuration errors that reduce the effectiveness of the appliance. It is important to practice defense-in-depth to ensure that any vulnerabilities present in the security appliance or its configuration do not adversely affect the organization as a whole."
          },
          {
            "text": "An additional challenge presented by appliance devices is that they often employ commonly used open-source software. This is normally not a problem, but it can become one when a vulnerability is found in the underlying software because it is frequently not possible to use the patch released by the open-source software group. Common reasons for this inability to use the patch include possible violations of the licensing or support agreements with the appliance manufacturer, and technical problems in applying updates to the appliance (e.g., administrators often do not have OS-level access to appliances). Therefore, appliances can be open to attack for a longer period of time than non-appliance systems because of the additional delay involved in appliance manufacturers developing, testing, and releasing patches. Another possible problem with appliances is that they usually do not allow the installation of additional software for administration or for security, such as antivirus software or host-based intrusion detection agents."
          }
        ]
      },
      {
        "title": "Considerations for Purchasing a Web Appliance",
        "content": [
          {
            "text": "The following are some issues to consider when contemplating the purchase of a Web appliance:"
          },
          {
            "text": "What is the underlying OS and how has it fared in security testing?\nHow has the appliance itself fared in security testing? (Note that the configuration options of Web appliances are necessarily limited, so a Web appliance will generally only be as secure as its default installation configuration.)\nHow heterogeneous is the organization's Web server infrastructure? (Different)# Brands of Appliances"
          },
          {
            "text": "May not work well together."
          },
          {
            "text": "Are the expansion options inherent in the appliance acceptable to the organization? (Organizations that are anticipating or experiencing rapid growth in Web traffic may not want to limit themselves to a single appliance or appliance vendor.)\nHow difficult is it to configure the appliance? Is the appliance flexible enough to meet the organization's needs?\nHow quickly does the manufacturer respond to and provide patches for potential vulnerabilities?\nIs the underlying software used on the appliance proprietary, open source, or a combination of both?\nHow long will the manufacturer support the appliance and what is the manufacturer's history of support for legacy appliances?"
          }
        ]
      }
    ]
  },
  {
    "title": "4. Securing the Web Server Operating System",
    "subsections": [
      {
        "content": "Protecting a Web server from compromise involves hardening the underlying OS, the Web server application, and the network to prevent malicious entities from directly attacking the Web server. The first step in securing a Web server, hardening the underlying OS, is discussed at length in this section. (Securing the Web server application and the network are addressed in Sections 5 and 8, respectively.)"
      },
      {
        "content": "All commonly available Web servers operate on a general-purpose OS. Many security issues can be avoided if the OSs underlying the Web servers are configured appropriately. Default hardware and# Software Configurations and Security"
      },
      {
        "content": "Software configurations are typically set by manufacturers to emphasize features, functions, and ease of use, at the expense of security. Because manufacturers are unaware of each organization's security needs, each Web server administrator must configure new servers to reflect their organization's security requirements and reconfigure them as those requirements change. The practices recommended here are designed to help Web server administrators configure and deploy Web servers that satisfy their organizations' security requirements. Web server administrators managing existing Web servers should confirm that their systems address the issues discussed."
      },
      {
        "content": "The techniques for hardening different OSs vary greatly; therefore, this section includes the generic procedures common in securing most OSs. Security configuration guides and checklists for many OSs are publicly available; these documents typically contain recommendations for settings that improve the default level of security, and they may also contain step-by-step instructions for securing systems. In addition, many organizations maintain their own guidelines specific to their requirements. Some automated tools also exist for hardening OSs, and their use is strongly recommended (see Appendix D)."
      },
      {
        "title": "Five Basic Steps for OS Security",
        "content": [
          {
            "text": "Five basic steps are necessary to maintain basic OS security:"
          },
          {
            "text": "Planning the installation and deployment of the host OS and other components for the Web server\nPatching and updating the host OS as required\nHardening and configuring the host OS to address security adequately\nInstalling and configuring additional security controls, if needed\nTesting the host OS to ensure that the previous four steps adequately addressed all security issues."
          },
          {
            "text": "The first step is discussed in Section 3. The other steps are covered in Sections 4.1 and 4.2."
          },
          {
            "subsection": "4.1 Installing and Configuring the Operating System",
            "text": []
          },
          {
            "text": "This section provides an overview of the second, third, and fourth steps in the list above. The combined result of these steps should be a reasonable level of protection for the Web server's OS."
          },
          {
            "text": "Once an OS is installed, applying needed patches or upgrades to correct for known vulnerabilities is essential. Any known vulnerabilities an OS has should be corrected before using it to host a Web server or otherwise exposing it to untrusted users. To adequately detect and correct these vulnerabilities, Web server administrators should do the following:"
          },
          {
            "text": "Create, document, and implement a patching process.\nIdentify vulnerabilities and applicable patches.\nMitigate vulnerabilities temporarily if needed and if feasible (until patches are available, tested, and installed).\nInstall permanent fixes (commonly called patches, hotfixes, service packs, or updates)."
          },
          {
            "text": "Administrators should ensure that Web servers, particularly new ones, are adequately protected during the patching process. For example, a Web server that is not fully patched or not configured securely could be compromised by threats if it is publicly accessible while it is being patched. When preparing new Web servers for deployment, administrators should do either of the following:"
          },
          {
            "text": "Keep the servers disconnected from networks or connect them only to an isolated \"build\" network.# CURRENT_PAGE_RAW_OCR_TEXT"
          },
          {
            "text": "until all patches have been transferred to the servers through out-of-band means (e.g., CDs) and installed, and the other configuration steps listed in Section 4.1 have been performed."
          },
          {
            "text": "Place the servers on a virtual local area network (VLAN) or other network segment that severely restricts what actions the hosts on it can perform and what communications can reach the hosts\u2014only allowing those events that are necessary for patching and configuring the hosts. Do not transfer the hosts to regular network segments until all the configuration steps listed in Section 4.1 have been performed."
          },
          {
            "text": "Administrators should generally not apply patches to Web servers without first testing them on another identically configured system because patches can inadvertently cause unexpected problems with proper system operation. Although administrators can configure Web servers to download patches automatically, the servers should not be configured to install them automatically so that they can first be tested."
          }
        ]
      },
      {
        "title": "4.1.2 Remove or Disable Unnecessary Services and Applications",
        "content": [
          {
            "text": "Ideally, a Web server should be on a dedicated, single-purpose host. When configuring the OS, disable everything except that which is expressly permitted\u2014that is, disable all services and applications, re-enable only those required by the Web server, and then remove the unneeded services and applications. If possible, install the minimal OS configuration and then add or remove services and applications as needed. Choose the \"minimal installation\" option, if available, to minimize the effort required in removing unnecessary services. Furthermore, many uninstall scripts or programs are far from perfect in completely removing all components of a service; therefore, it is always better not to install unnecessary services. Some common types of services and applications that should usually be disabled if not required include the following:"
          },
          {
            "text": "File and printer sharing services (e.g., Windows Network Basic Input/Output System [NetBIOS] file and printer sharing, Network File System [NFS], File Transfer Protocol [FTP])\nWireless networking services\nRemote control and remote access programs, particularly those that do not strongly encrypt their communications (e.g., Telnet)\nDirectory services (e.g., Lightweight Directory Access Protocol [LDAP], Kerberos, Network Information System [NIS])\nEmail services (e.g., Simple Mail Transfer Protocol [SMTP])\nLanguage compilers and libraries\nSystem development tools\nSystem and network management tools and utilities, including Simple Network Management Protocol (SNMP)."
          },
          {
            "text": "Removing unnecessary services and applications is preferable to simply disabling them through configuration settings because attacks that attempt to alter settings and activate a disabled service cannot succeed when the functional components are completely removed. Disabled services could also be enabled inadvertently through human error."
          },
          {
            "text": "Eliminating or disabling unnecessary services enhances the security of a Web server in several ways.# CURRENT_PAGE_RAW_OCR_TEXT"
          }
        ]
      },
      {
        "title": "Security Considerations for Web Servers",
        "content": [
          {
            "text": "Other services cannot be compromised and used to attack the host or impair the services of the Web server. Each service added to a host increases the risk of compromise for that host because each service is another possible avenue of access for an attacker. Less is more secure in this case.\nOther services may have defects or may be incompatible with the Web server itself. By disabling or removing them, they should not affect the Web server and should potentially improve its availability.\nThe host can be configured to better suit the requirements of the particular service. Different services might require different hardware and software configurations, which could lead to unnecessary vulnerabilities or negatively affect performance.\nBy reducing services, the number of logs and log entries is reduced; therefore, detecting unexpected behavior becomes easier (see Section 9)."
          },
          {
            "text": "Organizations should determine the services to be enabled on a Web server. Services in addition to the Web server service that might be installed include database access protocols, file transfer protocols, and remote administration services. These services may be required in certain instances, but they may increase the risks to the server. Whether the risks outweigh the benefits is a decision for each organization to make."
          }
        ]
      },
      {
        "title": "4.1.3 Configure Operating System User Authentication",
        "content": [
          {
            "text": "For Web servers, the authorized users who can configure the OS are limited to a small number of designated Web server administrators and Webmasters. The users who can access the public Web server, however, may range from unrestricted to restricted subsets of the Internet community. To enforce policy restrictions, if required, the Web server administrator should configure the OS to authenticate a prospective user by requiring proof that the user is authorized for such access. Even though a Web server may allow unauthenticated access to most of its services, administrative and other types of specialized access should be limited to specific individuals and groups."
          },
          {
            "text": "Enabling authentication by the host computer involves configuring parts of the OS, firmware, and applications on the server, such as the software that implements a network service. Although not normally the case for public Web servers, in special situations, such as high-value/high-risk sites, organizations may also use authentication hardware, such as tokens or one-time password devices. Use of authentication mechanisms where authentication information is reusable (e.g., passwords) and transmitted in the clear over a network is strongly discouraged because the information can be intercepted and used by an attacker to masquerade as an authorized user."
          },
          {
            "text": "To ensure the appropriate user authentication is in place, take the following steps [Alle00]:"
          },
          {
            "text": "Remove or Disable Unneeded Default Accounts and Groups\u2014The default configuration of the OS often includes guest accounts (with and without passwords), administrator or root level accounts, and accounts associated with local and network services. The names and passwords for those accounts are well known. Remove or disable unnecessary accounts to eliminate their use by unauthorized individuals.# Security Guidelines for User Accounts"
          }
        ]
      },
      {
        "title": "Attackers",
        "content": [
          {
            "text": "Including guest accounts on computers containing sensitive information. If there is no requirement to retain a guest account or group, severely restrict access to it and change the password in accordance with the organizational password policy."
          },
          {
            "text": "For default accounts that need to be retained, change the names (where possible and particularly for administrator or root level accounts) and passwords to be consistent with the organizational password policy. Default account names and passwords are commonly known in the attacker community."
          }
        ]
      },
      {
        "title": "Disable Non-Interactive Accounts",
        "content": [
          {
            "text": "Disable accounts (and the associated passwords) that need to exist but do not require an interactive login. For Unix systems, disable the login shell or provide a login shell with NULL functionality (e.g., /bin/false)."
          }
        ]
      },
      {
        "title": "Create the User Groups",
        "content": [
          {
            "text": "Assign users to the appropriate groups. Then assign rights to the groups, as documented in the deployment plan. This approach is preferable to assigning rights to individual users, which becomes unwieldy with large numbers of users."
          }
        ]
      },
      {
        "title": "Create the User Accounts",
        "content": [
          {
            "text": "The deployment plan identifies who will be authorized to use each computer and its services. Create only the necessary accounts. Permit the use of shared accounts only when no viable alternatives exist."
          }
        ]
      },
      {
        "title": "Check the Organization's Password Policy",
        "content": [
          {
            "text": "Set account passwords appropriately. This policy should address the following:"
          },
          {
            "subsection": "Length",
            "text": []
          },
          {
            "text": "A minimum length for passwords. Specify a minimum length of at least eight characters."
          },
          {
            "subsection": "Complexity",
            "text": []
          },
          {
            "text": "The mix of characters required. Require passwords to contain both uppercase and lowercase letters and at least one nonalphabetic character, and to not be a \"dictionary\" word."
          },
          {
            "subsection": "Aging",
            "text": []
          },
          {
            "text": "How long a password may remain unchanged. Require users to change their passwords periodically. Administrator or root level passwords should be changed every 30 to 120 days. The period for user-level passwords should be determined by the enforced length and complexity of the password combined with the sensitivity of the information protected. When considering the appropriate aging duration, the exposure level of user passwords should also be taken into account. Consideration should also be given to enforcing a minimum aging duration to prevent users from rapidly cycling through password changes to clear out their password history and bypass reuse restrictions."
          },
          {
            "subsection": "Reuse",
            "text": []
          },
          {
            "text": "Whether a password may be reused. Some users try to defeat a password aging requirement by changing the password to one they have used previously. If possible, ensure that users cannot change their passwords by merely appending characters to the beginning or end of their original passwords (e.g., original password was \"mysecret\" and is changed to \"1mysecret\" or \"mysecret1\")."
          },
          {
            "subsection": "Authority",
            "text": []
          },
          {
            "text": "Who is allowed to change or reset passwords and what sort of proof is required before initiating any changes.# Password Security"
          },
          {
            "text": "Password Security\u2014how passwords should be secured, such as not storing passwords unencrypted on the mail server, and requiring administrators to use different passwords for their email administration accounts than their other administration accounts."
          }
        ]
      },
      {
        "title": "Configure Computers to Prevent Password Guessing",
        "content": [
          {
            "text": "It is relatively easy for an unauthorized user to try to gain access to a computer by using automated software tools that attempt all passwords. If the OS provides the capability, configure it to increase the period between login attempts with each unsuccessful attempt. If that is not possible, the alternative is to deny login after a limited number of failed attempts (e.g., three). Typically, the account is \"locked out\" for a period of time (such as 30 minutes) or until a user with appropriate authority reactivates it.\n\n\nThe choice to deny login is another situation that requires the Web server administrator to make a decision that balances security and convenience. Implementing this recommendation can help prevent some kinds of attacks, but it can also allow an attacker to use failed login attempts to prevent user access, resulting in a DoS condition. The risk of DoS from account lockout is much greater if an attacker knows or can surmise a pattern to your naming convention that allows them to guess account names.\n\n\nFailed network login attempts should not prevent an authorized user or administrator from logging in at the console. Note that all failed login attempts, whether via the network or console, should be logged. If remote administration is not to be implemented (see Section 9.5), disable the ability for the administrator or root level accounts to log in from the network."
          },
          {
            "text": "It is relatively easy for an unauthorized user to try to gain access to a computer by using automated software tools that attempt all passwords. If the OS provides the capability, configure it to increase the period between login attempts with each unsuccessful attempt. If that is not possible, the alternative is to deny login after a limited number of failed attempts (e.g., three). Typically, the account is \"locked out\" for a period of time (such as 30 minutes) or until a user with appropriate authority reactivates it."
          },
          {
            "text": "The choice to deny login is another situation that requires the Web server administrator to make a decision that balances security and convenience. Implementing this recommendation can help prevent some kinds of attacks, but it can also allow an attacker to use failed login attempts to prevent user access, resulting in a DoS condition. The risk of DoS from account lockout is much greater if an attacker knows or can surmise a pattern to your naming convention that allows them to guess account names."
          },
          {
            "text": "Failed network login attempts should not prevent an authorized user or administrator from logging in at the console. Note that all failed login attempts, whether via the network or console, should be logged. If remote administration is not to be implemented (see Section 9.5), disable the ability for the administrator or root level accounts to log in from the network."
          }
        ]
      },
      {
        "title": "Install and Configure Other Security Mechanisms to Strengthen Authentication",
        "content": [
          {
            "text": "If the information on the Web server requires it, consider using other authentication mechanisms such as biometrics, smart cards, client/server certificates, or one-time password systems. They can be more expensive and difficult to implement, but they may be justified in some circumstances. When such authentication mechanisms and devices are used, the organization's policy should be changed accordingly, if necessary. Some organizational policies may already require the use of strong authentication mechanisms.\n\n\nAs mentioned earlier, attackers using network sniffers can easily capture passwords passed across a network in clear text. However, passwords are economical and appropriate if properly protected while in transit. Organizations should implement authentication and encryption technologies, such as Secure Sockets Layer (SSL)/Transport Layer Security (TLS), Secure Shell (SSH), or virtual private networking (VPN), to protect passwords during transmission. Requiring user-friendly server authentication to be used with encryption technologies reduces the likelihood of successful man-in-the-middle and spoofing attacks."
          },
          {
            "text": "If the information on the Web server requires it, consider using other authentication mechanisms such as biometrics, smart cards, client/server certificates, or one-time password systems. They can be more expensive and difficult to implement, but they may be justified in some circumstances. When such authentication mechanisms and devices are used, the organization's policy should be changed accordingly, if necessary. Some organizational policies may already require the use of strong authentication mechanisms."
          },
          {
            "text": "As mentioned earlier, attackers using network sniffers can easily capture passwords passed across a network in clear text. However, passwords are economical and appropriate if properly protected while in transit. Organizations should implement authentication and encryption technologies, such as Secure Sockets Layer (SSL)/Transport Layer Security (TLS), Secure Shell (SSH), or virtual private networking (VPN), to protect passwords during transmission. Requiring user-friendly server authentication to be used with encryption technologies reduces the likelihood of successful man-in-the-middle and spoofing attacks."
          }
        ]
      },
      {
        "title": "4.1.4 Configure Resource Controls Appropriately",
        "content": [
          {
            "text": "All commonly used modern server OSs provide the capability to specify access privileges individually for files, directories, devices, and other computational resources. By carefully# Setting Access Controls"
          },
          {
            "text": "By setting access controls and denying personnel unauthorized access, the Web server administrator can reduce intentional and unintentional security breaches. For example, denying read access to files and directories helps to protect the confidentiality of information, and denying unnecessary write (modify) access can help maintain the integrity of information. Limiting the execution privilege of most system-related tools to authorized system administrators can prevent users from making configuration changes that could reduce security. It also can restrict the attacker's ability to use those tools to attack the system or other systems on the network."
          }
        ]
      },
      {
        "title": "4.1.5 Install and Configure Additional Security Controls",
        "content": [
          {
            "text": "Operating systems (OSs) often do not include all of the security controls necessary to secure the OS, services, and applications adequately. In such cases, administrators need to select, install, and configure additional software to provide the missing controls. Commonly needed controls include the following:"
          },
          {
            "text": "Anti-malware software, such as antivirus software, anti-spyware software, and rootkit detectors, to protect the local OS from malware and to detect and eradicate any infections that occur. Examples of when anti-malware software would be helpful include a Web administrator bringing infected media to the Web server and a network service worm contacting the server and infecting it.\n\n\nHost-based intrusion detection and prevention software, to detect attacks performed against the Web server, including DoS attacks. Section 7.2.2 contains additional information on host-based intrusion detection and prevention software.\n\n\nHost-based firewalls, to protect the server from unauthorized access.\n\n\nPatch management software to ensure that vulnerabilities are addressed promptly. Patch management software can be used only to apply patches or also to identify new vulnerabilities in the Web server's OSs, services, and applications."
          },
          {
            "text": "Anti-malware software, such as antivirus software, anti-spyware software, and rootkit detectors, to protect the local OS from malware and to detect and eradicate any infections that occur. Examples of when anti-malware software would be helpful include a Web administrator bringing infected media to the Web server and a network service worm contacting the server and infecting it."
          },
          {
            "text": "Host-based intrusion detection and prevention software, to detect attacks performed against the Web server, including DoS attacks. Section 7.2.2 contains additional information on host-based intrusion detection and prevention software."
          },
          {
            "text": "Host-based firewalls, to protect the server from unauthorized access."
          },
          {
            "text": "Patch management software to ensure that vulnerabilities are addressed promptly. Patch management software can be used only to apply patches or also to identify new vulnerabilities in the Web server's OSs, services, and applications."
          },
          {
            "text": "Some Web server administrators also install one or more forms of host-based intrusion detection or intrusion prevention software on their servers. For example, file integrity checking software can identify changes to critical system files."
          },
          {
            "text": "When planning security controls, Web server administrators should consider the resources that the security controls will consume. A server's performance could degrade if it does not have enough memory and processing capacity for the controls."
          }
        ]
      }
    ]
  },
  {
    "title": "Vulnerability Scanning and Penetration Testing",
    "subsections": [
      {
        "content": "Vulnerability scanning should be conducted periodically, at least weekly to monthly, and penetration testing should be conducted at least annually. Because both of these testing techniques are also applicable to testing the Web server application, they are discussed in detail in Section 9.4."
      },
      {
        "content": "Testing generally should not be performed on the production Web server itself. As mentioned in Section 4.1.1, testing for patches and changes to the system should be performed on a separate system; this same testing environment should be used to perform security testing of the Web server."
      },
      {
        "title": "4.3 Checklist for Securing the Web Server Operating System",
        "content": [
          {
            "subsection": "5. Securing the Web Server",
            "text": []
          },
          {
            "text": "Once the OS has been installed and secured, installing the chosen Web server software can begin. Before starting this process, read the Web server manufacturer's documentation carefully and understand the various options available during the installation process. Also, be sure to visit the manufacturer's Web site or a vulnerability database Web site, such as the National Vulnerability Database (NVD), to determine whether there are known vulnerabilities and related patches available that should be installed or configured as part of the setup process. Only after these preliminary steps are accomplished should the installation be started. Note that this section discusses only generic installation and configuration procedures; specific directions for particular Web servers are available from Web server manufacturers and from security checklist repositories."
          },
          {
            "text": "A partially configured and/or patched server should not be exposed to external networks (e.g., the Internet) or external users. In addition, internal network access should be as limited as possible until all software is installed, patched, and configured securely. Insecure Web servers can be compromised in a matter of minutes after being placed on the Internet. While it is ideal to fully harden the platform before placing it on the network, it is not always feasible. For example, some application development tool combinations cannot be installed, configured, and tested on top of a pre-hardened OS and Web server configuration. In such situations, stepwise or incremental hardening is a viable option to consider, with full validation of complete hardening occurring at production deployment."
          },
          {
            "text": "In many respects, the secure installation and configuration of the Web server application mirrors the OS process discussed in Section 4. The overarching principle, as before, is to install only the services required for the Web server and to eliminate any known vulnerabilities through patches or upgrades. Any unnecessary applications, services, or scripts that are installed should be removed immediately once the installation process is complete. During the installation of the Web server, the following steps should be performed:"
          },
          {
            "text": "Install the Web server software either on a dedicated host or on a dedicated guest OS if virtualization is being employed.\nApply any patches or upgrades to correct for known vulnerabilities.\nCreate a dedicated physical disk or logical partition (separate from OS and Web server application).\n```# Web Server Security Guidelines"
          }
        ]
      },
      {
        "title": "General Recommendations",
        "content": [
          {
            "text": "Remove or disable all services installed by the Web server application but not required (e.g., gopher, FTP, remote administration).\nRemove or disable all unneeded default login accounts created by the Web server installation.\nRemove all manufacturers' documentation from the server.\nRemove all example or test files from the server, including scripts and executable code.\nApply appropriate security template or hardening script to server.\nReconfigure HTTP service banner (and others as required) not to report Web server and OS type and version (this may not be possible with all Web servers)."
          },
          {
            "text": "Organizations should consider installing the Web server with non-standard directory names, directory locations, and filenames. Many Web server attack tools and worms targeting Web servers only look for files and directories in their default locations. While this will not stop determined attackers, it will force them to work harder to compromise the server, and it also increases the likelihood of attack detection because of the failed attempts to access the default filenames and directories and the additional time needed to perform an attack."
          }
        ]
      },
      {
        "title": "5.2 Configuring Access Controls",
        "content": [
          {
            "text": "Most Web server host OSs provide the capability to specify access privileges individually for files, devices, and other computational resources on that host. Any information that the Web server can access using these controls can potentially be distributed to all users accessing the public Web site. The Web server software is likely to include mechanisms to provide additional file, device, and resource access controls specific to its operation. It is important to set identical permissions for both the OS and Web server application; otherwise, too much or too little access may be granted to users. Web server administrators should consider how best to configure access controls to protect information stored on public Web servers from two perspectives:\n- Limit the access of the Web server application to a subset of computational resources.\n- Limit the access of users through additional access controls enforced by the Web server, where more detailed levels of access control are required."
          },
          {
            "text": "The proper setting of access controls can help prevent the disclosure of sensitive or restricted information that is not intended for public dissemination. In addition, access controls can be used to limit resource use in the event of a DoS attack against the Web server. Similarly, access controls can enforce separation of duty by ensuring Web server logs cannot be modified by Web server administrators and potentially ensure that the Web server process is only allowed to append to the log files."
          },
          {
            "subsection": "Typical files to which access should be controlled are as follows:",
            "text": []
          },
          {
            "text": "Application software and configuration files\nFiles related directly to security mechanisms:\nPassword hash files and other files used in authentication\nFiles containing authorization information used in controlling access\nCryptographic key material used in confidentiality, integrity, and non-repudiation services\nServer log and system audit files\nSystem software and configuration files\nWeb content files.# 5.2.1 Configuring the Permissions of the Web Server Application"
          },
          {
            "text": "It is vital that the Web server application executes only under a unique individual user and group identity with very restrictive access controls. New user and group identities should be established for exclusive use by the Web server software. The new user and new group should be independent from all other users and groups and unique. This is a prerequisite for implementing the access controls described in the following steps. During initialization, the server may have to run with root (Unix) or administrator/system (Windows) privileges to bind to Transmission Control Protocol (TCP) ports numbered below 1024 (80 and 443 are the default ports for HTTP and HTTPS). Ensure that the Web server is configured to reduce its privileges to those of the Web server user after performing its initialization functions."
          },
          {
            "text": "In addition, use the Web server OS to limit which files can be accessed by the Web server's service processes. These processes should have read-only access to those files necessary to perform the service and should have no access to other files, such as server log files. Use Web server host OS access controls to enforce the following [Koss00]:"
          },
          {
            "text": "Service processes are configured to run as a user with a strictly limited set of privileges (i.e., not running as root, administrator, or equivalent).\nWeb content files can be read but not written by service processes.\nService processes cannot write to the directories where public Web content is stored.\nOnly processes authorized for Web server administration can write Web content files.\nThe Web server application can write Web server log files, but log files cannot be read by the Web server application. Only root/system/administrative level processes can read Web server log files.\nTemporary files created by the Web server application, such as those that might be generated in the creation of dynamic Web pages or by users uploading content, are restricted to a specified and appropriately protected subdirectory (if possible).\nAccess to any temporary files created by the Web server application is limited to the Web server processes that created the files (if possible)."
          },
          {
            "text": "It is also necessary to ensure that the Web server application cannot save (or, in some cases, read) files outside the specified file structure dedicated to public Web content. This may be a configuration choice in the server software, or it may be a choice in how the server process is controlled by the OS. Ensure that such directories and files (outside the specified directory tree) cannot be accessed, even if users perform direct browsing by accessing the URLs of those files or through directory traversal attacks against the Web server process."
          },
          {
            "text": "To mitigate the effects of certain types of DoS attacks, configure the Web server to limit the amount of OS resources it can consume. Some examples include\u2014"
          },
          {
            "text": "Installing Web content on a different hard drive or logical partition than the OS and Web server application.\nPlacing a limit on the amount of hard drive space that is dedicated for uploads, if uploads to the Web.# CURRENT_PAGE_RAW_OCR_TEXT"
          }
        ]
      },
      {
        "title": "Server Uploads",
        "content": [
          {
            "text": "Server uploads are allowed. Ideally, uploads should be placed on a separate partition to provide stronger assurance that the hard drive limit cannot be exceeded."
          },
          {
            "text": "If uploads are allowed to the Web server, ensuring that these files are not readable by the Web server until after some automated or manual review process is used to screen them. This measure prevents the Web server from being used to propagate malware or traffic pirated software, attack tools, pornography, etc. It is also possible to limit the size of each uploaded file, which could limit the potential effects of a DoS attack involving uploading many large files.\n\n\nEnsuring that log files are stored in a location that is sized appropriately. Ideally, log files should be stored on a separate partition. If an attack causes the size of the log files to increase beyond acceptable limits, a physical partition helps ensure the Web server has enough resources to handle the situation appropriately.\n\n\nConfiguring the maximum number of Web server processes and/or network connections that the Web server should allow."
          },
          {
            "text": "If uploads are allowed to the Web server, ensuring that these files are not readable by the Web server until after some automated or manual review process is used to screen them. This measure prevents the Web server from being used to propagate malware or traffic pirated software, attack tools, pornography, etc. It is also possible to limit the size of each uploaded file, which could limit the potential effects of a DoS attack involving uploading many large files."
          },
          {
            "text": "Ensuring that log files are stored in a location that is sized appropriately. Ideally, log files should be stored on a separate partition. If an attack causes the size of the log files to increase beyond acceptable limits, a physical partition helps ensure the Web server has enough resources to handle the situation appropriately."
          },
          {
            "text": "Configuring the maximum number of Web server processes and/or network connections that the Web server should allow."
          },
          {
            "text": "To some degree, these actions protect against attacks that attempt to fill the file system on the Web server host OS with extraneous and incorrect information that may cause the system to crash. Logging information generated by the Web server host OS may help in recognizing such attacks. As discussed in Section 9.1, administrators should store Web server logs on centralized logging servers whenever possible and also store logs locally if feasible. If an attack causes the Web server to be compromised, the attacker could modify or erase locally stored logs to conceal information on the attack. Maintaining a copy of the logs on a centralized logging server gives administrators more information to use when investigating such a compromise."
          },
          {
            "text": "In addition to the controls mentioned above, it is often necessary to configure timeouts and other controls to further reduce the impact of certain DoS attacks. One type of DoS attack takes advantage of the practical limits on simultaneous network connections by quickly establishing connections up to the maximum permitted, such that no new legitimate users can gain access. By setting network connection timeouts (the time after which an inactive connection is dropped) to a minimum acceptable time limit, established connections will time out as quickly as possible, opening up new connections to legitimate users. This measure only mitigates the effects; it does not defeat the attack."
          },
          {
            "text": "If the maximum number of open connections (or connections that are half-open\u2014that is, the first part of the TCP handshake was successful) is set to a low number, an attacker can easily consume the available connections with illegitimate requests (often called a SYN flood). Setting the maximum to a much higher number may mitigate the effect of such an attack, but at the expense of consuming additional resources. Note that this is only an issue for Web servers that are not protected by a firewall that stops SYN flood attacks. Most enterprise-level firewalls protect Web servers from SYN floods by intercepting them before they reach the Web servers."
          }
        ]
      },
      {
        "title": "5.2.2 Configuring Secure Web Content Directory# Do not use links, aliases, or shortcuts in the public Web content file directory",
        "content": [
          {
            "text": "Do not use links, aliases, or shortcuts in the public Web content file directory tree that point to directories or files elsewhere on the server host or the network file system. If possible, disable the ability of the Web server software to follow links and aliases. As stated earlier, Web server log files and configuration files should reside outside the specified file directory tree for public Web content."
          }
        ]
      },
      {
        "title": "The following steps are required to restrict access to a specific Web content file directory tree:",
        "content": [
          {
            "text": "Dedicate a single hard drive or logical partition for Web content and establish related subdirectories exclusively for Web server content files, including graphics but excluding scripts and other programs.\nDefine a single directory tree exclusively for all external scripts or programs executed as part of Web content (e.g., CGI, Active Server Page [ASP], PHP).\nDisable the execution of scripts that are not exclusively under the control of administrative accounts. This action is accomplished by creating and controlling access to a separate directory intended to contain authorized scripts.\nDisable the use of hard or symbolic links.\nDefine a complete Web content access matrix. Identify which folders and files within the Web server document should be restricted and which should be accessible (and by whom)."
          },
          {
            "text": "Most Web server software vendors provide directives or commands that allow the Web administrator to restrict user access to public Web server content files. For example, the Apache Web server software provides a <Limit> directive, which allows the Web administrator to restrict which optional access features (such as New, Delete, Connect, Head, and Get) are associated with each Web content file; any HTTP method omitted from the <Limit> directive will be allowed. Within the <Limit> directive, administrators can specify the requirements that must be met for the Limited action to be allowed. The Apache Require directive allows the Web administrator to restrict available content to authenticated users or groups."
          },
          {
            "text": "Many directives or commands can be overridden on a per-directory basis. The convenience of being able to make local exceptions to global policy is offset by the threat of a security hole being introduced in a distant subdirectory, which could be controlled by a hostile user. The Web administrator should disable a subdirectory's ability to override top-level security directives unless that override is absolutely necessary."
          },
          {
            "text": "In most cases, Web server file directory listings should be disabled. The HTTP specifies that a URL ending in a slash character be treated as a request for a listing of the files in the directory with that name. Web servers should be prohibited from responding to such requests with a file listing, even if the public can read all of the directory files. Such requests often indicate an attempt to locate information by means other than those intended by the Web administrator or Webmaster. Users may attempt this if they are having difficulty navigating through the site or if a link appears to be broken. Intruders may attempt this to locate information hidden by the Web site's interface. Web administrators should investigate requests of this type found in the Web server log files (see Section 9)."
          }
        ]
      },
      {
        "title": "5.2.3 Uniform Resource Identifiers and Cookies# Uniform Resource Identifiers (URI)",
        "content": [
          {
            "text": "Uniform Resource Identifiers (URI) are the address technology from which URLs are created. Technically URLs (e.g., http://www.mywww.gov) are a subset of URIs. There are a number of security issues that arise from URIs. Because URIs are sent in the clear, any data stored within them can be easily compromised. For example, URIs are recorded in numerous locations, including Web browser logs (i.e., browser history), proxy server logs, and third-party HTTP referrer logs. Thus, hiding sensitive data such as usernames and passwords or hidden server resources in URIs is not recommended. Security through obscurity is not secure."
          }
        ]
      },
      {
        "title": "Public Web Content and URIs",
        "content": [
          {
            "text": "URIs are often included with public Web content. Although these URIs may not display as Web content in a user's Web browser, they can be easily discovered in the source code. Therefore, no publicly served Web content should include sensitive URIs hidden in the source code. Many attackers and malicious bots (see Section 5.2.4) search the source code for sensitive URI information, including\u2014"
          },
          {
            "text": "E-mail addresses\nImages on other servers\nLinks to other servers\nParticular text expressions (e.g., userid, password, root, administrator)\nHidden form values\nHyperlinks."
          }
        ]
      }
    ]
  },
  {
    "title": "5.2.4 Controlling Impact of Web \"Bots\" on Web Servers",
    "subsections": [
      {
        "content": "Web bots (also known as crawlers or spiders) are software applications used to collect, analyze, and index Web content. Web bots are used by numerous organizations for many purposes. Some examples include\u2014"
      },
      {
        "content": "MSNBot, Slurp, and Googlebot slowly and carefully analyze, index, and record Web sites for Web search engines such as Windows Live Search, Yahoo! and Google.\nMediabot is used by Google to analyze content served by an AdSense page so that contextually relevant ads will be supplied.# Hyperlink Validators and Bots"
      },
      {
        "title": "Hyperlink Validators",
        "content": [
          {
            "text": "Hyperlink \"validators\" are used by Webmasters to automatically validate the hyperlinks on their Web site."
          }
        ]
      },
      {
        "title": "Email Crawlers",
        "content": [
          {
            "text": "EmailSiphon and Cherry Picker are bots specifically designed to crawl Web sites for electronic mail (e-mail) addresses to add to spam mailing lists. These are common examples of bots that may have a negative impact on a Web site or its users."
          }
        ]
      },
      {
        "title": "Spambots",
        "content": [
          {
            "text": "Many spambots crawl Web sites for login forms to create free e-mail addresses from which to send spam or to spam blogs, guestbooks, wikis, and forums to boost the search engine rankings of a particular Web site."
          }
        ]
      },
      {
        "title": "Screen Scrapers",
        "content": [
          {
            "text": "Screen scrapers retrieve content from Web sites to put up a copy on another server. These copies can be used for phishing or for attempting to generate ad revenue by having users visit the copy."
          }
        ]
      },
      {
        "title": "Malicious Bots",
        "content": [
          {
            "text": "Some malicious bots crawl Web sites looking for vulnerable applications containing sensitive data (e.g., Social Security Numbers [SSN], credit card data)."
          }
        ]
      },
      {
        "title": "Challenges Presented by Bots",
        "content": [
          {
            "text": "Bots can present a challenge to Webmasters' administration of their servers because\u2014\n- Web servers often contain directories that do not need to be indexed.\n- Organizations might not want part of their site appearing in search engines.\n- Web servers often contain temporary pages that should not be indexed.\n- Organizations operating the Web server are paying for bandwidth and want to exclude robots and spiders that do not benefit their goals.\n- Bots are not always well written or well intentioned and can hit a Web site with extremely rapid requests, causing a reduction in responsiveness or outright DoS for legitimate users.\n- Bots may uncover information that the Webmaster would prefer remained secret or at least unadvertised (e.g., e-mail addresses)."
          }
        ]
      },
      {
        "title": "Controlling Bot Behavior",
        "content": [
          {
            "text": "Fortunately, Web administrators or the Webmaster can influence the behavior of most bots on their Web site. A series of agreements called the Robots Exclusion Protocol (REP) has been created. Although REP is not an official Internet standard, it is supported by most well-written and well-intentioned bots, including those used by most major search engines."
          },
          {
            "text": "Web administrators who wish to limit bots' actions on their Web server need to create a plain text file named \"robots.txt.\" The file must always have this name, and it must reside in the Web server's root document directory. In addition, only one file is allowed per Web site. Note that the robots.txt file is a standard that is voluntarily supported by bot programmers, so malicious bots (such as EmailSiphon and Cherry Picker) often ignore this file."
          }
        ]
      }
    ]
  },
  {
    "title": "Robots.txt Overview",
    "subsections": [
      {
        "content": "If you specify \"GoogleBot\" only, then the \"*\" would apply to any other robot."
      },
      {
        "content": "Disallow tells the bot(s) specified in the user-agent field which sections of the Web site are excluded. For example, /images informs the bot not to open or index any files in the images directory or any subdirectories. Thus, the directory /images/special/ would not be indexed by the excluded bot(s)."
      },
      {
        "content": "Note that /do matches any directory beginning with /do (e.g. /do, /document, /docs, etc.), whereas /do/ matches only the directory named /do/. A Web administrator can also specify individual files for exclusion. For example, the Web administrator could specify /mydata/help.html to prevent only that one file from being accessed by the bots. A value of just / indicates that nothing on the Web site is allowed to be accessed by the specified bot(s)."
      },
      {
        "content": "At least one disallow per user-agent record must exist."
      },
      {
        "title": "Examples of Using robots.txt",
        "content": [
          {
            "text": "There are many ways to use the robots.txt file. Some simple examples are as follows:"
          },
          {
            "text": "To disallow all (compliant) bots from specific directories:"
          },
          {
            "text": "User-agent:  *\nDisallow: /images/\nDisallow: /banners/\nDisallow: /Forms/\nDisallow: /Dictionary/\nDisallow: /_borders/\nDisallow: /_fpclass/\nDisallow: /_overlay/\nDisallow: /_private/\nDisallow: /_themes/"
          },
          {
            "text": "To disallow all (compliant) bots from the entire Web site:"
          },
          {
            "text": "User-agent:  *\nDisallow: /"
          },
          {
            "text": "To disallow a specific bot (in this case the Googlebot) from examining a specific Web page:"
          },
          {
            "text": "User-agent:  GoogleBot\nDisallow: tempindex.htm"
          },
          {
            "text": "Note that the robots.txt file is available to everyone and does not provide access control mechanisms to the disallowed files. Thus, a Web administrator should not specify the names of sensitive files or folders because attackers often analyze robots.txt files to guide their initial investigations of Web sites. If files or directories must be excluded, it is better to use password-protected pages that cannot be accessed by bots. Password protection is the only reliable way to exclude noncompliant bots or curious users. See Section 7 for more information on Web-based authentication methods."
          }
        ]
      },
      {
        "title": "Spambots and Their Impact",
        "content": [
          {
            "text": "Often, spambots ignore robots.txt and search for email addresses on the Web site and/or forms to which they can submit spam-related content. Spambots that merely scan the Web site typically do not affect its availability. Nevertheless, it may be beneficial to prevent them from harvesting e-mail addresses by performing address munging [Unsp06]\u2014displaying e-mail addresses in an alternative human-readable format, such as listing name@mywww.gov as <name at mywww dot gov>. Unfortunately, these techniques do not stop all spambots. The best defense against address harvesting is not to display e-mail addresses."
          },
          {
            "text": "Spambots searching for Web forms to submit spam-related content are a direct threat to the Web site. They can affect the organization's image if visitors view the submitted content.\n```# CURRENT_PAGE_RAW_OCR_TEXT"
          },
          {
            "text": "as an endorsement. They may also affect the Web site's availability by making it difficult for users to find necessary content."
          },
          {
            "text": "There are several techniques available to reduce the amount of spam submissions, including\u2014"
          },
          {
            "text": "Blocking form submissions that use spam-related keywords\nUsing the rel=\"nofollow\" keyword in all submitted links, which will cause search engines to omit the links in their page-ranking algorithms, directly affecting the goals of a spambot [Google05]\nRequiring submitters to solve a Completely Automated Public Turing Test to Tell Computers and Humans Apart (CAPTCHA) prior to being allowed to submit content."
          },
          {
            "text": "These techniques all have benefits and drawbacks associated with them. For example, some CAPTCHA techniques, which can be implemented as an obscured word in an image, do not comply with American Disability Association (ADA) or Section 508 accessibility guidelines. Information about ongoing research on detecting spambots can be found through the Adversarial Information Retrieval on the Web (AIRWeb) workshops."
          }
        ]
      },
      {
        "title": "6. Securing Web Content",
        "content": [
          {
            "text": "The two main components of Web security are the security of the underlying server application and OS, and the security of the actual content. Of these, the security of the content is often overlooked. Maintaining effective content security itself has two components. The more obvious is not placing any proprietary, classified, or other sensitive information on a publicly accessible Web server, unless other steps have been taken to protect the information via user authentication and encryption (see Section 7)."
          },
          {
            "text": "The less obvious component of content security is avoiding compromises caused by the way particular types of content are processed on a server. As organizations have gotten better at protecting and hardening their network perimeters, OSs, and Web servers, attackers have increasingly turned to exploiting vulnerabilities in Web applications and the way information is processed on Web servers. These application layer attacks exploit the interactive elements of Web sites."
          },
          {
            "subsection": "6.1 Publishing Information on Public Web Sites",
            "text": []
          },
          {
            "text": "Too often, little thought is given to the security implications of the content placed on the Web site. Many organizations do not have a Web publishing process or policy that determines what type of information to publish openly, what information to publish with restricted access, and what information should be omitted from any publicly accessible repository. This is troublesome because Web sites are often one of the first places that malicious entities search for valuable information. For example, attackers often read the contents of a target organization's Web site to gather intelligence before any attacks [Scam01]. Also, attackers can take advantage of content available on a Web site to craft a social engineering attack or to use individuals' identifying information in identity theft [FTC06]."
          },
          {
            "text": "Absent compelling reasons, a public Web site should not contain the following information:"
          },
          {
            "text": "Classified records\nInternal personnel rules and procedures\nSensitive or proprietary information\n\nPersonal information about an organization's personnel or users# CURRENT_PAGE_RAW_OCR_TEXT\n\n\nHome addresses and telephone numbers\n\nUniquely identifying information, particularly SSNs\nDetailed biographical material (that could be employed for social engineering)\nStaff family members\nTelephone numbers, e-mail addresses, or general listings of staff unless necessary to fulfill organizational requirements\nSchedules of organizational principals or their exact location (whether on or off the premises)\nInformation on the composition or preparation of hazardous materials or toxins\nSensitive information relating to homeland security\nInvestigative records\nFinancial records (beyond those already publicly available)\nMedical records\nThe organization's physical and information security procedures\nInformation about organization's network and information system infrastructure (e.g., address ranges, naming conventions, access numbers)\nInformation that specifies or implies physical security vulnerabilities\nPlans, maps, diagrams, aerial photographs, and architectural plans of organizational building, properties, or installations\nInformation on disaster recovery or continuity of operations plans except as absolutely required\nDetails on emergency response procedures, evacuation routes, or organizational personnel responsible for these issues\nCopyrighted material without the written permission of the owner\nPrivacy or security policies that indicate the types of security measures in place to the degree that they may be useful to an attacker."
          },
          {
            "text": "Personal information about an organization's personnel or users# CURRENT_PAGE_RAW_OCR_TEXT"
          },
          {
            "text": "Home addresses and telephone numbers"
          },
          {
            "text": "Organizations should not use public Web servers to host sensitive information intended to be accessed only by internal users. The compromise of a public Web server often leads to the compromise of such data."
          },
          {
            "text": "To ensure a consistent approach, an organization should create a formal policy and process for determining and approving the information to be published on a Web server. In many organizations, this is the responsibility of the CIO and/or public affairs officer. Such a process should include the following steps:"
          },
          {
            "text": "Identify information that should be published on the Web\nIdentify the target audience (Why publish if no audience exists?)\nIdentify possible negative ramifications of publishing the information\nIdentify who should be responsible for creating, publishing, and maintaining this particular information\nCreate or format information for Web publishing\nReview the information for sensitivity and distribution/release controls (including the sensitivity of the information in aggregate)\nDetermine the appropriate access and security controls\nPublish information\nVerify published information\nPeriodically review published information to confirm continued compliance with organizational guidelines."
          },
          {
            "text": "Any policy or process for determining and approving the information to be published on a Web server can benefit from the use of automated tools. Tools can scan incoming content for keywords, formatting, or metadata, and flag it for review, easing the burden of those required to verify.# CURRENT_PAGE_RAW_OCR_TEXT"
          },
          {
            "text": "content. Similarly, an internal automated system that allows users to post potential material to an internal Web site and notifies approving personnel (possibly via e-mail) of the posting allows material to be reviewed and posted to the public Web site more quickly through a repeatable process. Using an automated system also aids accountability because logs track who submitted the document and who approved it."
          },
          {
            "text": "An often-overlooked area of Web content is the information sometimes hidden within the source code of a Web page. This information can be viewed from any Web browser using the \"view source code\" menu option. The source code can, for example, contain points of contact and reveal portions of the directory structure of the Web server. Organizations often do not pay attention to the contents of the source code on their Web sites, even though this code may contain sensitive information. Attackers scour not only the obvious content of the Web site but also details within the source code. Thus, Web administrators or Webmasters should periodically review code on their public Web server."
          }
        ]
      },
      {
        "title": "6.2 Observing Regulations about the Collection of Personal Information",
        "content": [
          {
            "text": "Federal and state laws and regulations apply to the collection of user information on publicly accessible government Web sites. In addition, many government agencies have privacy guidelines that address the type of information that could be collected about users. Governmental organizations with Web sites should be aware of the appropriate and applicable laws, regulations, and agency guidelines. Private organizations may wish to use these guidelines and examples of sound security practices but should consult appropriate legal counsel and their privacy officials for the applicable legal and policy implications. However, Federal laws, regulations, and applicable agency guidelines do apply to commercial organizations that operate Web sites on behalf of Federal agencies. Organizations should be aware of changes to legal, regulatory, and contractual requirements and seek advice from knowledgeable legal and policy experts."
          },
          {
            "text": "Federal agencies that collect PII must do so in accordance with Federal law and the Constitution. The Privacy Act, for example, requires agencies to minimize the information collected to that which is relevant and necessary to the business purpose, and, in many cases, to collect information, to the greatest extent practicable, directly from the subject individual. In addition, accepted practices (many of which are reflected in laws applicable to both private and public institutions) are to provide subject individuals:"
          },
          {
            "text": "Notice that information about them is being collected, including descriptions of what data is being collected, with whom it is being shared, and what is being done with that data\nOpportunities to opt out of data collection unless the data collection is mandatory under law, necessary to the performance of a contract with the subject individual, or if the individual has freely offered his/her PII\nOpportunities to access and review the records kept about themselves, and to request corrections or additions, especially if that information may be used to make a determination# About the Individuals' Rights, Opportunities, or Benefits"
          },
          {
            "text": "The following are examples of personal information:"
          },
          {
            "text": "Name\nE-mail address\nMailing address\nTelephone number\nSSN\nFinancial information."
          },
          {
            "text": "Federal agencies and many state agencies are also restricted in their ability to use Web browser cookies [OMB00a, OMB00b, OMB00c, and MASS99]. A cookie is a small piece of information that may be written to a user's hard drive when a Web site is visited. There are two principal types of cookies:"
          },
          {
            "text": "Persistent cookies cause the most concern. These cookies can be used to track activities of users over time and across different Web sites. The most common use of persistent cookies is to retain and correlate information about users between sessions. Federal agencies and many state agencies are generally prohibited from using persistent cookies on publicly accessible Web sites.\n\n\nSession cookies are valid for a single session (visit) to a Web site. These cookies expire at the end of the session or within a limited time frame. Because these cookies cannot be used to track personal information, they are generally not subject to the prohibition that applies to persistent cookies. However, their use must be clearly stated and defined in the Web site's privacy statement."
          },
          {
            "text": "Persistent cookies cause the most concern. These cookies can be used to track activities of users over time and across different Web sites. The most common use of persistent cookies is to retain and correlate information about users between sessions. Federal agencies and many state agencies are generally prohibited from using persistent cookies on publicly accessible Web sites."
          },
          {
            "text": "Session cookies are valid for a single session (visit) to a Web site. These cookies expire at the end of the session or within a limited time frame. Because these cookies cannot be used to track personal information, they are generally not subject to the prohibition that applies to persistent cookies. However, their use must be clearly stated and defined in the Web site's privacy statement."
          }
        ]
      },
      {
        "title": "6.3 Mitigating Indirect Attacks on Content",
        "content": [
          {
            "text": "Indirect content attacks are not direct attacks on a Web server or its contents; they involve roundabout means to gain information from users who normally visit the Web site maintained on the Web server. The common theme of these attacks is to coerce users into visiting a malicious Web site set up by the attacker and divulging personal information in the belief that the site they visited is the legitimate Web site. While customers of electronic commerce and financial institutions are often targeted, such attacks are not limited to those Web sites. Besides acquiring personal information related to the targeted Web site, attacks may also be directed against the user's computer from the malicious Web site visited. The types of indirect attacks described in this section are phishing and pharming."
          },
          {
            "subsection": "6.3.1 Phishing",
            "text": []
          },
          {
            "text": "Phishing attackers use social engineering techniques to trick users into accessing a fake Web site and divulging personal information. In some phishing attacks, attackers send a legitimate-looking e-mail asking users to update their information on the company's Web site, but the URLs in the e-mail actually point to a false Web site. Other phishing attacks may be more advanced and take advantage of vulnerabilities in the legitimate Web site's application."
          },
          {
            "text": "Although phishing cannot be prevented entirely through technical means employed on a Web server, many techniques can reduce the likelihood that a Web site's users will be lured into a phishing attack [Ollm04]:"
          },
          {
            "text": "Ensuring customer awareness of the dangers of phishing attacks and how to avoid them. The Federal# Trade Commission (FTC) Consumer Alert"
          },
          {
            "text": "The Trade Commission (FTC) has posted a consumer alert outlining steps that users should take:"
          }
        ]
      },
      {
        "title": "Steps to Take",
        "content": [
          {
            "text": "Do not reply to email messages or popup ads asking for personal or financial information.\nDo not trust telephone numbers in e-mails or popup ads. Voice over Internet Protocol technology can be used to register a telephone with any area code.\nUse antivirus, anti-spyware, and firewall software. These can detect malware on a user's machine that is participating in a phishing attack.\nDo not email personal or financial information.\nReview credit card and bank account statements regularly.\nBe cautious about accessing untrusted Web sites because some Web browser vulnerabilities can be exploited simply by visiting such sites. Users should also be cautious about opening any attachment or downloading any file from untrusted emails or Web sites.\nForward phishing-related emails to spam@uce.gov and to the organization that is impersonated in the email.\nRequest a copy of your credit report yearly from each of the three credit reporting agencies: Equifax, TransUnion, and Experian. If an identity thief opens accounts in your name, they will likely show up on your credit report.\nValidating official communication by personalizing emails and providing unique identifying information that only the organization and user should know. However, confidential information should not be disclosed.\nUsing digital signatures on e-mail. However, digital signatures may not be validated automatically by the user's email application.\nPerforming content validation within the Web application. Vulnerabilities in the organization's Web applications may be used in a phishing attack.\nPersonalizing Web content, which can aid users in identifying a fraudulent Web site.\nUsing token-based or mutual authentication at the Web site to prevent phishers from reusing previous authentication information to impersonate the user."
          }
        ]
      },
      {
        "title": "Phishing Protection",
        "content": [
          {
            "text": "Most Web browsers provide some level of phishing protection. All Web browsers inform users when they visit a secured site via a padlock or some other GUI mechanism, and they also inform users if the Domain Name System (DNS) address visited does not match that of the Public Key Infrastructure (PKI) certificate. However, phishing sites often use DNS addresses that are similar to those of the original sites and that have a valid PKI certificate, making them harder to detect. In such cases, a Web browser would notify the user of the danger only if the site was a known phishing site."
          },
          {
            "text": "Browsers may either download a phishing blacklist from the browser manufacturer's Web site periodically or check all Web requests against an anti-phishing database. Organizations should use Web browser-provided anti-phishing features where applicable. In addition, a number of vendors offer more advanced anti-phishing solutions and services."
          }
        ]
      },
      {
        "title": "Advanced Anti-Phishing Solutions",
        "content": [
          {
            "text": "Cousin Domain Monitoring and Prevention\u2014Vendors (primarily domain name registrars) monitor and in some instances prevent the creation of domain names similar to those of...# Organizations That May Be Subject to Phishing Attacks\n\n\nAttack Detection and Analysis\u2014Vendors monitor e-mail and Web communication to discover ongoing phishing campaigns so that organizations can take appropriate responses.\n\nTakedown\u2014Vendors aid in limiting access to the phishing Web site.\nFraud Analysis\u2014Vendors monitor access to the organization's Web site for potential fraud attempts (such as phishers attempting to use captured credentials) or monitor the Web for fraudulent use of an organization's identity.\nForensic Services\u2014After discovery of a successful phishing attack, vendors aid in addressing issues that arise as a result of the attack.\nConsumer Toolbars\u2014Vendors provide browser plug-ins that can provide or augment phishing detection available in users' browsers.\nE-mail Authentication\u2014Vendors provide secure e-mail solutions allowing users to discern whether or not an e-mail is from the organization itself or is a potential phishing attack.\nE-mail Filtering\u2014Vendors provide solutions to prevent an organization's internal users from receiving phishing e-mails.\nWeb Filtering\u2014Vendors monitor an organization's outbound Web requests and prevent users from accessing known or suspected phishing Web sites.\nAuthentication\u2014Vendors provide strong authentication solutions that are less susceptible to phishing attacks.\nLaw Enforcement Enablement\u2014Vendors assist organizations in contacting law enforcement officials to aid in shutting down and prosecuting phishing attacks."
          },
          {
            "text": "Cousin Domain Monitoring and Prevention\u2014Vendors (primarily domain name registrars) monitor and in some instances prevent the creation of domain names similar to those of...# Organizations That May Be Subject to Phishing Attacks"
          },
          {
            "text": "Attack Detection and Analysis\u2014Vendors monitor e-mail and Web communication to discover ongoing phishing campaigns so that organizations can take appropriate responses."
          },
          {
            "text": "When contemplating anti-phishing measures, it is important to consider the type of information being hosted on the Web site. Web sites with little or no sensitive information may not need to implement more advanced or costly anti-phishing measures. Web sites storing PII should strongly consider implementing more robust anti-phishing measures."
          }
        ]
      },
      {
        "title": "6.3.2 Pharming",
        "content": [
          {
            "text": "Pharming attackers use technical means, instead of social engineering, to redirect users into accessing a fake Web site masquerading as a legitimate one and divulging personal information. Pharming is normally accomplished either by exploiting vulnerabilities in DNS software, which is used to resolve human-readable Internet domain names into IP addresses, or by altering the host files maintained on a client computer for locally resolving Internet domain names. In either case, the affected system incorrectly resolves legitimate names to the malicious Web site address. Various techniques can help reduce the likelihood that a Web site's users become involved in a pharming attack [Ollm05]:"
          },
          {
            "text": "Using the Current Versions of DNS Software with the Latest Security Patches Applied\u2014A compromised DNS server will allow attackers to direct users to a malicious server while maintaining a legitimate DNS name.\nInstalling Server-Side DNS Protection Mechanisms Against Pharming\u2014There are tools available to mitigate threats to DNS software, such as the DNS Security Extensions; these are discussed in...# NIST SP 800-81, Secure Domain Name System (DNS) Deployment Guide"
          }
        ]
      },
      {
        "title": "Monitoring Organizational Domains and the Registration of Similar Domains\u2014Pharming",
        "content": [
          {
            "text": "Attacks may take advantage of users who misspell the organization's domain name when accessing the site."
          }
        ]
      },
      {
        "title": "Simplifying the Structure and Number of Organizational Domain Names",
        "content": [
          {
            "text": "If an organization has a complicated naming structure for its servers, it becomes increasingly difficult for users to discern whether they are on an illegitimate site. For example, many organizations will have users login at one URL, such as https://www.organization.org/, but then redirect them to another URL, such as https://www.secure-organization.org/. A user redirected to https://www.secured-organization.org/ may not notice the attack."
          }
        ]
      },
      {
        "title": "Using Secure Connections (i.e., HTTPS) for Logins",
        "content": [
          {
            "text": "This allows users to verify that the server certificates are valid and associated with a legitimate web site. Modern browsers will notify a user if the DNS name does not match the one provided by the certificate, but some pharming sites could have a legitimate certificate."
          }
        ]
      },
      {
        "title": "Ensuring User Awareness of the Dangers of Pharming Attacks and How to Avoid Them",
        "content": [
          {
            "text": "Pharming is a recent phenomenon; many users may not know to watch for pharming attacks."
          }
        ]
      },
      {
        "title": "Verifying Third-Party Host Resolution",
        "content": [
          {
            "text": "A number of vendors provide third-party web browser plug-ins that support matching the Internet Protocol (IP) address of a web site against a previously verified \"good\" IP address, providing users with a warning if the web site is suspicious."
          }
        ]
      },
      {
        "title": "Using Pre-Shared Secrets",
        "content": [
          {
            "text": "Pre-shared secrets can be used to prevent pharming attacks. A common implementation of pre-shared secrets is to have authorized users set up certain questions and answers that only they should know. In addition, the web site provides each user with a specific image and/or phrase that only it and the user knows. Subsequently, when a user logs in to the web site, the user is asked one of the secret questions. If the user answers correctly, he or she is presented with the secret image/phrase and only then asked for a password. Since a pharming site would not know those pre-shared secrets and be able to respond accordingly, it should be recognizable as a malicious site."
          },
          {
            "text": "The main disadvantage of using pre-shared secrets is that user acceptance may be low because of the work involved to set up the secrets and log into a site. Moreover, some users might not recognize the missing data and use the pharming site anyway."
          },
          {
            "text": "Many of the techniques used to prevent phishing attacks\u2014particularly in commercial offerings\u2014are relevant to preventing pharming attacks. As with anti-phishing solutions, when contemplating anti-pharming measures, it is important to consider the type of information being hosted on the web site. Web sites with little or no sensitive information may not need to implement more advanced or costly anti-pharming measures. Web sites storing PII should strongly consider implementing more robust anti-pharming measures. Requiring strong authentication can greatly reduce the risk of successful phishing and pharming attacks."
          }
        ]
      },
      {
        "title": "6.4 Securing Active Content and Content Generation Technologies# In the early days of the Web",
        "content": [
          {
            "text": "Most sites presented textual, static HyperText Markup Language (HTML) pages. No interactivity occurred between the user and Web site beyond the user clicking on hyperlinks."
          },
          {
            "text": "Soon thereafter, various types of interactive elements were introduced that offered users new ways to interact more dynamically with Web sites. Unfortunately, these interactive elements introduced many Web-related vulnerabilities that remain a concern today."
          }
        ]
      },
      {
        "title": "Active Content",
        "content": [
          {
            "text": "Active content refers to interactive program elements downloaded to the client (i.e., a Web browser) and processed there instead of the server. A variety of active content technologies exists; some of the more popular examples are ActiveX, Java, VBScript, JavaScript, and Asynchronous JavaScript and XML (AJAX). The use of active content often requires users to reduce the security settings on their Web browsers for processing to occur. If not implemented correctly, active content can present a serious threat to the end user. For example, active content can take actions independently without the knowledge or expressed consent of the user. While active content poses risk to the client, it can also pose risk to the Web server. The reason is that information processed on the client is under the control of the user, who can potentially manipulate the results by reverse engineering and tampering with the active content. For example, form validation processing done with active content elements on the client side can be changed to return out-of-range options or other unexpected results to the server."
          },
          {
            "text": "Therefore, the results of processing done on the client by elements of active content should not be trusted by the server; instead, the results should be verified by the server. Organizations considering the deployment of client-side active content should carefully consider the risks to both their users and their Web servers."
          }
        ]
      },
      {
        "title": "Content Generators",
        "content": [
          {
            "text": "Content generators are programs on a Web server that dynamically generate HTML pages for users; these pages may be generated using information retrieved from a backend server, such as a database or directory, or possibly user-supplied input. Some of the earliest content generators were CGI scripts executed by the Web server when a specific URL was requested. In contrast, some modern content generators are an integral component of the servers on which they run, such as Java Enterprise Edition (Java EE) application servers. Because content generators are implemented on the server, they can open the Web server itself to threats. The danger with content generators occurs when they blindly accept input from users and apply it to actions taken on the Web server. If the content generator has not been implemented correctly to restrict input, an attacker can enter certain types of information that may negatively affect the Web server or compromise its security. For example, one common attack against content generators is Structured Query Language (SQL) injection. In this type of attack, a malicious entity sends specially crafted input to the content generator. The input includes a specific SQL command string that, when submitted unfiltered to a SQL database server, potentially returns to the attacker any or all of the information stored in the database. SQL injections and other attacks.# Security Considerations for Active Content"
          },
          {
            "text": "Active content technologies are used to execute commands or gain unauthorized access to the Web server or a backend database server. All Web sites that implement active content and content generators should perform additional steps to protect the active content from compromise. These steps, which are discussed in the following sections, may not apply to all installations; therefore, they should be used as guidance in conjunction with appropriate manufacturer's documentation."
          },
          {
            "text": "Special caution is also required for downloading preprogrammed scripts or executables from the Internet. Many Web administrators and Webmasters are tempted to save time by downloading freely available code from the Internet. Although this is obviously convenient, it is not risk-free. There are many examples of malicious code being distributed this way. In general, no third-party scripts should be installed on a Web server unless they are first subjected to a thorough code review by a trusted expert. Security code reviews should also be considered for content on Web servers that are critical to the organization or are highly threatened."
          }
        ]
      },
      {
        "title": "6.4.1 Vulnerabilities with Client-Side Active Content Technologies",
        "content": [
          {
            "text": "Each active content technology has its own strengths and weaknesses; none is perfectly secure. Some of the more popular active content technologies and their associated risks are discussed below. New technologies are emerging all the time and older technologies are continually enhanced. Any Web administrator or Webmaster who is considering deploying a Web site with features that require active content technology on the client side should carefully weigh the risks and benefits of the technology before implementation. The relative risk of active content technologies changes over time, complicating this task. Nevertheless, common risks to the Web server prevail with all active content technologies."
          },
          {
            "text": "Because these technologies involve placing application code on the client, an attacker may attempt to reverse engineer the code to gain an understanding of how it functions with an organization's Web site and exploit that relationship. For example, relying only on input validation checks performed at the client by the active content and not validating the input again at the Web server would allow an attacker to use an HTTP proxy tool or editor to modify the active content elements and bypass any or all checks performed."
          },
          {
            "text": "Java is a full-featured, object-oriented programming language compiled into platform-independent byte code executed by an interpreter called the Java Virtual Machine (JVM). The resulting byte code can be executed where compiled or transferred to another Java-enabled platform (e.g., conveyed via an HTML Web page as an applet). Java is useful for adding functionality to Web sites. Many services offered by various popular Web sites require the user to have a Java-enabled browser. When the Web browser sees references to Java code, the browser loads the code and processes it using the built-in JVM or a user-installed one."
          },
          {
            "text": "The Java programming language and runtime environment enforce security primarily through strong type safety, by which a program can perform certain operations only on certain kinds.# Security Concerns in Java and JavaScript"
          }
        ]
      },
      {
        "title": "Java Security Model",
        "content": [
          {
            "text": "Java follows a so-called sandbox security model, used to isolate memory and method access and to maintain mutually exclusive execution domains. Java code, such as a Web applet, is confined to a \"sandbox,\" which is designed to prevent it from performing unauthorized operations, such as inspecting or changing files on a client file system and using network connections to circumvent file protections or users' expectations of privacy. Java byte code downloaded to the client can be decompiled into a more readable form by the user, making it susceptible to reverse engineering and possible modification, which poses a threat to the Web server."
          },
          {
            "text": "Hostile applets can also pose security threats to the client, even while executing within the sandbox. A hostile applet can consume or exploit system resources inappropriately, or can cause a user to perform an undesired or unwanted action. Examples of hostile applet exploits include DoS, mail forging, invasion of privacy (e.g., exporting of identity, e-mail address, and platform information), and installing backdoors to the system. Because the Java security model is rather complex, it can be difficult for a user to understand and manage. This situation can increase risk. Moreover, many implementation bugs have also been found, enabling security mechanisms to be bypassed [NIST01]."
          }
        ]
      },
      {
        "title": "JavaScript Overview",
        "content": [
          {
            "text": "JavaScript is a general purpose, cross-platform scripting language, whose code can be embedded within standard Web pages to create interactive documents. The name JavaScript is a misnomer because the language has little relationship to Java technology and was developed independently from it. Within the context of the Web browser, JavaScript is extremely powerful, allowing prepared scripts to perform essentially the same actions as those a user could take. Within that context, JavaScript lacks methods for directly accessing a client file system or for directly opening connections to computers other than the host that provided the content source. The browser also normally confines a script's execution to the page from which it was downloaded [NIST01]."
          }
        ]
      },
      {
        "title": "Security Implications of JavaScript",
        "content": [
          {
            "text": "In theory, confining a scripting language to the boundaries of a Web browser should provide a relatively secure environment. In practice, this has not been the case. Many attacks against browsers stem from the use of a scripting language in combination with exploitation of a security vulnerability. The sources of most problems have been twofold: the prevalence of implementation flaws in the execution environment and the close binding of the browser to other functionality, such as an e-mail client. Past exploits include sending a user's URL history list to a remote site and using the mail address of the user to forge e-mails [NIST01]. Client-side JavaScript can also be read and analyzed by an attacker to identify possible vulnerabilities in the Web server."
          }
        ]
      },
      {
        "title": "Adobe Flash",
        "content": [
          {
            "text": "Adobe Flash is a browser plug-in for major Web browsers that provides support for improved animation and interactivity. Although plug-ins such as Flash allow browsers to support new types of content, they are not active content in and of themselves, but simply an active-content-enabling technology. The Flash plug-in allows browsers to support vector and raster graphics, streaming audio.# Current Page Raw OCR Text"
          },
          {
            "text": "and video, and\nActionScript, a programming language similar to JavaScript used to control Flash animations. Several versions of Flash contain security flaws that allow remote code execution, requiring users to apply patches to the plug-in.\nAdobe Shockwave is a browser plug-in similar to Adobe Flash but more robust. Shockwave provides a faster rendering engine and supports hardware-accelerated three-dimensional graphics, layered graphics, and network protocols. While Flash is widely used for Web animations and movies, Shockwave is commonly used for games. As with Flash, several versions of Shockwave contain security flaws that allow remote code execution, requiring users to apply patches to the plug-in."
          }
        ]
      },
      {
        "title": "AJAX",
        "content": [
          {
            "text": "AJAX is a collection of technologies that allows Web developers to improve the response times between Web pages. JavaScript code communicates with the Web server and dynamically modifies the contents of the Web browser's page without relying on the Web server to send a response with the XML markup for the entire page. Instead, only the required portion of the affected XML data is transmitted. AJAX allows Web content to behave more like traditional applications, while potentially reducing the load on the Web server. However, a number of security concerns exist with AJAX:\n- AJAX creates a larger attack surface than traditional Web applications by increasing the number of points where a client interacts with the application.\n- AJAX may reveal details of internal functions within the Web application.\n- Some AJAX endpoints may not require authentication and instead rely on the current state of the application [SPID06]."
          }
        ]
      },
      {
        "title": "Visual Basic Script (VBScript)",
        "content": [
          {
            "text": "Visual Basic Script (VBScript) is a programming language developed by Microsoft for creating scripts that can be embedded in Web pages for viewing with the Internet Explorer browser. However, other browsers do not necessarily support VBScript. Like JavaScript, VBScript is an interpreted language that can process client-side scripts. VBScript, which is a subset of the Microsoft Visual Basic programming language, works with Microsoft ActiveX controls. The language is similar to JavaScript and poses similar risks."
          }
        ]
      },
      {
        "title": "ActiveX",
        "content": [
          {
            "text": "ActiveX is a set of technologies from Microsoft that provide tools for linking desktop applications to the Web. ActiveX controls are reusable component program objects that can be attached to e-mail or downloaded from a Web site. ActiveX controls also come preinstalled on Windows platforms. Web pages invoke ActiveX controls using a scripting language or with an HTML OBJECT tag. ActiveX controls are compiled program objects, making them difficult to read and reverse engineer."
          },
          {
            "text": "Unlike the Java sandbox model, which restricts the permissions of applets to a set of safe actions, ActiveX places no restrictions on what a control can do. Instead, ActiveX controls are digitally signed by their authors under a technology scheme called Authenticode. The digital signatures are verified using identity certificates issued by a trusted certificate authority to an ActiveX software publisher, who must pledge that no harmful code will be knowingly distributed under this scheme. The Authenticode process# CURRENT_PAGE_RAW_OCR_TEXT"
          },
          {
            "text": "ensures that ActiveX controls cannot be distributed anonymously and that tampering with the controls can be detected. This certification process, however, does not ensure that a control will be well behaved [NIST01]. Vulnerabilities in key ActiveX controls have been reported, including components installed by popular applications such as Microsoft Office."
          }
        ]
      },
      {
        "title": "6.4.2 Vulnerabilities with Server-Side Content Generation Technologies",
        "content": [
          {
            "text": "Unlike the above technologies, CGI, ASP .NET, Java EE, and other similar server interfaces fall on the server side (Web) of the client-server model. Common uses of server-side execution include [Ziri02]\u2014"
          },
          {
            "text": "Database access\nE-commerce/e-government applications\nChat rooms\nThreaded discussion groups."
          },
          {
            "text": "The server-side applications can be written in many programming languages to run on a Web server. If scripts and components are not prepared carefully, however, attackers can find and exercise flaws in the code to penetrate the Web server or backend components such as a database server. Therefore, scripts must be written with security in mind; for example, they should not run arbitrary commands on a system or launch insecure programs. An attacker can find flaws through trial and error and does not necessarily need the source code for the script to uncover vulnerabilities [NIST01]."
          },
          {
            "text": "Server-side content generators can create the following security vulnerabilities at the server:"
          },
          {
            "text": "They may intentionally or unintentionally leak information about the Web server application and host OS that can aid an attacker, for example, by allowing access to information outside the areas designated for Web use.\nWhen processing user-provided input, such as the contents of a form, URL parameters, or a search query, they may be vulnerable to attacks whereby the user tricks the application into executing arbitrary commands supplied in the input stream (e.g., cross-site scripting or SQL injection).\nThey may allow attackers to deface or modify site content."
          },
          {
            "text": "Ideally, server-side applications should constrain users to a small set of well-defined functionality and validate the size and values of input parameters so that an attacker cannot overrun memory boundaries or piggyback arbitrary commands for execution. Applications should be run only with minimal privileges (i.e., non-administrator) to avoid compromising the entire Web site. However, potential security holes can be exploited, even when applications run with low privilege settings, so this option should only be used as a last resort. For example, a subverted script could have enough privileges to mail out the system password file, examine the network information maps, or launch a login to a high numbered port."
          },
          {
            "text": "The areas of vulnerability mentioned potentially affect all Web servers. Although these vulnerabilities have frequently occurred with CGI applications, other related interfaces and techniques for developing server applications have not been immune. CGI, being an early and well-supported standard, has simply gained more attention over the years, and the same areas of vulnerability exist when applying similar Web development technologies.# CGI Scripts"
          },
          {
            "text": "CGI scripts were the initial mechanism used to make Web sites interact with databases and other applications. However, as the Web evolved, server-side processing methods have been developed that are more efficient and easier to program; for example, Microsoft provides ASP.NET for its IIS servers, Sun/Netscape supports Java servlets, and the freeware PHP is supported by most major Web platforms, including Apache and IIS [NIST01]."
          }
        ]
      }
    ]
  },
  {
    "title": "Server Side Includes (SSI)",
    "subsections": [
      {
        "content": "Server Side Includes (SSI) is a limited server-side scripting language supported by most Web servers. SSI provides a set of dynamic features, including the current time or the last modification date of the HTML file, as an alternative to using a CGI program to perform the function. When the browser requests a document with a special file type, such as \".shtml\", it triggers the server to treat the document as a template, reading and parsing the entire document before sending the results back to the client (Web browser)."
      },
      {
        "content": "SSI commands are embedded within HTML comments (e.g., ). As the server reads the template file, it searches for HTML comments containing embedded SSI commands. When it finds one, the server replaces that part of the original HTML text with the output of the command. For example, the SSI command given above (i.e.,#include file`) replaces the entire SSI comment with the contents of another HTML file. This allows the display of a corporate logo or other static information prepared in another file to occur in a uniform way across all corporate Web pages. A subset of the directives available allows the server to execute arbitrary system commands and CGI scripts, which may produce unwanted side effects [NIST01]."
      }
    ]
  },
  {
    "title": "Microsoft ASP.NET",
    "subsections": [
      {
        "content": "Microsoft ASP.NET is a server-side scripting technology from Microsoft that can be used to create dynamic and interactive Web applications. An ASP page contains server-side scripts that run when a browser requests an \".asp\" resource from the Web server. The Web server processes the requested page and executes any script commands encountered before sending a generated HTML page to the user's browser."
      },
      {
        "content": "Both C# and VBScript are natively supported as ASP.NET scripting languages, but other languages can be accommodated if an ASP.NET-compliant interpreter for the language is installed. For example, ASP.NET engines are available for the Perl, REXX, and Python languages from various sources."
      }
    ]
  },
  {
    "title": "Java EE",
    "subsections": [
      {
        "content": "Java EE is based on Java technology (see Section 6.4.1) and provides a type of server-side applet called a servlet. The Web server first determines whether the browser's request requires dynamically generated information from a servlet, which processes the request and generates an HTTP response, a Java Server Page (JSP), or a static HTML page. If a servlet is required, the Web server can then locate or instantiate a servlet object corresponding to the request and invoke it to obtain the needed results. If a JSP is requested, Java EE compiles the JSP into a servlet, then instantiates and invokes it to obtain a response. If a static HTML page is requested, Java EE simply returns the HTML content like a traditional Web server."
      },
      {
        "content": "The Java EE server typically populates itself with the servlet objects, which remain inactive until invoked. Thus, little or no startup overhead is associated with execution of the servlet objects. A Web server may also offload the handling of servlets to another server. By relying on Java portability and observing a common API, servlet objects can run in nearly any server environment. Servlets allow developers to take advantage of an object-oriented environment on the Web server, which is flexible and extendible. Moreover, untrusted servlet objects can be executed in a secure area, with the dynamically generated information being passed from the secure area into the remaining server environment [NIST01]."
      }
    ]
  },
  {
    "title": "PHP",
    "subsections": [
      {
        "content": "PHP is a scripting language used to create dynamic Web pages. With syntax from C, Java, and Perl, PHP code is embedded within HTML pages for server-side execution. PHP is commonly used to extract data from a database and present it on a Web page. Most major Windows and Unix Web servers support the language, and it is widely used with the mySQL database [NIST01]."
      },
      {
        "title": "Important Points to Consider When Contemplating the Deployment of PHP:",
        "content": [
          {
            "text": "The latest version of PHP available should be used because older versions have numerous security vulnerabilities.# PHP Security Considerations\n\n\nPHP provides a number of options that simplify development; some of these options (e.g., the register_globals option, which converts all input parameters into PHP variables and may override values in the PHP script) can make it more difficult for novices to develop secure programs.\n\nMuch of the freely available third-party code for PHP is poorly written from a security perspective."
          },
          {
            "text": "The latest version of PHP available should be used because older versions have numerous security vulnerabilities.# PHP Security Considerations"
          },
          {
            "text": "PHP provides a number of options that simplify development; some of these options (e.g., the register_globals option, which converts all input parameters into PHP variables and may override values in the PHP script) can make it more difficult for novices to develop secure programs."
          }
        ]
      },
      {
        "title": "6.4.3 Server-Side Content Generator Security Considerations",
        "content": [
          {
            "text": "When examining or writing an active content executable or script, consider the following:"
          },
          {
            "text": "The executable code should be as simple as possible. The longer or more complex it is, the more likely it will have problems.\nThe executable code's ability to read and write programs should be limited. Code that reads files may inadvertently violate access restrictions or pass sensitive system information. Code that writes files may modify or damage documents or introduce Trojan horses.\nThe code's interaction with other programs or applications should be analyzed to identify security vulnerabilities. For example, many CGI scripts send e-mails in response to form input by opening up a connection with the sendmail program. Ensure this interaction is performed in a secure manner.\nOn Linux/Unix hosts, the code should not run with suid (set-user-id).\nThe code should use explicit path names when invoking external programs. Relying on the PATH environment variable to resolve partial path names is not recommended.\nWeb servers should be scanned periodically for vulnerabilities, even if they do not employ active content (see Section 9.4.1). Network security scanners may detect vulnerabilities in the Web server, OS, or other services running on the Web server. Web application vulnerability scanners specifically scan for content generator vulnerabilities (see Appendix C for more information).\nWeb content generation code should be scanned and/or audited (depending on the sensitivity of the Web server and its content). Commercially available tools can scan .NET or Java code. A number of commercial entities offer code review services.\nWeb content generation code should be developed following current recommended practices.\nFor data entry forms, determine a list of expected characters and filter out unexpected characters from input data entered by a user before processing a form. For example, on most forms, expected data falls in these categories: letters a-z, A-Z, and 0-9. Care should be taken when accepting special characters such as &, ', \", @, and !. These symbols may have special meanings within the content generation language or other components of the Web application.\nEnsure that the dynamically generated pages do not contain dangerous metacharacters. It is possible for a malicious user to place these tags in a database or a file. When a dynamic page is generated using the altered data, the malicious code embedded in the tags may be passed to the client browser. Then the user's browser can be tricked into running a program of the attacker's choice. This program will execute in the browser's security context for communicating with the legitimate Web server, not the browser's security context for communicating with the attacker. Thus, the# Security Considerations for Web Applications"
          }
        ]
      },
      {
        "title": "Inappropriate Security Context",
        "content": [
          {
            "text": "The program will execute in an inappropriate security context with inappropriate privileges."
          }
        ]
      },
      {
        "title": "Character Set Encoding",
        "content": [
          {
            "text": "Character set encoding should be explicitly set in each page. Then the user data should be scanned for byte sequences that represent special characters for the given encoding scheme.\nEach character in a specified character set can be encoded using its numeric value. Encoding the output can be used as an alternate for filtering the data. Encoding becomes especially important when special characters, such as copyright symbols, can be part of the dynamic data. However, encoding data can be resource intensive, and a balance must be struck between encoding and other methods for filtering the data."
          }
        ]
      },
      {
        "title": "Cookies Examination",
        "content": [
          {
            "text": "Cookies should be examined for any special characters. Any special characters should be filtered out."
          }
        ]
      },
      {
        "title": "Encryption Mechanism",
        "content": [
          {
            "text": "An encryption mechanism should be used to encrypt passwords entered through script forms (see Section 7.5)."
          }
        ]
      },
      {
        "title": "Access Control",
        "content": [
          {
            "text": "For Web applications that are restricted by username and password, none of the Web pages in the application should be accessible without executing the appropriate login process."
          }
        ]
      },
      {
        "title": "Sample Scripts and Vulnerabilities",
        "content": [
          {
            "text": "Many Web servers and some other Web server software install sample scripts or executables during the installation process. Many of these have known vulnerabilities and should be removed immediately. See appropriate manufacturer's documentation or Web sites for more information."
          }
        ]
      },
      {
        "title": "Server-Side Content Generators",
        "content": [
          {
            "text": "When considering a server-side content generator, it is important to review public vulnerability and security databases (such as NVD, http://nvd.nist.gov/) to determine the relative risk of the various technologies under consideration. Although the historical record will not be a perfect indicator of future risk, it does indicate which technologies appear to be more vulnerable."
          },
          {
            "text": "Various organizations research network and system security topics and periodically publish information concerning recently discovered vulnerabilities in software. This includes Web server software and supporting technologies, such as scripting languages and external programs. External programs that are in wide use are regularly analyzed by researchers, users, and security incident response teams and by attackers. Attackers will often publish exploit scripts that take advantage of known vulnerabilities in Web service software and external programs commonly used by public Web servers. Web administrators should review public information sources frequently and be aware of all security-relevant information about any external programs that they are considering."
          }
        ]
      },
      {
        "title": "Location of Server-Side Content Generators",
        "content": [
          {
            "text": "The location of active content on the Web server is critical. If located in an incorrect directory or in a directory with the wrong permissions, it can quickly lead to the compromise of the Web server. To avoid this problem:\n- Writable files should be identified and placed in separate folders. No script files should exist in writable folders. As an example, guest book data is usually saved in simple text files. These files need write permissions for guests to be able to submit their comments.\n- Executable files (e.g., CGI, .EXE, .CMD, and PL) should be placed in separate folders.# CURRENT_PAGE_RAW_OCR_TEXT"
          },
          {
            "text": "folders. No other readable or writable documents should be placed in these folders."
          },
          {
            "text": "Script files (e.g., ASP, PHP, and PL) should have separate folders. It may also be beneficial to store the scripts in a folder with a non-obvious name (e.g., not \"Scripts\") to make it more difficult for an attacker to find the scripts through direct browsing.\nInclude files (e.g., INC, SHTML, SHTM, and ASP) created for code reusability should be placed in separate directories. SSI should not generally be used on public Web servers. ASP include files should have an .asp extension instead of .inc. Note that much of the risk with include files is in their execute capability. If the execute capability is disabled, this risk is drastically reduced."
          }
        ]
      },
      {
        "title": "6.4.5 Cross-Site Scripting Vulnerabilities",
        "content": [
          {
            "text": "Cross-site scripting (XSS) is a vulnerability typically found in interactive Web applications that allows code injection by malicious Web users into the Web pages viewed by other users. It generally occurs in Web pages that do not do the appropriate bounds checking on data input by users. An exploited cross-site scripting vulnerability can be used by attackers to compromise other users' computers or to receive data from another user's Web session (e.g., user ID and password or session cookie). Thus, although this is a client side exploit, it also impacts the server indirectly since a compromised user, particularly one with elevated privileges, represents a threat to the server. XSS vulnerabilities are used frequently to conduct phishing attacks or exploit Web browser vulnerabilities to gain control of end user PCs."
          },
          {
            "text": "XSS vulnerabilities are highly varied and frequently unique to a particular Web application. They encompass two general categories:"
          },
          {
            "text": "Persistent XSS vulnerabilities allow for the more powerful attacks. This vulnerability occurs when data provided to a Web application by an untrusted user is stored persistently on the server and displayed to other users as Web content but is not validated or encoded using HTML. A common example of this is online message boards, wikis, and blogs where users are allowed to post HTML-formatted messages for other users to view. In this example, after a malicious user posts a malicious message or reply, any other user who accesses a page displaying that data and whose browser is vulnerable to the exploit can be compromised.\n\n\nNon-persistent XSS vulnerabilities, sometimes called reflected, are more common and somewhat less dangerous than persistent vulnerabilities. Non-persistent vulnerabilities occur when data provided by a Web client is used immediately by server-side scripts to generate a results page for that user (e.g., login screen, search results page). If the unvalidated client-supplied data is included in the returned page without any HTML encoding, this will allow client-side code to be injected into the dynamic page. This might not appear to be a problem on the surface, since an attacker can only exploit himself or herself. However, an attacker could send a specially crafted URL to a user and trick the user through social engineering into clicking on the maliciously crafted URL. If the user's Web browser was vulnerable to the exploit, the user's machine could be compromised.# Compromised"
          },
          {
            "text": "Persistent XSS vulnerabilities allow for the more powerful attacks. This vulnerability occurs when data provided to a Web application by an untrusted user is stored persistently on the server and displayed to other users as Web content but is not validated or encoded using HTML. A common example of this is online message boards, wikis, and blogs where users are allowed to post HTML-formatted messages for other users to view. In this example, after a malicious user posts a malicious message or reply, any other user who accesses a page displaying that data and whose browser is vulnerable to the exploit can be compromised."
          },
          {
            "text": "Non-persistent XSS vulnerabilities, sometimes called reflected, are more common and somewhat less dangerous than persistent vulnerabilities. Non-persistent vulnerabilities occur when data provided by a Web client is used immediately by server-side scripts to generate a results page for that user (e.g., login screen, search results page). If the unvalidated client-supplied data is included in the returned page without any HTML encoding, this will allow client-side code to be injected into the dynamic page. This might not appear to be a problem on the surface, since an attacker can only exploit himself or herself. However, an attacker could send a specially crafted URL to a user and trick the user through social engineering into clicking on the maliciously crafted URL. If the user's Web browser was vulnerable to the exploit, the user's machine could be compromised.# Compromised"
          },
          {
            "text": "Since this attack does require some level of social engineering, it is considered somewhat less dangerous than attacks on persistent vulnerabilities."
          },
          {
            "text": "The solution to XSS attacks is to validate all user input and remove any unexpected or potentially risky data. Another solution is to use an HTML-quoted version of any user input that is presented back to other users. This will prevent the Web browsers of other users from interpreting that input and acting on any embedded commands present."
          }
        ]
      },
      {
        "title": "7. Using Authentication and Encryption Technologies",
        "content": [
          {
            "text": "Public Web servers often support a range of technologies for identifying and authenticating users with differing privileges for accessing information. Some of these technologies are based on cryptographic functions that can provide an encrypted channel between a Web browser client and a Web server that supports encryption."
          },
          {
            "text": "Without user authentication, organizations will not be able to restrict access to specific information to authorized users. All information that resides on a public Web server will then be accessible by anyone with access to the server. In addition, without some process to authenticate the server, users will not be able to determine if the server is the \"authentic\" Web server or a counterfeit version operated by a malicious entity."
          },
          {
            "text": "Encryption can be used to protect information traversing the connection between a Web browser client and a public Web server. Without encryption, anyone with access to the network traffic can determine, and possibly alter, the content of sensitive information, even if the user accessing the information has been authenticated carefully. This may violate the confidentiality and integrity of critical information."
          },
          {
            "subsection": "7.1 Determining Authentication and Encryption Requirements",
            "text": []
          },
          {
            "text": "Organizations should periodically examine all information accessible on the public Web server and determine the necessary security requirements. While doing so, the organization should identify information that shares the same security and protection requirements. For sensitive information, the organization should determine the users or user groups that should have access to each set of resources."
          },
          {
            "text": "For information that requires some level of user authentication, the organization should determine which of the following technologies or methods would provide the appropriate level of authentication and encryption. Each has its own unique benefits and costs that should be weighed carefully with client and organizational requirements and policies. It may be desirable to use some authentication methods in combination."
          },
          {
            "text": "This guide discusses the authentication mechanisms most commonly associated with public Web servers and Web applications. More advanced authentication mechanisms can be supported by these servers and applications and are discussed in NIST SP 800-63."
          },
          {
            "subsection": "7.2 Address-Based Authentication",
            "text": []
          },
          {
            "text": "The simplest authentication mechanism that is supported by most Web servers is address-based authentication. Access control is based on the IP address and/or hostname of the host requesting.# CURRENT_PAGE_RAW_OCR_TEXT"
          }
        ]
      },
      {
        "title": "Information",
        "content": [
          {
            "text": "Although it is easy to implement for small groups of users, address authentication can be unwieldy for Web sites that have a large potential user population (i.e., most public Web servers). It is susceptible to several types of attacks, including IP spoofing and DNS poisoning. This type of authentication should be used only where minimal security is required, unless it is used in conjunction with stronger authentication methods."
          }
        ]
      },
      {
        "title": "7.3 Basic Authentication",
        "content": [
          {
            "text": "The basic authentication technology uses the Web server content's directory structure. Typically, all files in the same directory are configured with the same access privileges. A requesting user provides a recognized user identification and password for access to files in a given directory. More restrictive access control can be enforced at the level of a single file within a directory if the Web server software provides this capability. Each vendor's Web server software has its own method and syntax for defining and using this basic authentication mechanism."
          },
          {
            "text": "From a security perspective, the main drawback of this technology is that all password information is transferred in an encoded, rather than an encrypted, form. Anyone who knows the standardized encoding scheme can decode the password after capturing it with a network sniffer. Furthermore, any Web content is transmitted as unencrypted plaintext, so this content also can be captured, violating confidentiality. These limitations can be overcome using basic authentication in conjunction with SSL/TLS (see Section 7.5). Basic authentication is supported by standard-compliant Web browsers [Koss00]. Basic authentication is useful for protecting information from malicious bots (see Section 5.2.4) because the bots should not have the necessary credentials to access the protected directories. However, this mechanism should not be considered secure against more determined and sophisticated attackers."
          }
        ]
      },
      {
        "title": "7.4 Digest Authentication",
        "content": [
          {
            "text": "Because of the drawbacks with basic authentication, an improved technique known as digest authentication was introduced in version 1.1 of the HTTP protocol. Digest authentication uses a challenge-response mechanism for user authentication. Under this approach, a nonce or arbitrary value is sent to the user, who is prompted for an ID and password, as with basic authentication. However, in this case, the information entered by the user is concatenated and a cryptographic hash of the result is formed. This hash is concatenated with the nonce and a hash of the requested method and URL, and the result is then rehashed as a response value that is sent to the server."
          },
          {
            "text": "Because the user's password is not sent in the clear, it cannot be directly sniffed from the network. The user's password is not needed by the server to authenticate the user\u2014only the hashed value of the user ID and password. Because the nonce can serve as an indicator of timeliness (e.g., it can be composed of date and time information), replay attacks are also thwarted. Unfortunately, all other information is sent in the clear and is vulnerable to interception and alteration. Digest authentication is also susceptible to offline dictionary attacks (see Section 7.6) where the attacker tries various passwords.# CURRENT_PAGE_RAW_OCR_TEXT"
          }
        ]
      },
      {
        "title": "In an attempt to recreate",
        "content": [
          {
            "text": "the captured digest value. These limitations can be overcome using digest authentication in conjunction with SSL/TLS (see Section 7.5). Like basic authentication, digest authentication is useful for protecting information from malicious bots (see Section 5.2.4)."
          }
        ]
      },
      {
        "title": "7.5 SSL/TLS",
        "content": [
          {
            "text": "The SSL and TLS protocols provide server and client authentication and encryption of communications. SSL was first introduced by Netscape Communications in 1994 and was revised twice (SSL version 3 is the current version). In 1996, the Internet Engineering Task Force (IETF) established the TLS working group to formalize and advance the SSL protocol to the level of Internet standard. The TLS protocol version 1.0 is formally specified in IETF Request for Comments (RFC) 2246, which was published in 1999 and is based in large part on SSL version 3. SSL version 3 and TLS version 1 are essentially identical and are discussed together in this document. Most major Internet components, such as Web browsers, support the use of both SSL 3 and TLS 1.0. TLS 1.1, specified in RFC 4436, was released in April 2006, and future versions of Web browsers will likely support it."
          },
          {
            "text": "TCP/IP governs the transport and routing of data over the Internet. Other protocols, such as HTTP, LDAP, and Internet Message Access Protocol (IMAP), run \"on top of\" TCP/IP in that they all use TCP/IP to support typical application tasks, such as displaying Web pages or delivering e-mail messages. Thus, SSL/TLS can support more than just secure Web communications. Figure 7-1 shows how SSL/TLS fits between the application and network/transport layers of the Internet protocol suite."
          },
          {
            "subsection": "7.5.1 SSL/TLS Capabilities",
            "text": []
          },
          {
            "text": "SSL/TLS provides the following capabilities to HTTP and other application layer protocols [SSL98]:"
          },
          {
            "text": "Server Authentication\u2014SSL/TLS allows a Web client (user) to confirm a Web server's identity. SSL/TLS-enabled Web clients (browsers) can employ standard techniques of public key cryptography to check that a server's name and public key are contained in a valid certificate issued by a CA listed in the client's list of trusted CAs. This confirmation might be important if the user, for example, is sending a credit card number over the network and wants to confirm the receiving server's identity.\n\n\nClient Authentication\u2014SSL/TLS allows a Web server to confirm a user's identity using the same techniques as those used for server authentication by reversing the roles. SSL/TLS-enabled Web server software can confirm that a client's certificate is valid and was issued by a CA listed in the server's list of trusted CAs. This confirmation might be important if the server, for example, is a bank that is sending confidential financial information to a customer and wants to confirm the recipient's identity. If client authentication is to be performed, server authentication must also be performed.\n\n\nCommunication Encryption\u2014SSL/TLS can encrypt most of the information being transmitted between a Web browser (client) and a Web server or even between two Web servers. With an# CURRENT_PAGE_RAW_OCR_TEXT"
          },
          {
            "text": "Server Authentication\u2014SSL/TLS allows a Web client (user) to confirm a Web server's identity. SSL/TLS-enabled Web clients (browsers) can employ standard techniques of public key cryptography to check that a server's name and public key are contained in a valid certificate issued by a CA listed in the client's list of trusted CAs. This confirmation might be important if the user, for example, is sending a credit card number over the network and wants to confirm the receiving server's identity."
          },
          {
            "text": "Client Authentication\u2014SSL/TLS allows a Web server to confirm a user's identity using the same techniques as those used for server authentication by reversing the roles. SSL/TLS-enabled Web server software can confirm that a client's certificate is valid and was issued by a CA listed in the server's list of trusted CAs. This confirmation might be important if the server, for example, is a bank that is sending confidential financial information to a customer and wants to confirm the recipient's identity. If client authentication is to be performed, server authentication must also be performed."
          },
          {
            "text": "Communication Encryption\u2014SSL/TLS can encrypt most of the information being transmitted between a Web browser (client) and a Web server or even between two Web servers. With an# CURRENT_PAGE_RAW_OCR_TEXT"
          }
        ]
      },
      {
        "title": "appropriate encryption algorithm, SSL/TLS provides a high degree of confidentiality.",
        "content": [
          {
            "text": "In addition, all data sent over an encrypted SSL/TLS connection is protected with a mechanism for detecting tampering; that is, for automatically determining if the data has been altered in transit."
          },
          {
            "subsection": "7.5.2 Weaknesses of SSL/TLS",
            "text": []
          },
          {
            "text": "Several limitations are inherent with SSL/TLS. Packets are encrypted at the TCP layer, so IP layer information is not encrypted. Although this protects the Web data being transmitted, a person monitoring an SSL/TLS session can determine both the sender and receiver via the unencrypted IP address information. In addition, SSL/TLS only protects data while it is being transmitted, not when it is stored at either endpoint. Thus, the data is still vulnerable while in storage (e.g., a credit card database) unless additional safeguards are taken at the endpoints."
          },
          {
            "text": "If SSL/TLS is implemented or used incorrectly, the communications intended to be protected may be vulnerable to a \"man in the middle\" attack. This occurs when a malicious entity intercepts all communication between the Web client and the Web server with which the client is attempting to establish an SSL/TLS connection. The attacker intercepts the legitimate keys that are passed back and forth during the SSL/TLS handshake (see Section 7.5.3) and substitutes the attacker's keys, making it appear to the Web client that the attacker is the Web server and to the Web server that the attacker is the Web client [SSL98]. If SSL/TLS is implemented and used properly, it is not susceptible to man-in-the-middle attacks."
          },
          {
            "text": "The encrypted information exchanged at the beginning of the SSL/TLS handshake is actually encrypted with the malicious entity's public key or private key, rather than the Web client's or Web server's real keys. The attacker program ends up establishing one set of session keys for use with the real Web server, and a different set of session keys for use with the Web client. This allows the attacker program not only to read all the data that flows between the Web client and the real Web server but also to change the data without being detected. This threat can be mitigated if clients rely upon server certificates issued by trusted CAs or on self-signed certificates obtained by secure mechanisms. Presentation of a self-signed certificate may be an indication that a man-in-the-middle attack is underway. Browsers may perform some checks automatically, but they cannot be relied upon in all instances."
          },
          {
            "text": "Even without performing a man-in-the-middle attack, attackers may attempt to trick users into accessing an invalid Web site. There are several possible methods for attack, including\u2014"
          },
          {
            "text": "Presenting a self-signed certificate unknown to the user and getting the user to accept it as legitimate. Although the Web browser can be configured to display a warning when a self-signed certificate is presented, organizations that rely on self-signed certificates might instruct users to ignore such warnings.\nExploiting vulnerabilities in a Web browser so that the Web site appears to be valid to an untrained user.\nTaking advantage of a cross-site scripting vulnerability on a legitimate Web site. The Web browser.# SSL/TLS Security Overview"
          },
          {
            "text": "will be accessing two different sites, but it will appear to the user that only the legitimate site is being accessed."
          },
          {
            "text": "Taking advantage of the certificate approval process to receive a valid certificate and apply it to the attacker's own site. By using a valid certificate on what appears to be a valid site, the certificate will validate, and the user would have to somehow realize that the site being accessed is malicious."
          },
          {
            "text": "SSL spoofing attacks can occur without requiring any user intervention. As a proof of concept in 2005, the Shmoo Group registered an internationalized domain name that looked similar to a valid site when displayed in a browser. The SSL certificate matched the internationalized domain name of the Web site, so no user warning occurred. In the address bar, the URL appeared almost identical to that of the original spoofed Web site [NVD06]."
          },
          {
            "text": "Users can prevent less sophisticated attacks by confirming the validity of a certificate before relying on the security of an SSL/TLS session and rejecting certificates for which the browser presents warnings. Browsers perform some checks automatically, but they cannot be relied upon in all instances."
          }
        ]
      },
      {
        "title": "7.5.3 Example SSL/TLS Session",
        "content": [
          {
            "text": "The SSL/TLS protocols use a combination of public key and symmetric key encryption. Symmetric key encryption is much faster than public key encryption, whereas public key encryption is better suited to provide authentication and establish symmetric keys. An SSL/TLS session always begins with an exchange of messages called the SSL/TLS handshake. The handshake allows the server to authenticate itself to the client using public key techniques; this allows the client and the server to cooperate in the creation of symmetric keys used for rapid encryption, decryption, and tamper detection during the session that follows. The handshake also allows the client to authenticate itself to the server."
          },
          {
            "text": "The exact programmatic details of the messages exchanged during the SSL/TLS handshake are beyond the scope of this document. However, the basic steps involved can be summarized as follows [SSL98]:"
          },
          {
            "text": "\"The client sends the server the client's SSL/TLS version number, cipher settings, randomly generated data, and other information the server needs to communicate with the client using SSL/TLS.\"\n\"The server sends the client the server's SSL/TLS version number, cipher settings, randomly generated data, and other information the client needs to communicate with the server over SSL/TLS. The server also sends its own certificate and, if the client is requesting a server resource that requires client authentication, requests the client's certificate.\"\n\"The client uses some of the information sent by the server to authenticate the server. If the server cannot be authenticated, the user is warned of the problem and informed that an encrypted and authenticated connection cannot be established. If the server can be successfully authenticated, the client goes on to Step 4.\"\n\n\"Using all data generated in the handshake to this point, the client (with...# SSL/TLS Handshake Process\n\n\nThe cooperation of the server, depending on the cipher being used, creates the premaster secret for the session, encrypts it with the server's public key (obtained from the server's certificate, sent in Step 2), and sends the encrypted premaster secret to the server.\n\n\nIf the server has requested client authentication (an optional step in the handshake), the client also signs another piece of data that is unique to this handshake and known by both the client and server. In this case, the client sends both the signed data and the client's own certificate to the server, along with the encrypted premaster secret.\n\n\nIf the server has requested client authentication, the server attempts to authenticate the client. If the client cannot be authenticated, the session is terminated. If the client can be successfully authenticated, the server uses its private key to decrypt the premaster secret, then performs a series of steps (which the client also performs, starting from the same premaster secret) to generate the master secret.\n\n\nBoth the client and the server use the master secret to generate the session keys, which are symmetric keys used to encrypt and decrypt information exchanged during the SSL/TLS session and to verify its integrity\u2014that is, to detect any changes in the data between the time it was sent and the time it is received over the SSL/TLS connection."
          },
          {
            "text": "Both the client and the server use the master secret to generate the session keys, which are symmetric keys used to encrypt and decrypt information exchanged during the SSL/TLS session and to verify its integrity\u2014that is, to detect any changes in the data between the time it was sent and the time it is received over the SSL/TLS connection.\n\n\nThe client sends a message to the server informing it that future messages from the client will be encrypted with the session key. It then sends a separate (encrypted) message indicating that the client portion of the handshake is finished.\n\n\nThe server sends a message to the client informing it that future messages from the server will be encrypted with the session key. It then sends a separate (encrypted) message indicating that the server portion of the handshake is finished.\n\n\nThe SSL/TLS handshake is now complete, and the SSL/TLS session has begun. The client and the server use the session keys to encrypt and decrypt the data they send to each other and to validate its integrity."
          },
          {
            "text": "\"Using all data generated in the handshake to this point, the client (with...# SSL/TLS Handshake Process"
          },
          {
            "text": "The cooperation of the server, depending on the cipher being used, creates the premaster secret for the session, encrypts it with the server's public key (obtained from the server's certificate, sent in Step 2), and sends the encrypted premaster secret to the server."
          },
          {
            "text": "If the server has requested client authentication (an optional step in the handshake), the client also signs another piece of data that is unique to this handshake and known by both the client and server. In this case, the client sends both the signed data and the client's own certificate to the server, along with the encrypted premaster secret."
          },
          {
            "text": "If the server has requested client authentication, the server attempts to authenticate the client. If the client cannot be authenticated, the session is terminated. If the client can be successfully authenticated, the server uses its private key to decrypt the premaster secret, then performs a series of steps (which the client also performs, starting from the same premaster secret) to generate the master secret."
          },
          {
            "text": "Both the client and the server use the master secret to generate the session keys, which are symmetric keys used to encrypt and decrypt information exchanged during the SSL/TLS session and to verify its integrity\u2014that is, to detect any changes in the data between the time it was sent and the time it is received over the SSL/TLS connection."
          },
          {
            "text": "The client sends a message to the server informing it that future messages from the client will be encrypted with the session key. It then sends a separate (encrypted) message indicating that the client portion of the handshake is finished."
          },
          {
            "text": "The server sends a message to the client informing it that future messages from the server will be encrypted with the session key. It then sends a separate (encrypted) message indicating that the server portion of the handshake is finished."
          },
          {
            "text": "The SSL/TLS handshake is now complete, and the SSL/TLS session has begun. The client and the server use the session keys to encrypt and decrypt the data they send to each other and to validate its integrity."
          }
        ]
      },
      {
        "title": "7.5.4 SSL/TLS Encryption Schemes",
        "content": [
          {
            "text": "The SSL/TLS protocols support the use of a variety of different cryptographic algorithms for operations such as authenticating the Web server and Web client to each other, transmitting certificates, and establishing session keys. Web clients and Web servers may support different cipher suites, or sets of ciphers, depending on factors such as the versions of SSL/TLS they support; organizational policies regarding acceptable encryption strength; and government restrictions on export, import, and use of SSL/TLS-enabled software. Among other functions, the SSL/TLS handshake protocols determine how the Web server and Web client negotiate which cipher suites they will use to authenticate each other, transmit certificates, and establish session keys. Table 7-1 provides a list of Federal cipher suites, their recommended usage, and their relative strength [SSL98 and Chow02]."
          },
          {
            "text": "The server certificate used in the SSL/TLS handshake (described in more detail in Section 7.5.5) specifies.# The Algorithm"
          },
          {
            "text": "The certificate's public key supports, the public key, and the key size. The certificate may also describe its intended use (e.g., for generating digital signatures, encryption, or authentication)."
          },
          {
            "text": "During the handshake phase, the client indicates the cipher suites and key lengths it supports. Ultimately, the choice of cipher suite and key length is dictated by the server. Choosing an appropriate encryption algorithm depends on several factors that vary for each organization."
          },
          {
            "text": "Although at first glance it might appear that the strongest encryption available should always be used, that is not always true. The higher the level of the encryption, the greater impact it will have on the Web server's resources and communications speed. Furthermore, a number of countries still maintain restrictions on the export, import, and/or use of encryption. Patents and licensing issues may affect which commercial encryption schemes can be used. Common factors that influence the choice of encryption algorithm are as follows:"
          },
          {
            "text": "Required security\nValue of the data (to either the organization and/or other entities\u2014the more valuable the data, the stronger the required encryption)\nTime value of data (if data is valuable but for only a short time period [e.g., days as opposed to years], then a weaker encryption algorithm could be used)\nThreat to data (the higher the threat level, the stronger the required encryption)\nOther protective measures that are in place and that may reduce the need for stronger encryption\u2014for example, using protected methods of communications, such as dedicated circuits as opposed to the public Internet\nRequired performance (higher performance requirements may require procurement of additional system resources, such as a hardware cryptographic accelerator, or may necessitate weaker encryption)\nSystem resources (fewer resources [e.g., process, memory] may necessitate weaker encryption)\nImport, export, or usage restrictions\nEncryption schemes supported by Web server application\nEncryption schemes supported by Web browsers of expected users."
          }
        ]
      }
    ]
  },
  {
    "title": "Types of CSRs",
    "subsections": [
      {
        "content": "The most popular is the encoded Public Key Cryptography Standard (PKCS) #10, Certification Request Syntax Standard, which is used by newer Web servers [RSA00]. The other CSR type, based on the Privacy Enhanced Mail (PEM) specification, is called either PEM message header or Web site professional format. The use of this CSR is generally limited to older Web servers. Most Web servers generate PKCS #10-compliant CSRs similar to the sample CSR shown in Figure 7-2. A CSR provides not only additional information about a given entity, or a \"challenge password\" by which the entity may later request certificate revocation, but also attributes for inclusion in X.509 certificates [RSA00]."
      },
      {
        "title": "CSR Generation Process",
        "content": [
          {
            "text": "Spelling and punctuation should be checked when information is provided during the CSR generation process. The URL that is supplied must exactly match the URL for which the certificate is used. SSL/TLS clients are configured to generate an error if the URLs do not match. In some instances, a user may acknowledge this error in an alert box and proceed."
          },
          {
            "text": "Once the CSR has been generated, the organization submits it to a CA. The CA's role is to fulfill the CSR by authenticating the requesting entity and verifying the entity's signature. If the request is valid, the CA constructs an X.509 certificate from the domain name (DN) and public key; the issuer name (more commonly referred to as the common name [CN]); and the CA's choice of serial number, validity period, or signature algorithm. Upon receiving a submitted CSR, the CA must verify the CSR and create a signed X.509 certificate. At this point, most CAs will then alert the applicant by telephone, e-mail, etc., that the X.509 certificate is available. Once notified, applicants will be able to download their certificates through an SSL/TLS-protected Web-based interface."
          }
        ]
      },
      {
        "title": "X.509 Certificate",
        "content": [
          {
            "text": "Figure 7-3 shows an X.509 certificate encoded in PEM format. Similar to the CSR, when supplying a certificate to a configuration wizard or even saving it to a hard drive, the lines \"BEGIN CERTIFICATE\" and \"END CERTIFICATE\" are vital. Without them, the Web server application will be unable to interpret the encoded contents of the certificate."
          }
        ]
      },
      {
        "title": "Certificate Validation",
        "content": [
          {
            "text": "When a Web browser contacts a Web site that is using SSL/TLS, the browser examines the certificate and attempts to verify its validity. Each browser has a store of certificates, and these typically include root certificates for many third-party CAs. If the browser has a root certificate for the third-party CA that issued the Web server's certificate, it can use the CA's root certificate to validate the Web server's certificate, which is based partially on that root certificate. If the browser does not have the needed root certificate, then it typically displays a warning to the user that says the Web server's certificate could not be verified and asks the user how it should proceed."
          }
        ]
      },
      {
        "title": "Self-Signed Certificates",
        "content": [
          {
            "text": "Some organizations decide to create and sign their own Web server certificates instead of having certificates issued by third-party CAs. The main advantage of self-signed certificates is that it avoids the cost of purchasing and renewing certificates; however, by default, Web browsers will not be able to validate self-signed certificates, so they provide no assurance of the\n```# Legitimacy of the Certificate or the Web Server"
          },
          {
            "text": "Given the increasing use of phishing and other techniques to trick users into visiting rogue Web sites, failing to authenticate the server's identity before using SSL/TLS with it is unacceptable. If a particular Web server is only going to be accessed from the organization's own systems (e.g., telecommuters using laptops issued and managed by the organization), then those systems' browsers could be provisioned with the root certificates needed to validate the Web server's self-signed certificate and authenticate the server's identity. Otherwise, the use of self-signed certificates for public Web servers is generally not recommended because of the logistics involved in securely distributing the root certificates to users' Web browsers and having the certificates installed correctly by the users so that the browsers can authenticate the organization's Web servers."
          }
        ]
      },
      {
        "title": "Recommendations for SSL/TLS Certificates",
        "content": [
          {
            "text": "For all SSL/TLS certificates for Web servers, regardless of issuer or format, administrators should take extreme caution in securing their certificate and encryption keys. The following are recommendations:"
          },
          {
            "text": "Create and store a backup copy of the certificate on read-only media in case the original certificate is deleted accidentally. If the certificate is lost and cannot be recovered from backup media, a new certificate must be created.\nCreate and store a backup copy of the encryption keys on read-only media in case the keys are deleted accidentally. If the keys are lost and cannot be recovered from backup media, a new key pair and certificate must be created. Note that the backup copy of the keys must be physically secured and should be encrypted as well.\nStore the original certificate in a folder or partition accessible by only Web or system administrators and secured by appropriate authentication mechanisms.\nConsider running a file integrity checker in the Web server (see Section 8.2.2) and ensure that it is monitoring for any changes to the certificate.\nExamine system logs regularly to validate and ensure prevention of unauthorized system access."
          },
          {
            "text": "If a malicious user gains unauthorized access to a Web server, the integrity of the entire server is lost immediately once the encryption key pair is modified. Once a key in an SSL/TLS certificate is compromised, it can remain compromised; for example, some CAs do not issue revocation information, and many client implementations do not obtain or process revocation information."
          }
        ]
      },
      {
        "title": "Installing and Configuring SSL",
        "content": [
          {
            "text": "Once a certificate is ready, it needs to be installed, and SSL needs to be enabled and configured. Some steps are common to all Web servers:"
          },
          {
            "text": "Disable SSL 1.0 and SSL 2.0.\nConfigure SSL/TLS to restrict cryptographic algorithms to the selected cipher suite(s) (see Section 7.5.4).\nIndicate the location of the SSL/TLS certificate and instruct the server to start using SSL/TLS. In certain cases, the Web server must be instructed to begin using SSL/TLS, and even given the location of the SSL/TLS certificate and private keys if they were stored as files on the hard drive.\nInstruct server to listen on TCP port 443. This is the default TCP port from which SSL/TLS operates.# SSL/TLS Configuration and Implementations"
          }
        ]
      },
      {
        "title": "Accessing Resources",
        "content": [
          {
            "text": "Resources are accessed by clients (other ports can be used). In most cases, if the server was not previously using SSL/TLS, this port would be disabled for security reasons. It will probably be necessary to configure the network infrastructure supporting the Web server to allow SSL/TLS traffic (see Section 8.2). All ports other than TCP 443 should be closed and the network infrastructure (e.g., firewalls) should be updated to block attempts to connect to all other ports. However, if the Web server is to host both HTTP and HTTPS content, TCP 80 should be open as well."
          },
          {
            "text": "Configure the server to protect the necessary resources (directories and/or files) using SSL/TLS. Configure the Web server application so that the appropriate resources are protected with SSL/TLS. These resources are then accessible only from a URL that starts with \"https://\". Newer versions of the HTML standard have been amended to include a response to inform clients when a file they have requested is available only via SSL/TLS. The HTTP status code 403.4 indicates that a HTTP GET request must be prefixed with an https:// because the resource requested is protected with SSL/TLS. For more information, consult RFCs 2246, 2626, and 2817. Most current Web browsers also provide users with some user-friendly visual indication of a server's SSL/TLS certificate status, such as changing the color of a status bar."
          }
        ]
      },
      {
        "title": "SSL/TLS Implementations",
        "content": [
          {
            "text": "Although some Web servers come packaged with SSL capabilities already integrated, many do not. This section discusses various commercial and open-source SSL/TLS implementations. Some of these packages contain the functionality to generate SSL certificates without the need of a CA. The following list includes some of the SSL toolkits available:"
          },
          {
            "text": "OpenSSL is an open-source implementation of SSL/TLS for Unix and Linux platforms (http://www.openssl.org/).\nNetwork Security Services (NSS) is an open-source implementation of SSL/TLS developed by the Mozilla foundation. NSS is derived from the original Netscape SSL implementation.\nGnuTLS is an open-source implementation of SSL/TLS developed by the Free Software Foundation.\nJava Secure Socket Extension (JSSE) is an implementation of SSL/TLS developed by Sun for distribution as part of the Java Runtime Environment (JRE).\nSecurity Support Provider Interface (SSPI) is an implementation of SSL/TLS available on Microsoft Windows Server 2003.\nIBM Java Secure Sockets Extension (IBMJSSE) is an implementation of SSL/TLS for the WebSphere Application Server."
          },
          {
            "text": "Federal government organizations are required to use Federal Information Processing Standards (FIPS)-validated SSL/TLS implementations when protecting data using SSL/TLS. The Cryptographic Module Validation Program (CMVP) performs validation testing of cryptographic modules, including SSL/TLS implementations. NIST provides a list of FIPS 140-2-compliant vendors and implementations. Regardless of what SSL/TLS implementation is used, it is important to ensure that security patches are applied.# Security Flaws in SSL/TLS Implementations"
          },
          {
            "text": "Security flaws in SSL/TLS implementations can potentially allow attackers to spoof PKI certificates, forge digital signatures, perform DoS attacks, or execute arbitrary code in the Web server."
          }
        ]
      },
      {
        "title": "7.6 Brute Force Attacks",
        "content": [
          {
            "text": "Many Web sites authenticate users via username and password combinations\u2014whether through HTTP Basic, HTTP Digest, or a Web form over SSL. Regardless of implementation, username and password combinations are susceptible to brute force attacks. Brute force attacks can occur in multiple forms:"
          },
          {
            "text": "Username Harvesting\u2014Applications that differentiate between an invalid password and an invalid username can allow attackers to construct a list of valid user accounts.\nDictionary Attacks\u2014Attackers use common dictionary words and their variants to attempt to gain access to a user's account.\nBrute Force Attacks\u2014Attackers try every possible password to attempt to gain access to a user's account."
          },
          {
            "text": "There are a number of methods for reducing a Web server's vulnerability to brute force attack:"
          },
          {
            "text": "Use Strong Authentication\u2014Strong authentication techniques, such as hardware tokens, one-time passwords, biometric authentication, and SSL/TLS client certificates, are much more resistant to brute force attacks than passwords. Stronger authentication can be achieved by combining multiple authentication mechanisms to form a multi-factor authentication scheme. However, strong authentication may be prohibitively expensive or difficult to incorporate into a system.\n\n\nUse Timeouts\u2014Incurring a delay of several seconds after a failed login attempt can slow an attacker down. However, attackers can attempt multiple logins at the same time from different clients.\n\n\nUse Lockouts\u2014Locking out a user account after a number of failed login attempts prevents the attacker from successfully logging into an account. The primary disadvantage of this technique is that it can leave the system open to a DoS attack. Also, an attacker may try several common passwords against random usernames, which may grant the attacker access to the system while bypassing the lockout [Whit06].\n\n\nEnforce a Password Policy\u2014By requiring passwords to be of a certain length and to contain lowercase letters, uppercase letters, numerals, and/or symbols, a simple dictionary attack will not work on the system.\n\n\nEnforce a Password Change Policy\u2014By requiring passwords to be changed on a regular basis, an attacker might not have enough time to brute-force a potential password. However, strict password change policies can frustrate users and weaken passwords by causing users to follow patterns, such as using password1, password2, etc. [Bell06].\n\n\nUse Blacklists\u2014Blocking IP addresses or domains known to attempt brute force attacks from accessing the system may stop some attackers, but it is possible that some attacks may come from compromised systems that would otherwise be considered legitimate.\n\n\nUse Log Monitoring Software\u2014Vigilantly monitoring logs of invalid password attempts can help detect and respond to brute force attacks.# CURRENT_PAGE_RAW_OCR_TEXT"
          },
          {
            "text": "Use Strong Authentication\u2014Strong authentication techniques, such as hardware tokens, one-time passwords, biometric authentication, and SSL/TLS client certificates, are much more resistant to brute force attacks than passwords. Stronger authentication can be achieved by combining multiple authentication mechanisms to form a multi-factor authentication scheme. However, strong authentication may be prohibitively expensive or difficult to incorporate into a system."
          },
          {
            "text": "Use Timeouts\u2014Incurring a delay of several seconds after a failed login attempt can slow an attacker down. However, attackers can attempt multiple logins at the same time from different clients."
          },
          {
            "text": "Use Lockouts\u2014Locking out a user account after a number of failed login attempts prevents the attacker from successfully logging into an account. The primary disadvantage of this technique is that it can leave the system open to a DoS attack. Also, an attacker may try several common passwords against random usernames, which may grant the attacker access to the system while bypassing the lockout [Whit06]."
          },
          {
            "text": "Enforce a Password Policy\u2014By requiring passwords to be of a certain length and to contain lowercase letters, uppercase letters, numerals, and/or symbols, a simple dictionary attack will not work on the system."
          },
          {
            "text": "Enforce a Password Change Policy\u2014By requiring passwords to be changed on a regular basis, an attacker might not have enough time to brute-force a potential password. However, strict password change policies can frustrate users and weaken passwords by causing users to follow patterns, such as using password1, password2, etc. [Bell06]."
          },
          {
            "text": "Use Blacklists\u2014Blocking IP addresses or domains known to attempt brute force attacks from accessing the system may stop some attackers, but it is possible that some attacks may come from compromised systems that would otherwise be considered legitimate."
          },
          {
            "text": "Use Log Monitoring Software\u2014Vigilantly monitoring logs of invalid password attempts can help detect and respond to brute force attacks.# CURRENT_PAGE_RAW_OCR_TEXT"
          }
        ]
      },
      {
        "title": "attempts may help",
        "content": [
          {
            "text": "an organization detect brute force attacks, potentially giving the organization time to respond before the attack has been successful. Aside from strong authentication, none of these mechanisms completely prevent brute force attacks; however, using one or more of these techniques makes it more difficult for an attacker to gain access to the system. Nevertheless, when considering which technologies to adopt, it is important to consider passwords as part of the system as a whole. For example, a Web site that uses usernames and passwords to retrieve user customizations may not need to concern itself with preventing brute force attacks [Bell06]. In systems where sensitive information is being protected, some of these techniques may be necessary. Regardless, an organization may already have policies regarding brute force attacks. If so, those policies should be followed and enhanced if necessary."
          }
        ]
      },
      {
        "title": "8. Implementing a Secure Network Infrastructure",
        "content": [
          {
            "text": "The network infrastructure that supports the Web server plays a critical role in the security of the Web server. In most configurations, the network infrastructure is the first line of defense between the Internet and a public Web server. Network design alone, however, cannot protect a Web server. The frequency, sophistication, and variety of attacks perpetrated today lend support to the idea that Web security must be implemented through layered and diverse protection mechanisms (defense-in-depth). This section discusses those network components that can support and protect Web servers to further enhance their overall security. Although security issues are paramount, network infrastructure considerations are influenced by many factors other than security, including cost, performance, and reliability."
          },
          {
            "subsection": "8.1 Network Composition and Structure",
            "text": []
          },
          {
            "text": "Firewalls and routers are devices or systems that control the flow of network traffic between networks. They can protect Web servers from vulnerabilities inherent in the TCP/IP suite and help reduce the security issues associated with insecure applications and OSs. However, an organization has many choices when determining a network environment for a Web server, and security may not be the principal factor in deciding among those options. Network composition and structure are the first and in many respects the most critical decisions that affect Web server security because they determine what network infrastructure elements protect the Web server. For example, if the Web server is located before the organization's main firewall, then the firewall cannot be used to control traffic to and from the Web server. Network composition and structure also determine what other portions of the network are vulnerable if the Web server is compromised. For example, an externally accessible Web server located on the internal production network subjects the internal network to attack if the Web server is compromised. Also, an organization may choose not to have the Web server located on its network at all and to outsource the hosting to a third party."
          },
          {
            "text": "Some organizations choose to locate their public Web servers on their internal# Production Networks"
          },
          {
            "text": "That is, their Web servers reside on the same network as the internal users and servers. The principal weakness of this layout is that it exposes internal network components to additional risks. Web servers are often targets of attackers. If attackers manage to compromise a Web server, they will have access to the internal network and will be able to more easily compromise internal hosts. Therefore, this layout is not recommended."
          },
          {
            "text": "Another network layout that is not generally recommended is placing the Web server before an organization's firewall or router that provides IP filtering. In this structure, the network provides little, if any, protection for the Web server. Because the Web server itself has to maintain security, it provides a single point of failure. To be even somewhat secure in this location, the Web server OS and application have to be extremely well-hardened, with all unnecessary and insecure services disabled and all necessary security patches applied. To maintain the security of the setup, the Web server administrator must stay up to date on vulnerabilities and related patches. Another limitation of this structure is that providing any sort of secure remote administration or content update capability is difficult."
          }
        ]
      },
      {
        "title": "8.1.2 Demilitarized Zone",
        "content": [
          {
            "text": "A demilitarized zone (DMZ) describes a host or network segment inserted as a \"neutral zone\" between an organization's private network and the Internet. It prevents outside users of the Web server from gaining direct access to an organization's internal network (intranet). A DMZ mitigates the risks of locating a Web server on an internal network or exposing it directly to the Internet. It is a compromise solution that offers the most benefits with the least amount of risk for most organizations. The DMZ allows access to the resources located within it to both internal and external users. There are a wide variety of DMZ configurations, each with its own strengths and weaknesses."
          },
          {
            "text": "Creating a DMZ involves placing a firewall between an organization's border router and its internal network, and creating a new network segment that can only be reached through the DMZ device. The Web server is placed on the new segment, along with other network infrastructure components and servers that need to be externally accessible. In some configurations, the border router itself may act as a basic firewall."
          },
          {
            "text": "Figure 8-1 illustrates an example of this simple DMZ using a router with access control lists (ACL) to restrict certain types of network traffic to and from the DMZ. A single-firewall DMZ is a low-cost approach because the organization needs only to add a single firewall and use its existing border router to provide protection to the DMZ. It is usually appropriate only for small organizations that face a minimal threat. The basic weakness in the approach is that although the router is able to protect against most network attacks, it is not \"aware\" of the Web server application layer protocols (e.g., HTTP) and thus cannot protect against application layer attacks aimed at the Web server. A superior approach is to add a second firewall between the Internet and the DMZ, as shown in Figure 8-2."
          },
          {
            "text": "A two-firewall DMZ configuration improves protection over a router-firewall DMZ.# CURRENT_PAGE_RAW_OCR_TEXT"
          },
          {
            "text": "because the dedicated firewalls can have more complex and powerful security rule sets. In addition, because a dedicated firewall is often able to analyze incoming and outgoing HTTP traffic, it can detect and defend against application layer attacks aimed at the Web server. Depending on the rule sets of the firewalls and the level of traffic the DMZ receives, this type of DMZ may result in some performance degradation."
          },
          {
            "text": "For organizations that desire the security of the two-firewall DMZ but do not have the resources to purchase two firewalls, another option exists called the \"service leg\" DMZ. In this configuration, a firewall is constructed with three (or more) network interfaces. One network interface attaches to the border router, another interface attaches to the internal network, and a third network interface connects to the DMZ (see Figure 8-3)."
          },
          {
            "text": "This configuration subjects the firewall to an increased risk of service degradation during a DoS attack aimed at the DMZ. In the standard single-firewall DMZ network configuration discussed above, a DoS attack against the Web server generally affects only the Web server. In a service-leg DMZ network configuration, the firewall bears the brunt of any DoS attack because it must examine any network traffic before the traffic reaches the Web server (or any other DMZ or internal network resource) [NIST02a]."
          },
          {
            "text": "However, it is increasingly likely that a DoS attack will take the form of a DDoS attack and consume all of the incoming network bandwidth and related devices (e.g., Internet border routers) before ever reaching a DMZ firewall."
          }
        ]
      },
      {
        "title": "Advantages of a DMZ from a Security Standpoint",
        "content": [
          {
            "text": "The advantages of a DMZ from a security standpoint are as follows:"
          },
          {
            "text": "The Web server may be better protected, and network traffic to and from the Web server can be monitored.\nCompromise of the Web server does not directly threaten the internal production network.\nGreater control can be provided over the security of the Web server because traffic to and from the Web server can be controlled.\nThe DMZ network configuration can be optimized to support and protect the Web servers."
          }
        ]
      },
      {
        "title": "Disadvantages of a DMZ from a Security Standpoint",
        "content": [
          {
            "text": "The disadvantages of a DMZ from a security standpoint are as follows:"
          },
          {
            "text": "DoS attacks aimed at the Web server may have an effect on the internal network.\nDepending on the firewall configuration controlling traffic between the DMZ and internal network, it may be possible for the Web server to be used to attack or compromise hosts on the internal network."
          },
          {
            "text": "In other words, protection offered by the DMZ depends in large part on the firewall configuration."
          },
          {
            "text": "For organizations that support their own Web server, a DMZ is almost invariably the best option. It offers protection for the Web server and other externally accessible servers without exposing the internal network. However, it should only be considered secure when employed in conjunction with the other steps discussed in this document."
          },
          {
            "subsection": "8.1.3 Outsourced Hosting",
            "text": []
          },
          {
            "text": "Some organizations choose to outsource the hosting of their Web servers to a third party (e.g., an ISP, Web hosting service, or other government agency). In this case, the Web server# Outsourcing Web Server Management"
          },
          {
            "text": "would not be located on the organization's network. The hosting service network would have a dedicated network that hosts many Web servers (for many organizations) operating on a single network (see Figure 8-4)."
          }
        ]
      },
      {
        "title": "Advantages of Outsourcing",
        "content": [
          {
            "text": "From a security standpoint, the advantages of outsourcing are as follows:"
          },
          {
            "text": "DoS attacks aimed at the Web server have no effect on the organization's production network.\nCompromise of the Web server does not directly threaten the internal production network.\nThe outsourcer may have greater knowledge of securing and protecting Web servers.\nThe network can be optimized solely for the support and protection of Web servers."
          }
        ]
      },
      {
        "title": "Disadvantages of Outsourcing",
        "content": [
          {
            "text": "The disadvantages of outsourcing from a security standpoint are as follows:"
          },
          {
            "text": "It requires trusting a third party with Web server content.\nIt is more difficult to remotely administer the Web server or remotely update Web server content.\nLess control can be provided over the security of the Web server.\nThe Web server may be affected by attacks aimed at other Web servers hosted by the outsourcer on the same network."
          },
          {
            "text": "Outsourcing often makes sense for smaller organizations that cannot afford to support the necessary Web server staff. It may also be appropriate for larger organizations that do not wish to host their own Web servers. Outsourcing usually does not make sense for organizations that wish to maintain tight control over their Web servers."
          }
        ]
      },
      {
        "title": "8.1.4 Management Network",
        "content": [
          {
            "text": "Web servers and other important components can be connected to each other and managed through an organization's standard networks or through a separate network known as a management network. If a management network is used, each host being managed through the network has an additional network interface known as a management interface that connects to the management network. Also, each host being managed is unable to pass any traffic between its management interface and any of its other network interfaces. Consoles and other hosts that are used to manage the Web components are attached to the management network only. This architecture effectively isolates the management network from the production networks. The benefits of doing this are to protect the components from some attacks and to ensure that the components can be managed under adverse conditions (e.g., DDoS attack). Disadvantages of using a management network include the additional costs of networking equipment and other hardware (e.g., personal computers [PC] for the consoles) and the inconvenience for Web component administrators of using separate computers for management and monitoring."
          }
        ]
      },
      {
        "title": "8.2 Network Element Configuration",
        "content": [
          {
            "text": "Once the Web server has been positioned in the network, the network infrastructure elements should be configured to support and protect it. The primary elements of a network infrastructure that affect Web server security are firewalls, routers, IDSs, intrusion prevention systems (IPS), switches, load balancers, and reverse proxies. Each has an important role to play and is critical to the overall strategy of protecting the Web server through defense-in-depth. Unfortunately, when it comes to securing a Web server, there...# CURRENT_PAGE_RAW_OCR_TEXT"
          },
          {
            "text": "is no single \"silver bullet\" solution. A firewall or IPS alone cannot adequately protect a public Web server from all threats or attacks."
          }
        ]
      },
      {
        "title": "8.2.1 Router/Firewall Configuration",
        "content": [
          {
            "text": "Several types of firewalls exist. The most basic firewalls are routers that can provide access control for IP packets. In the middle are stateful firewalls that can provide access control based on TCP and User Datagram Protocol (UDP) as well as IP. The most powerful firewalls are application layer or proxy firewalls that are able to understand and filter Web content."
          },
          {
            "text": "A common misperception about firewalls (and routers acting as firewalls) is that they eliminate all risk and can protect against misconfiguration of the Web server or poor network design. Unfortunately, this is not the case. Firewalls and routers themselves are vulnerable to misconfiguration and software vulnerabilities. In addition, many firewalls have limited insight into the application layer where many attacks occur. Thus, Web servers in particular are vulnerable to many attacks, even when located behind a secure, well-configured firewall."
          },
          {
            "text": "A firewall (or router acting as a firewall) that is protecting a Web server should be configured to block all access to the Web server from the Internet except the necessary ports, such as TCP ports 80 (HTTP) and 443 (HTTPS). A firewall is the first line of defense for a Web server; however, to be truly secure, organizations need to implement layered protection for their Web servers (and networks). Most importantly, organizations should strive to maintain all systems in a secure posture and not depend solely on firewalls, routers, or any other single component to stop attackers."
          },
          {
            "text": "A modern enterprise router is able to function as a network and transport layer filter (e.g., a basic firewall). A router functioning as a network/transport layer firewall can provide filtering based on several pieces of information [NIST02a], including the following:"
          },
          {
            "text": "Source IP address\nDestination IP address\nTraffic type\nTCP/UDP port number and state."
          },
          {
            "text": "The strength of routers is in their cost. Most organizations already have a border router that can be configured to provide network/transport layer firewall capabilities."
          },
          {
            "text": "The weaknesses of routers include the following:"
          },
          {
            "text": "Susceptibility to application layer attacks (e.g., cannot examine Web content for embedded malicious code)\nSusceptibility to attacks via allowed ports\nDifficulty of configuration and administration\nLimitations in logging capabilities\nProcessing capabilities that may be more limited and overtaxed by complex rule sets (i.e., access control lists)\nInsufficient rule set expressiveness and filtering capabilities."
          },
          {
            "text": "The only \"pure\" network layer firewalls available today are small office/home office (SOHO) firewall appliances and personal firewalls [NIST02a] that may only perform basic packet-level filtering. Stateful inspection firewalls are transport layer devices that incorporate \"awareness\" of the state of a TCP connection. Stateful inspection firewalls maintain internal information, such as the state of the# Firewall Types and Their Capabilities"
          },
          {
            "text": "Connections passing through them and the contents of some of the data streams. This allows better and more accurate rule sets and filtering to be specified. Stateful inspection firewalls add the capability to enforce rules based on connection state to the capabilities of a filtering router."
          }
        ]
      },
      {
        "title": "Application Layer Firewalls",
        "content": [
          {
            "text": "Application layer firewalls (sometimes called application-proxy gateway firewalls) are advanced firewalls that combine network and transport layer access control with application layer functionality. Application layer firewalls permit no traffic directly between the Internet and the internal network. They can usually perform extensive logging and access control."
          },
          {
            "text": "Application layer firewalls are considered the most secure type of firewall and have numerous advantages over packet filtering routers and stateful inspection firewalls, including the following:"
          },
          {
            "text": "Logging capabilities\nFiltering capabilities (can filter specific types of Web content and specific HTTP commands)\nProtocol conformance\nValidation of protocol behaviors\nIntegrated signature-based detection of application layer attacks\nEase of configuration\nUser authentication capabilities."
          },
          {
            "text": "The primary disadvantages that application layer firewalls have when compared to packet filtering routers and stateful inspection firewalls are as follows:"
          },
          {
            "text": "Speed of throughput (if platform is not adequately sized)\nCost (if high-end hardware is required to operate efficiently)\nInadequate support for less popular and new protocols."
          },
          {
            "text": "Although not strictly a limitation, some application layer firewalls are implemented on hosts running general-purpose OSs (e.g., Windows, Linux, Unix). This arrangement introduces an added layer of complexity and some additional risk because the general-purpose OS must also be secured in addition to the firewall software itself. Application layer firewalls are increasingly being deployed as appliance-based devices, which may use specialized OSs. Routers and stateful inspection firewalls also typically run on specialized OSs."
          }
        ]
      },
      {
        "title": "Protecting a Web Server with a Firewall",
        "content": [
          {
            "text": "To successfully protect a Web server using a firewall, ensure that the firewall is patched to the latest or most secure level (both the application and the underlying OS) and is configured to perform the following:"
          },
          {
            "text": "Control all traffic between the Internet and the Web server\nBlock all inbound traffic to the Web server except traffic which is required, such as TCP ports 80 (HTTP) and/or 443 (HTTPS)\nBlock all inbound traffic with an internal IP address (to prevent IP spoofing attacks)\nBlock client connections from the Web server to the Internet and the organization's internal network (this will reduce the impact of some successful compromises)\nBlock (in conjunction with the intrusion detection or prevention system [see Section 8.2.2]) IP addresses or subnets that the IDS or IPS reports are attacking the organizational network\nNotify the network or Web server administrator or appropriate security personnel of suspicious activity through an appropriate means (e.g., page, e-mail, network trap)\nProvide content filtering\n\nProtect against DoS attacks# CURRENT_PAGE_RAW_OCR_TEXT\n\n\nDetect malformed or known attack URL requests\n\nLog critical events, including the following details:\nTime/date\nInterface IP address\nManufacturer-specific event name\nStandard attack event (if one exists)\nSource and destination IP addresses\nSource and destination port numbers\nNetwork protocol."
          },
          {
            "text": "Protect against DoS attacks# CURRENT_PAGE_RAW_OCR_TEXT"
          },
          {
            "text": "Detect malformed or known attack URL requests"
          },
          {
            "text": "Most firewalls perform some type of logging of the traffic they receive. For most firewalls, the default logging configuration is suitable, provided logging is enabled. Administrators should consult the manufacturer's documentation if they believe they require additional information to be logged. Certain firewalls include an ability to track and log information for each rule, which enables accountability to a very specific extent."
          },
          {
            "text": "Many firewalls support the ability to selectively decide what information to log. If a firewall receives a series of similar packets from the same location, it may decide not to log any additional packets after the first one. Although this is a valuable feature, consider the consequences: each packet that is dropped and not logged is potential evidence of malicious intent. The principle of logging, a fundamental aspect of accountability, is discussed in detail in Section 9.1."
          },
          {
            "text": "As with OSs and other security-enforcing elements, a firewall requires updates. This includes updating firmware for hardware and router-based firewalls. Specific instructions on how to update a firewall are found in the manufacturer's documentation. Administrators should check for firewall updates frequently."
          }
        ]
      },
      {
        "title": "8.2.2 Intrusion Detection and Prevention Systems",
        "content": [
          {
            "text": "An IDS is an application that monitors the events occurring in a system or network and analyzes them for signs of potential incidents, which are violations or imminent threats of violation of computer security policies, acceptable use policies, or standard security practices. An IPS has all the capabilities of an IDS and can also attempt to stop potential incidents. Because IDS and IPS systems offer many of the same capabilities, they are often collectively called intrusion detection and prevention systems (IDPS)."
          },
          {
            "text": "When an IDPS detects a potential incident, it notifies administrators through IDPS console messages, emails, pages, or other mechanisms."
          },
          {
            "text": "The two types of IDPSs most relevant for Web security are host-based and network-based. A host-based IDPS monitors the characteristics of a single host and the events occurring within that host to identify and stop suspicious activity. Host-based IDPS software must be installed on each individual computer that is to be monitored or protected. Host-based IDPSs are very closely integrated with the OSs of the host computers they protect. Thus, a host-based IDPS must be designed specifically for each OS (and often each version of that OS). Host-based IDPSs monitor various aspects of hosts, such as network traffic, system logs, running processes, file access and modification, and system and application configuration changes."
          },
          {
            "text": "Host-based IDPSs are especially useful when most of the network traffic to and from a Web server is encrypted (e.g., SSL/TLS is in use) because the functionality and capability of...# Network-Based and Host-Based IDPS"
          },
          {
            "text": "Network-based IDPSs (see below) is severely limited when network traffic is encrypted. Also, because they are located on the server, host-based IDPSs can detect some attacks and penetration attempts not recognized by network-based IDPSs. Unfortunately, host-based IDPSs can have a negative effect on host performance. In general, enabling more extensive detection capabilities and having more events to monitor both have a negative impact on the performance of the host. Host-based IDPSs may not detect some network-based attacks, such as certain DoS attacks. If a host-based IDPS is on a Web server that is compromised, it is very likely that the attacker will also compromise the IDPS itself."
          }
        ]
      },
      {
        "title": "Network-Based IDPS",
        "content": [
          {
            "text": "A network-based IDPS monitors network traffic for particular network segments or network devices and analyzes the network and application protocol activity to identify and stop suspicious activity. Most network-based IDPSs use predefined \"attack signatures\" to detect and identify attacks. Attack signatures are patterns that correspond to known types of intrusions. Network-based IDPSs also use other detection methods to identify anomalous activity, protocol violations, and other unusual activity."
          },
          {
            "text": "Unlike a host-based IDPS, a network-based IDPS can monitor network activity for many hosts simultaneously. Network-based IDPSs can usually detect more network-based attacks and can more easily provide a comprehensive picture of the current attacks against a network. Because network-based IDPSs are installed on dedicated hosts, they do not have a negative effect on the performance of the Web server host and are not immediately compromised by a successful attack on the Web server."
          }
        ]
      },
      {
        "title": "Limitations of Network-Based IDPS",
        "content": [
          {
            "text": "Network-based IDPSs do have some limitations. The timing of an attack can have a significant impact on the ability of a network-based IDPS to detect an attack. For example, if an attacker spreads out the timing of an attack over a period of hours or days, the attack may not be detected by the IDPS. Network configuration, such as the use of asymmetric routing, can have a negative impact on the ability of a network-based IDPS to detect attacks. Network-based IDPSs are also more susceptible to being disabled by DoS attacks (even those not directly targeted at the IDPS). Also, depending on how the network-based IDPS is integrated into the network, it is possible to negatively affect the availability of the network in the event of an IDPS hardware failure."
          }
        ]
      },
      {
        "title": "Updates and Zero-Day Attacks",
        "content": [
          {
            "text": "Both host-based and network-based IDPSs require frequent updates to their attack signature databases so that they can recognize new attacks. An IDPS that is not updated frequently will fail to recognize the latest (and often most popular) attacks. Both types of IDPSs may be limited in their ability to detect zero-day attacks because it is unlikely that an appropriate signature is available. A host-based IDPS may have a better chance of detecting a zero-day attack because it is better able to detect the actions taken by an attacker after a successful exploit (e.g., new unauthorized privileged accounts, installation of malicious software)."
          }
        ]
      },
      {
        "title": "File Integrity Checkers",
        "content": [
          {
            "text": "File integrity checkers are a simple form of host-based IDPS. A file integrity checker computes and stores a hash for every file of interest and establishes a database of file hashes. It provides a tool for...# CURRENT_PAGE_RAW_OCR_TEXT"
          }
        ]
      },
      {
        "title": "System Administrators and File Integrity",
        "content": [
          {
            "text": "System administrators to recognize changes to files, particularly unauthorized changes. File integrity checkers are available as standalone products and bundled with other host-based IDPS techniques. Some host-based IDPSs can monitor file access attempts and stop suspicious attempts to read, modify, delete, and execute files. A host-based IDPS with this capability could be configured to protect important Web server files."
          },
          {
            "text": "To successfully protect a Web server using an IDPS, ensure that the IDPS is configured to\u2014"
          },
          {
            "text": "Monitor network traffic to and from the Web server\nMonitor changes to critical files on the Web server (file integrity checking capability)\nMonitor the system resources available on the Web server host (host-based)\nBlock (in conjunction with the firewall) IP addresses or subnets that are attacking the organizational network\nNotify the appropriate parties (e.g., IDPS administrator, Web server administrator, incident response team) of suspected attacks through appropriate means according to the organizational incident response policy and procedures\nDetect as wide a variety of scanning and attacks as possible with an acceptable level of false positives\nLog events, including the following details:\nTime/date\nSensor IP address\nManufacturer-specific attack name\nStandard attack name (if one exists)\nSource and destination IP addresses\nSource and destination port numbers\nNetwork protocol\nFor network events, capture packet header information to assist with the analysis and forensics process\nUpdate with new attack signatures frequently (e.g., on a daily to weekly basis, typically after testing the updates)."
          },
          {
            "text": "In addition, it is critical that network-based IDPSs and their underlying OSs are hardened because network-based IDPSs are often a target of attackers. In particular, the network-based IDPSs should not respond to any type of system interrogation through their monitoring interfaces. If remote management is desired, it should be conducted through an out-of-band means (e.g., separate isolated network). Although sometimes difficult to administer and interpret, IDPSs are a critical early warning system that can provide the Web server administrator with the necessary information to defend the Web server from attack [NIST07]."
          }
        ]
      },
      {
        "title": "8.2.3 Network Switches",
        "content": [
          {
            "text": "Network switches are devices that provide connectivity between two or more hosts located on the same network segment. They are similar to hubs in that they allow communications between hosts, but, unlike hubs, the switches have more \"intelligence\" and send communications to only those hosts to which the communications are addressed. The benefit of this from a security standpoint is that when switches are employed on a network, it is much more difficult to eavesdrop on communications between other hosts on the network segment. This is extremely important when a Web server is on a...# Network Security Considerations"
          }
        ]
      },
      {
        "title": "Network Segments",
        "content": [
          {
            "text": "A network segment that is used by other hosts. For example, if a hub is used and a host on the DMZ is compromised, an attacker may be able to eavesdrop on the communications of other hosts on the DMZ, possibly leading to the compromise of those hosts or the information they communicate across the network. For example, e-mail servers in their default configurations receive unencrypted passwords; a compromise of the Web server would lead to the exposure of e-mail passwords by sniffing them from the compromised Web server."
          }
        ]
      },
      {
        "title": "Switch Security",
        "content": [
          {
            "text": "Many switches include specific security settings that further enhance the security of the network by making it difficult for a malicious entity to \"defeat\" the switch. Some examples include the ability to minimize the risk of Address Resolution Protocol (ARP) spoofing and ARP poisoning attacks. If a switch has these security capabilities, they should be enabled (see appropriate manufacturer documentation)."
          },
          {
            "text": "Switches can have a negative impact on network-based IDPSs (see Section 8.2.2). Most network switches allow network administrators to configure a specific port on the switch, known as a span port, so that it replicates all the switch's traffic to the port used by the IDPS. This allows a network-based IDPS to see all traffic on a particular network segment. However, under high loads, the switch might have to stop sending traffic to the span port, causing the IDPS to be unable to monitor network activity. Also, other devices use span ports, and there are typically very few span ports on a switch; therefore, it might not be possible to connect an IDPS to a particular switch because its span ports are all in use."
          }
        ]
      },
      {
        "title": "Load Balancers",
        "content": [
          {
            "subsection": "Functionality",
            "text": []
          },
          {
            "text": "Load balancers distribute HTTP requests over multiple Web servers, allowing organizations to increase the capacity of their Web site by transparently adding additional servers. Load balancers act as virtual servers, receiving all HTTP requests to the Web site. These requests are forwarded, based on the load balancer's policy, to one of the servers that hosts the Web site. The load balancer's policy attempts to ensure that each server receives a similar number of requests. Many load balancers are capable of monitoring the servers and compensating if one of the servers becomes unavailable."
          },
          {
            "subsection": "Caching Mechanisms",
            "text": []
          },
          {
            "text": "Load balancers are often augmented by caching mechanisms. Many of the HTTP requests an organization's Web server receives are identical and return identical HTTP responses. However, when dynamic content generation is in use, these identical responses need to be regenerated each time the request is made. To alleviate this requirement and further reduce the load on individual Web servers, organizations can deploy caching servers."
          },
          {
            "subsection": "Importance in Security",
            "text": []
          },
          {
            "text": "Like network switches, load balancers are not specifically security appliances, but they are essential tools for maintaining the availability of a Web site. By ensuring that several individual Web servers are sharing the load, rather than placing it on a single Web server, the organization is better able to withstand the high volume of requests used in many DoS attacks. Firewalls, switches, and routers should also be...# CURRENT_PAGE_RAW_OCR_TEXT"
          }
        ]
      },
      {
        "title": "configured (when possible) to limit the amount of traffic that is passed to the Web servers, which further reduces the risk of successful DoS attacks.",
        "content": [
          {
            "subsection": "8.2.5 Reverse Proxies",
            "text": []
          },
          {
            "text": "Reverse proxies are devices that sit between a Web server and the server's clients. The term \"reverse proxy\" is used because the data flow is the reverse of a traditional (forward) proxy. Reverse proxies can serve as a valuable addition to the security of a Web server. The term reverse proxy is used rather loosely in the industry and can include some or all of the following functionality:"
          },
          {
            "text": "Encryption accelerators, which off-load the computationally expensive processing required for initiating SSL/TLS connections\nSecurity gateways, which monitor HTTP traffic to and from the Web server for potential attacks and take action as necessary, acting in essence as an application level firewall\nContent filters, which can monitor traffic to and from the Web server for potentially sensitive or inappropriate data and take action as necessary\nAuthentication gateways, which authenticate users via a variety of mechanisms and control access to URLs hosted on the Web server itself."
          },
          {
            "text": "Reverse proxies should be considered for any high-risk Web server deployment. While they do add risk by requiring the deployment of additional hardware and software, the risk is generally outweighed by the benefits. In addition to the functionality list above, Web proxies are also valuable because they add an additional layer between a Web server and its less trusted users. Due to their highly specialized nature, proxies are easier to secure than Web servers. Proxies also further obfuscate a Web server's configuration, type, location, and other details that are pertinent to attackers. For example, Web servers have banners that frequently reveal the Web server type and version, and these banners sometimes cannot be changed. With a reverse proxy, this is not an issue because the proxy can rewrite the banner before it is sent to users."
          }
        ]
      },
      {
        "title": "9. Administering the Web Server",
        "content": [
          {
            "text": "After initially deploying a Web server, administrators need to maintain its security continuously. This section provides general recommendations for securely administering Web servers. Vital activities include handling and analyzing log files, performing regular Web server backups, recovering from Web server compromises, testing Web server security regularly, and performing remote administration securely."
          },
          {
            "subsection": "9.1 Logging",
            "text": []
          },
          {
            "text": "Logging is a cornerstone of a sound security posture. Capturing the correct data in the logs and then monitoring those logs closely is vital. Network and system logs are important, especially system logs in the case of HTTPS-protected communications, where network monitoring is less effective. Web server software can provide additional log data relevant to Web-specific events. Similarly, Web applications may also maintain their own logs of actions."
          },
          {
            "text": "Reviewing logs is mundane and reactive, and many Web server administrators devote their time to performing duties that they consider more important or urgent. However, log files are often the only# Record of Suspicious Behavior"
          },
          {
            "text": "Enabling the mechanisms to log information allows the logs to be used to detect failed and successful intrusion attempts and to initiate alert mechanisms when further investigation is needed. Procedures and tools need to be in place to process and analyze the log files and to review alert notifications."
          }
        ]
      }
    ]
  },
  {
    "title": "and they appear in the log file as follows:",
    "subsections": [
      {
        "title": "Version: 1.0",
        "content": []
      },
      {
        "title": "Fields: date time c-ip sc-bytes time-taken cs-version",
        "content": [
          {
            "text": "1999-08-01 02:10:57 192.0.0.2 6340 3 HTTP/1.0"
          },
          {
            "text": "This example contains the date, time, originating address, number of bytes transmitted, time taken for transmission, and HTTP version."
          },
          {
            "text": "Other Log File Formats\u2014Some server software provides log information in different file formats, such as database formats or delimiter-separated formats. Other server software provides the capability for an administrator to define specific log file formats in the Web server configuration file using a particular syntax (if the default CLF format is insufficient)."
          }
        ]
      },
      {
        "title": "9.1.2 Identifying Additional Logging Requirements",
        "content": [
          {
            "text": "If a public Web server supports the execution of programs, scripts, or plug-ins, it may be necessary for the programs, scripts, or plug-ins to perform additional logging. Often, critical events take place within the Web application code itself and will not be logged by the Web server. If Webmasters develop or acquire application programs, scripts, or plug-ins, it is strongly recommended that they define and implement a comprehensive and easy-to-understand logging approach based on the logging mechanisms provided by the Web server host OS. Log information associated with programs, scripts, and plug-ins can add significantly to the typical information logged by the Web server and may prove invaluable when investigating events."
          }
        ]
      },
      {
        "title": "9.1.3 Recommended Generic Logging Configuration",
        "content": [
          {
            "text": "The following configuration is a good starting point for logging into public Web servers [Alle00]:"
          },
          {
            "text": "Use the combined log format for storing the Transfer Log, or manually configure the information described by the combined log format to be the standard format for the Transfer Log.\nEnable the Referrer Log or Agent Log if the combined log format is unavailable.\nEstablish different log file names for different virtual Web sites that may be implemented as part of a single physical Web server.\nUse the remote user identity as specified in RFC 1413.\nEnsure procedures or mechanisms are in place so that log files do not fill up the hard drive."
          },
          {
            "text": "Ensuring that sufficient log capacity is available is a concern because logs often take considerably more space than administrators initially estimate, especially when logging is set to a highly detailed level. Administrators should closely monitor the size of the log files when they implement different logging settings to ensure that the log files do not fill up the allocated storage. Because of the size of the log files, removing and archiving the logs more frequently or reducing the logging level of detail may be necessary."
          },
          {
            "text": "Some Web server programs provide a capability to enforce or disable the checking of specified access controls during program startup. This level of control may be helpful, for example, to avoid inadvertent alteration of log files because of errors in file access administration. Web server administrators should determine the circumstances under which they may wish to enable such checks (assuming the Web server software supports this feature).\n```# 9.1.4 Reviewing and Retaining Log Files"
          },
          {
            "text": "Reviewing log files is a tedious and time-consuming task that informs administrators of events that have already occurred. Accordingly, files are often useful for corroborating other evidence, such as a CPU utilization spike or anomalous network traffic reported by an IDPS. When a log is used to corroborate other evidence, a focused review is in order. For example, if an IDPS reported an outbound FTP connection from the Web server at 8:17 a.m., then a review of the logs generated around 8:17 a.m. is appropriate. Web server logs should also be reviewed for indications of attacks. The frequency of the reviews depends on the following factors:"
          },
          {
            "text": "Amount of traffic the server receives\nGeneral threat level (certain sites receive many more attacks than other sites and thus should have their logs reviewed more frequently)\nSpecific threats (at certain times, specific threats arise that may require more frequent log file analysis)\nVulnerability of the Web server\nValue of data and services provided by the Web server."
          },
          {
            "text": "Reviews should take place regularly (e.g., daily) and when a suspicious activity has been noted or a threat warning has been issued. Obviously, the task could quickly become burdensome to a Web server administrator. To reduce this burden, automated log analysis tools have been developed (see Section 9.1.5)."
          },
          {
            "text": "In addition, a long-term and more in-depth analysis of the logs is needed. Because a typical Web server attack can involve hundreds of unique requests, an attacker may attempt to disguise a Web server attack by increasing the interval between requests. In this case, reviewing a single day's or week's logs may not show recognizable trends. However, when trends are analyzed over a week, month, or quarter, multiple attacks from the same host or subnet can be more easily recognized."
          },
          {
            "text": "Log files should be protected to ensure that if an attacker does compromise a Web server, the log files cannot be altered to cover the attack. Although encryption can be useful in protecting log files, the best solution is to store log files on a host separate from the Web server. This is often called a centralized logging server. Centralized logging is often performed using syslog, which is a standard logging protocol. Alternately, some organizations use security information and event management (SIEM) software that uses centralized servers to perform log analysis, database servers to store logs, and either agents installed on the individual hosts or processes running on the centralized servers to transfer Web server logs or log data from the hosts to the servers and parse the logs."
          },
          {
            "text": "Log files should be backed up and archived regularly. Archiving log files for a period of time is important for several reasons, including supporting certain legal actions and troubleshooting problems with the Web server. The retention period for archived log files depends on a number of factors, including the following:"
          },
          {
            "text": "Legal requirements\nOrganizational requirements\nSize of logs (which is directly related to the traffic of the site and the# CURRENT_PAGE_RAW_OCR_TEXT"
          }
        ]
      },
      {
        "title": "Number of details logged",
        "content": [
          {
            "text": "Value of Web server data and services\nThreat level."
          },
          {
            "subsection": "9.1.5 Automated Log File Analysis Tools",
            "text": []
          },
          {
            "text": "Most public Web servers receive significant amounts of traffic, and the log files quickly become voluminous. Automated log analysis tools should be installed to ease the burden on the Web server administrator. These tools analyze the entries in the Web server log files and identify suspicious and unusual activity. As mentioned in Section 9.1.2, some organizations use SIEM software for centralized logging, which can also perform automated log file analysis."
          },
          {
            "text": "Many commercial and public domain tools are available to support regular analysis of Transfer Logs. Most operate on either the common or the combined log formats. These tools can identify IP addresses that are the source of high numbers of connections and transfers."
          },
          {
            "text": "Error Log tools indicate not only errors that may exist within available Web content (such as missing files) but also attempts to access nonexistent URLs. Such attempts could indicate the following:\n- Probes for the existence of vulnerabilities to be used later in launching an attack\n- Information gathering\n- Interest in specific content, such as databases."
          },
          {
            "text": "The automated log analyzer should forward any suspicious events to the responsible Web server administrator or security incident response team as soon as possible for follow-up investigation. Some organizations may wish to use two or more log analyzers, which will reduce the risk of missing an attacker or other significant events in the log files [NIST06b]."
          },
          {
            "subsection": "9.2 Web Server Backup Procedures",
            "text": []
          },
          {
            "text": "One of the most important functions of a Web server administrator is to maintain the integrity of the data on the Web server. This is important because Web servers are often some of the most exposed and vital servers on an organization's network. There are two principal components to backing up data on a Web server: regular backup of the data and OS on the Web server, and maintenance of a separate protected authoritative copy of the organization's Web content."
          },
          {
            "text": "The Web server administrator needs to perform backups of the Web server on a regular basis for several reasons. A Web server could fail as a result of a malicious or unintentional act or a hardware or software failure. In addition, Federal agencies and many other organizations are governed by regulations on the backup and archiving of Web server data. Web server data should also be backed up regularly for legal and financial reasons."
          },
          {
            "text": "All organizations need to create a Web server data backup policy. Three main factors influence the contents of this policy:\n- Legal requirements\n- Applicable laws and regulations (Federal, state, and international)\n- Litigation requirements\n- Mission requirements\n- Contractual\n- Accepted practices\n- Criticality of data to organization\n- Organizational guidelines and policies.# Web Server Backup Policy"
          },
          {
            "text": "Although each organization's Web server backup policy will be different to reflect its particular environment, it should address the following issues:"
          },
          {
            "text": "Purpose of the policy\nParties affected by the policy\nWeb servers covered by the policy\nDefinitions of key terms, especially legal and technical\nDetailed requirements from the legal, business, and organization's perspective\nRequired frequency of backups\nProcedures for ensuring data is properly retained and protected\nProcedures for ensuring data is properly destroyed or archived when no longer required\nProcedures for preserving information for Freedom of Information Act (FOIA) requests, legal investigations, and other such requests\nResponsibilities of those involved in data retention, protection, and destruction activities\nRetention period for each type of information logged\nSpecific duties of a central/organizational data backup team, if one exists."
          }
        ]
      },
      {
        "title": "Types of Backups",
        "content": [
          {
            "text": "Three primary types of backups exist: full, incremental, and differential."
          },
          {
            "subsection": "Full Backups",
            "text": []
          },
          {
            "text": "Full backups include the OS, applications, and data stored on the Web server (i.e., an image of every piece of data stored on the Web server hard drives). The advantage of a full backup is that it is easy to restore the entire Web server to the state (e.g., configuration, patch level, data) it was in when the backup was performed. The disadvantage of full backups is that they take considerable time and resources to perform."
          },
          {
            "subsection": "Incremental Backups",
            "text": []
          },
          {
            "text": "Incremental backups reduce the impact of backups by backing up only data that has changed since the previous backup (either full or incremental)."
          },
          {
            "subsection": "Differential Backups",
            "text": []
          },
          {
            "text": "Differential backups reduce the number of backup sets that must be accessed to restore a configuration by backing up all changed data since the last full backup. However, each differential backup increases as time lapses from the last full backup, requiring more processing time and storage than would an incremental backup. Generally, full backups are performed less frequently (weekly to monthly or when a significant change occurs), and incremental or differential backups are performed more frequently (daily to weekly)."
          },
          {
            "text": "The frequency of backups will be determined by several factors:"
          },
          {
            "text": "Volatility of information on the Web site\nStatic Web content (less frequent backups)\nDynamic Web content (more frequent backups)\nE-commerce/e-government (very frequent backups)\nVolatility of configuring the Web server\nAmount of data to be backed up\nBackup device and media available\nTime available for dumping backup data\nCriticality of data\nThreat level faced by the Web server\nEffort required for data reconstruction without data backup\nOther data backup or redundancy features of the Web server (e.g., Redundant Array of Inexpensive Disks [RAID])."
          }
        ]
      },
      {
        "title": "Maintain a Test Web Server",
        "content": [
          {
            "text": "Most organizations will probably wish to maintain a test or development Web server. Ideally, this server should have hardware and software identical to the production or live Web server and be located on an internal network segment (intranet) where it can be fully protected by the organization's perimeter.# Network Defenses"
          },
          {
            "text": "Although the cost of maintaining an additional Web server is not inconsequential, having a test Web server offers numerous advantages:"
          },
          {
            "text": "It provides a platform to test new patches and service packs prior to application on the production Web server.\nIt provides a development platform for the Webmaster and Web server administrator to develop and test new content and applications.\nIt provides a platform to test configuration settings before applying them to production Web servers.\nSoftware critical for development and testing but that might represent an unacceptable security risk on the production server can be installed on the development server (e.g., software compilers, administrative tool kits, remote access software)."
          },
          {
            "text": "The test Web server should be separate from the server that maintains an authoritative copy of the content on the production Web server (see Section 9.2.3)."
          }
        ]
      },
      {
        "title": "9.2.3 Maintain an Authoritative Copy of Organizational Web Content",
        "content": [
          {
            "text": "All organizations should maintain an authoritative (i.e., verified and trusted) copy of their public Web sites on a host that is inaccessible to the Internet. This is a supplement to, but not a replacement for, an appropriate backup policy (see Section 9.2.1). For simple, relatively static Web sites, this could be as simple as a copy of the Web site on a read-only medium (e.g., Compact Disc-Recordable [CD-R])."
          },
          {
            "text": "However, for most organizations, the authoritative copy of the Web site is maintained on a secure host. This host is usually located behind the organization's firewall on the internal network and not on the DMZ (see Section 8.1.2). The purpose of the authoritative copy is to provide a means of restoring information on the public Web server if it is compromised as a result of an accident or malicious action. This authoritative copy of the Web site allows an organization to rapidly recover from Web site integrity breaches (e.g., defacement)."
          },
          {
            "text": "To successfully accomplish the goal of providing and protecting an authoritative copy of the Web server content, the following requirements must be met:"
          },
          {
            "text": "Protect authoritative copy from unauthorized access.\nUse write-once media (appropriate for relatively static Web sites).\nLocate the host with the authoritative copy behind a firewall, and ensure there is no outside access to the host.\nMinimize users with authorized access to host.\nControl user access in as granular a manner as possible.\nEmploy strong user authentication.\nEmploy appropriate logging and monitoring procedures.\nConsider additional authoritative copies at different physical locations for further protection.\nEstablish appropriate authoritative copy update procedures.\nUpdate authoritative copy first (any testing on code should occur before updating the authoritative copy).\nEstablish policies and procedures for who can authorize updates, who can perform updates, when updates can occur, etc.\nEstablish a process for copying authoritative copy to a production Web server.\nPhysically transfer data using secure physical media (e.g., encrypted and/or write-once media, such as CD-Rs).# Security Protocols and Recovery Procedures"
          }
        ]
      },
      {
        "title": "Secure Protocols for Network Transfers",
        "content": [
          {
            "text": "Use a secure protocol (e.g., SSH) for network transfers.\nInclude the procedures for restoring from the authoritative copy in the organizational incident response procedures (see Section 9.3).\nConsider performing automatic updates from the authoritative copy to the Web server periodically (e.g., every 15 minutes, hourly, or daily) because this will overwrite a Web site defacement automatically."
          }
        ]
      },
      {
        "title": "9.3 Recovering From a Security Compromise",
        "content": [
          {
            "text": "Most organizations eventually face a successful compromise of one or more hosts on their network. The first step in recovering from a compromise is to create and document the required policies and procedures for responding to successful intrusions before an intrusion. The response procedures should outline the actions that are required to respond to a successful compromise of the Web server and the appropriate sequence of these actions (sequence can be critical). Most organizations already have a dedicated incident response team in place, which should be contacted immediately when there is suspicion or confirmation of a compromise. In addition, the organization may wish to ensure that some of its staff are knowledgeable in the fields of computer and network forensics."
          },
          {
            "text": "A Web server administrator should follow the organization's policies and procedures for incident handling, and the incident response team should be contacted for guidance before the organization takes any action after a suspected or confirmed security compromise. Examples of steps commonly performed after discovering a successful compromise are as follows:"
          },
          {
            "text": "Report the incident to the organization's computer incident response capability.\nIsolate the compromised systems or take other steps to contain the attack so that additional information can be collected.\nConsult expeditiously, as appropriate, with management, legal counsel, and law enforcement.\nInvestigate similar hosts to determine if the attacker also has compromised other systems.\nAnalyze the intrusion, including\u2014\nThe current state of the server, starting with the most ephemeral data (e.g., current network connections, memory dump, files time stamps, logged in users)\nModifications made to the system's software and configuration\nModifications made to the data\nTools or data left behind by the attacker\nSystem, intrusion detection, and firewall log files.\nRestore the system.\nEither install a clean version of the OS, applications, necessary patches, and Web content; or restore the system from backups (this option can be more risky because the backups may have been made after the compromise, and restoring from a compromised backup may still allow the attacker access to the system).\nDisable unnecessary services.\nApply all patches.\nChange all passwords (including on uncompromised hosts, if their passwords are believed to have been seen by the compromised host, or if the same passwords are used on other hosts).\nReconfigure network security elements (e.g., firewall, router, IDPS) to...# CURRENT_PAGE_RAW_OCR_TEXT"
          }
        ]
      },
      {
        "title": "provide additional protection and notification.",
        "content": [
          {
            "text": "Test system to ensure security.\nReconnect system to network.\nMonitor system and network for signs that the attacker is attempting to access the system or network again.\nDocument lessons learned."
          },
          {
            "text": "Based on the organization's policy and procedures, system administrators should decide whether to reinstall the OS of a compromised system or restore it from a backup. Factors that are often considered include the following:"
          },
          {
            "text": "Level of access that the attacker gained (e.g., root, user, guest, system)\nType of attacker (internal or external)\nPurpose of compromise (e.g., Web page defacement, illegal software repository, platform for other attacks)\nMethod used for the system compromise\nActions of the attacker during and after the compromise (e.g., log files, intrusion detection reports)\nDuration of the compromise\nExtent of the compromise on the network (e.g., the number of hosts compromised)\nResults of consultation with management and legal counsel."
          },
          {
            "text": "The lower the level of access gained by the intruder and the more the Web server administrator understands about the attacker's actions, the less risk there is in restoring from a backup and patching the vulnerability. For incidents in which there is less known about the attacker's actions and/or in which the attacker gains high-level access, it is recommended that the OS and applications be reinstalled from the manufacturer's original distribution media and that the Web server data be restored from a known good backup."
          },
          {
            "text": "If legal action is pursued, system administrators need to be aware of the guidelines for handling a host after a compromise. Consult legal counsel and relevant law enforcement authorities as appropriate."
          }
        ]
      },
      {
        "title": "9.4 Security Testing Web Servers",
        "content": [
          {
            "text": "Periodic security testing of public Web servers is critical. Without periodic testing, there is no assurance that current protective measures are working or that the security patch applied by the Web server administrator is functioning as advertised. Although a variety of security testing techniques exists, vulnerability scanning is the most common. Vulnerability scanning assists a Web server administrator in identifying vulnerabilities and verifying whether the existing security measures are effective. Penetration testing is also used, but it is used less frequently and usually only as part of an overall penetration test of the organization's network."
          },
          {
            "subsection": "9.4.1 Vulnerability Scanning",
            "text": []
          },
          {
            "text": "Vulnerability scanners are automated tools that are used to identify vulnerabilities and misconfigurations of hosts. Many vulnerability scanners also provide information about mitigating discovered vulnerabilities."
          },
          {
            "text": "Vulnerability scanners attempt to identify vulnerabilities in the hosts scanned. Vulnerability scanners can help identify out-of-date software versions, missing patches, or system upgrades, and they can validate.# Compliance and Vulnerability Scanning"
          },
          {
            "text": "Compliance with or deviations from the organization's security policy. To accomplish this, vulnerability scanners identify OSs and major software applications running on hosts and match them with known vulnerabilities in their vulnerability databases."
          },
          {
            "text": "However, vulnerability scanners have some significant weaknesses. Generally, they identify only surface vulnerabilities and are unable to address the overall risk level of a scanned Web server. Although the scan process itself is highly automated, vulnerability scanners can have a high false positive error rate (reporting vulnerabilities when none exist). This means an individual with expertise in Web server security and administration must interpret the results. Furthermore, vulnerability scanners cannot generally identify vulnerabilities in custom code or applications."
          },
          {
            "text": "Vulnerability scanners rely on periodic updating of the vulnerability database to recognize the latest vulnerabilities. Before running any scanner, Web server administrators should install the latest updates to its vulnerability database. Some databases are updated more regularly than others (the frequency of updates should be a major consideration when choosing a vulnerability scanner)."
          },
          {
            "text": "Vulnerability scanners are often better at detecting well-known vulnerabilities than more esoteric ones because it is impossible for any one scanning product to incorporate all known vulnerabilities in a timely manner. In addition, manufacturers want to keep the speed of their scanners high (the more vulnerabilities detected, the more tests required, which slows the overall scanning process). Therefore, vulnerability scanners may be less useful to Web server administrators operating less popular Web servers, OSs, or custom-coded applications."
          }
        ]
      },
      {
        "title": "Capabilities of Vulnerability Scanners",
        "content": [
          {
            "text": "Vulnerability scanners provide the following capabilities:"
          },
          {
            "text": "Identifying active hosts on a network\nIdentifying active services (ports) on hosts and which of these are vulnerable\nIdentifying applications and banner grabbing\nIdentifying OSs\nIdentifying vulnerabilities associated with discovered OSs and applications\nTesting compliance with host application usage/security policies."
          },
          {
            "text": "Organizations should conduct vulnerability scanning to validate that OSs and Web server applications are up-to-date on security patches and software versions. Vulnerability scanning is a labor-intensive activity that requires a high degree of human involvement to interpret the results. It may also be disruptive to operations by taking up network bandwidth, slowing network response times, and potentially affecting the availability of the scanned server or its applications. However, vulnerability scanning is extremely important for ensuring that vulnerabilities are mitigated as soon as possible, before they are discovered and exploited by adversaries. Vulnerability scanning should be conducted on a weekly to monthly basis."
          },
          {
            "text": "Many organizations also run a vulnerability scan whenever a new vulnerability database is released for the organization's scanner application. Vulnerability scanning results should be documented and discovered deficiencies should be corrected."
          },
          {
            "text": "Organizations should also consider running more than one vulnerability scanner. As previously discussed, no scanner is able to detect all known vulnerabilities; however, using two scanners generally...# CURRENT PAGE RAW OCR TEXT"
          }
        ]
      },
      {
        "title": "Vulnerability Scanners",
        "content": [
          {
            "text": "Increases the number of vulnerabilities detected. A common practice is to use one commercial and one freeware scanner. Network-based and host-based vulnerability scanners are available for free or for a fee."
          }
        ]
      },
      {
        "title": "9.4.2 Penetration Testing",
        "content": [
          {
            "text": "\"Penetration testing is security testing in which evaluators attempt to circumvent the security features of a system based on their understanding of the system design and implementation\" [NISS99]. The purpose of penetration testing is to exercise system protections (particularly human response to attack indications) by using common tools and techniques developed by attackers. This testing is highly recommended for complex or critical systems. Penetration testing can be an invaluable technique to any organization's information security program. However, it is a very labor-intensive activity and requires great expertise to minimize the risk to targeted systems. At a minimum, it may slow the organization's network response time because of network mapping and vulnerability scanning. Furthermore, the possibility exists that systems may be damaged or rendered inoperable in the course of penetration testing. Although this risk is mitigated by the use of experienced penetration testers, it can never be fully eliminated."
          },
          {
            "text": "Penetration testing does offer the following benefits [NIST02b]:"
          },
          {
            "text": "Tests the network using the same methodologies and tools employed by attackers\nVerifies whether vulnerabilities exist\nGoes beyond surface vulnerabilities and demonstrates how these vulnerabilities can be exploited iteratively to gain greater access\nDemonstrates that vulnerabilities are not purely theoretical\nProvides the \"realism\" necessary to address security issues\nAllows for testing of procedures and susceptibility of the human element to social engineering."
          }
        ]
      }
    ]
  }
]