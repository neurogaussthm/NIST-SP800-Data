[
  "# Abstract\nThis document provides a definition of and terminology for approximate matching. Approximate matching is a promising technology designed to identify similarities between two digital artifacts. It is used to find objects that resemble each other or to find objects that are contained in another object. This can be very useful for filtering data for security monitoring, digital forensics, or other applications. The purpose of this document is to provide a definition and terminology to describe approximate matching in order to promote discussion, research, tool development and tool acquisition.\n\n# Introduction\n\n## 1.1 Purpose and Scope\nApproximate matching is a promising technology designed to identify similarities between two digital artifacts. It is used to find objects that resemble each other or to find objects that are contained in another object. This can be very useful for filtering data for security monitoring, digital forensics, or other applications. The purpose of this document is to provide a definition and terminology to describe approximate matching in order to promote discussion, research, tool development and tool acquisition. Approximate matching has the potential to provide valuable filtering in a world inundated with information, but the technique is not widely used. The goal of the document is to provide infrastructure to support advances in approximate matching and its use in forensics and security.\n\n## 1.2 Audience\nThe intended audience of this document is security digital forensics programmers and other technical professionals with a need to determine, build, or use technology to identify similarity.",
  "# Introduction\n\n## 1.1 Purpose and Scope\nApproximate matching is a promising technology designed to identify similarities between two digital artifacts. It is used to find objects that resemble each other or to find objects that are contained in another object. This can be very useful for filtering data for security monitoring, digital forensics, or other applications. The purpose of this document is to provide a definition and terminology to describe approximate matching in order to promote discussion, research, tool development and tool acquisition. Approximate matching has the potential to provide valuable filtering in a world inundated with information, but the technique is not widely used. The goal of the document is to provide infrastructure to support advances in approximate matching and its use in forensics and security.\n\n## 1.2 Audience\nThe intended audience of this document is security digital forensics programmers and other technical professionals with a need to determine, build, or use technology to identify similarity.\n\n# 2. Definition and terminology\nApproximate matching is a generic term describing any technique designed to identify similarities between two digital artifacts. In this context, an artifact (or an object) is defined as an arbitrary byte sequence, such as a file, which has some meaningful interpretation. Different approximate matching methods may operate at different levels of abstraction. At the lowest level, generic techniques may detect the presence of common byte sequences (substrings) without any attempt to interpret the artifacts. At higher levels, approximate matching can incorporate more abstract analysis. In general lower level methods are expected to be faster and more generic in their applicability, whereas higher level methods are typically more targeted and require more processing. One common approach in security and forensic analysis is to find identical objects using cryptographic hashing. Approximate matching can be viewed as a generalization of that idea in that, instead of providing a yes/no {0, 1} answer to a comparison, it provides a range of outcomes, [0, 1], with the result interpreted as a measure of similarity.\n\n## 2.1 Use cases# Similarity Queries",
  "## 2.1 Use cases# Similarity Queries\n\nBroadly, there are two types of similarity queries that are of interest \u2013 resemblance and containment [1]. In the case of resemblance, we compare two similarly sized objects and interpret the result as a measure of the commonality between them; for example, two successive versions of a piece of code are likely to resemble each other substantially. When the compared objects differ in size significantly, such as a file and a whole-disk image, the test for commonality is interpreted as a containment query because it addresses the question of whether the large object contains the smaller one.\n\nAn approximate matching algorithm should address at least one of the following problems (divided according to whether the query type is (R)esemblance or (C)ontainment) [2, 3]:\n\n## Object similarity detection (R)\n\nIdentify related artifacts, e.g., different versions of a document.\n\n## Cross correlation (R)\n\nIdentify artifacts that share a common object, e.g., a Microsoft Word document and a PDF document containing the same image, or other embedded object.\n\n## Embedded object detection (C)\n\nIdentify a given object inside an artifact, e.g., an image within a compound document or an executable inside a memory capture.\n\n## Fragment detection (C)\n\nIdentify the presence of traces/fragments of a known artifact, e.g., identify the presence of a file in a network stream based on individual packets.\n\nIn most analytical scenarios, approximate matching is used to filter data in, or out, based on a known reference set. In security monitoring applications, approximate matching could potentially be used to blacklist known bad artifacts, and (by extension) anything closely resembling them. However, approximate matching is not nearly as useful when it comes to whitelisting artifacts as malicious content can often be quite similar to benign content; e.g., a backdoored ssh server would approximately match a regular one.\n\n## 2.2 Terminology",
  "## Embedded object detection (C)\n\nIdentify a given object inside an artifact, e.g., an image within a compound document or an executable inside a memory capture.\n\n## Fragment detection (C)\n\nIdentify the presence of traces/fragments of a known artifact, e.g., identify the presence of a file in a network stream based on individual packets.\n\nIn most analytical scenarios, approximate matching is used to filter data in, or out, based on a known reference set. In security monitoring applications, approximate matching could potentially be used to blacklist known bad artifacts, and (by extension) anything closely resembling them. However, approximate matching is not nearly as useful when it comes to whitelisting artifacts as malicious content can often be quite similar to benign content; e.g., a backdoored ssh server would approximately match a regular one.\n\n## 2.2 Terminology\n\nAlthough the common language definition of 'similarity' is sufficient to give an intuitive sense of the term, the multitude of ways in which two artifacts can be said to be similar poses a challenge when attempting to describe the purpose and behavior of approximate matching algorithms. For example, two strings 'ababa' and 'cdcdc' might be considered similar in that they both have five characters ranging over two alternating values, or they might be treated as dissimilar because they have no common characters. To resolve this ambiguity, approximate matching algorithms define similarity in terms of features that represent characteristics of the artifacts pertinent to the algorithm's method of comparison.\n\n### Features\n\nThe basic elements through which artifacts are compared. Comparison of two features always yields a binary {0, 1} outcome indicating a match or non-match; because features are defined as the most basic comparison unit that the algorithm considers, partial matches are not permitted. Generally, a feature can be any value derived from an artifact. Each approximate matching algorithm must define the structure of its features and the method by which they are derived.# CURRENT_PAGE_RAW_OCR_TEXT\n\n## For\n\nFor example, an algorithm might define a feature as a (byte, offset) pair produced by reading the value of a byte and storing it along with the offset at which it was read.\n\n## Feature set",
  "### Features\n\nThe basic elements through which artifacts are compared. Comparison of two features always yields a binary {0, 1} outcome indicating a match or non-match; because features are defined as the most basic comparison unit that the algorithm considers, partial matches are not permitted. Generally, a feature can be any value derived from an artifact. Each approximate matching algorithm must define the structure of its features and the method by which they are derived.# CURRENT_PAGE_RAW_OCR_TEXT\n\n## For\n\nFor example, an algorithm might define a feature as a (byte, offset) pair produced by reading the value of a byte and storing it along with the offset at which it was read.\n\n## Feature set\n\nThe set of all features associated with a single artifact is its feature set. Each algorithm must include a criteria by which candidate features are selected for inclusion in this set. For example, an algorithm might select all the (byte, offset) pairs produced by reading every 16th byte in the artifact.\n\n## Similarity\n\nThe similarity of two artifacts, as measured by a particular approximate matching algorithm, is defined as an increasing monotonic function of the number of matching features contained in their respective feature sets.\n\nBased on the level of abstraction of the similarity analysis performed, approximate matching methods can be placed in one of three main categories [4]:\n\n### Bytewise matching\n\nBytewise matching relies only on the sequences of bytes that make up a digital object, without reference to any structures within the data stream, or to any meaning the byte stream may have when appropriately interpreted. Such methods have the widest applicability as they can be applied to any piece of data; however, they also carry the implicit assumption that artifacts that humans perceive as similar have similar byte-level encodings. This assumption is not universally valid. Analyst expertise is necessary to evaluate the significance of a byte-level match.\n\n### Syntactic matching\n\nSyntactic matching uses internal structures present in digital objects. For example, the structure of a TCP network packet is defined by an international standard and matching tools can make use of this structure during network packet analysis to match the source, destination or content of the packet. Syntax-sensitive similarity measurements are specific to a particular class of objects that share an encoding but require no interpretation of the content to produce meaningful results.\n\n### Semantic matching",
  "### Bytewise matching\n\nBytewise matching relies only on the sequences of bytes that make up a digital object, without reference to any structures within the data stream, or to any meaning the byte stream may have when appropriately interpreted. Such methods have the widest applicability as they can be applied to any piece of data; however, they also carry the implicit assumption that artifacts that humans perceive as similar have similar byte-level encodings. This assumption is not universally valid. Analyst expertise is necessary to evaluate the significance of a byte-level match.\n\n### Syntactic matching\n\nSyntactic matching uses internal structures present in digital objects. For example, the structure of a TCP network packet is defined by an international standard and matching tools can make use of this structure during network packet analysis to match the source, destination or content of the packet. Syntax-sensitive similarity measurements are specific to a particular class of objects that share an encoding but require no interpretation of the content to produce meaningful results.\n\n### Semantic matching\n\nSemantic matching uses contextual attributes of the digital object to interpret the artifact in a manner that more closely corresponds with human perceptual categories. For example, perceptual hashes allow the matching of visually similar images and are unconcerned with the low-level details of how the images are persistently stored. Semantic methods tend to provide the most specific results but also tend to be the most computationally expensive ones.\n\nIn current literature, researchers use a number of terms to refer to various approximate matching methods: fuzzy hashing and similarity hashing denote bytewise approximate matching; perceptual hashing and robust hashing denote semantic approximate matching. There is no widely-used pre-existing terminology for syntactic approximate matching as it is mostly viewed as pre-processing (to separate the features) before hashing, or applying a bytewise approximate matching algorithms. For example, network flows are usually reconstructed before any processing is done on them.\n\n## Bytewise approximate matching algorithms\n\nBytewise approximate matching algorithms work in two phases. In the first, a similarity digest representation (also referred to as a signature or fingerprint) is generated from the original data. In the second phase, digests are compared to produce a similarity score.\n\n### More precisely:\n\n**Similarity digest.** A similarity digest is a (compressed) representation of the original data.# Object's Feature Set",
  "In current literature, researchers use a number of terms to refer to various approximate matching methods: fuzzy hashing and similarity hashing denote bytewise approximate matching; perceptual hashing and robust hashing denote semantic approximate matching. There is no widely-used pre-existing terminology for syntactic approximate matching as it is mostly viewed as pre-processing (to separate the features) before hashing, or applying a bytewise approximate matching algorithms. For example, network flows are usually reconstructed before any processing is done on them.\n\n## Bytewise approximate matching algorithms\n\nBytewise approximate matching algorithms work in two phases. In the first, a similarity digest representation (also referred to as a signature or fingerprint) is generated from the original data. In the second phase, digests are compared to produce a similarity score.\n\n### More precisely:\n\n**Similarity digest.** A similarity digest is a (compressed) representation of the original data.# Object's Feature Set\n\nAn object\u2019s feature set that is suitable for comparison with other similarity digests created by the same algorithm. In most cases, the digest is much smaller than the original artifact and the original object is not recoverable from the digest.\n\n## Core Functions\n\nEvery bytewise approximate matching technique requires at least two core functions:\n\n### Feature Extraction Function\n\nIdentifies and extracts features/attributes from each digital artifact. The mechanism by which features are picked and interpreted depends on the approximate matching algorithm. The representation of this collection is the similarity digest of the object.\n\n### Similarity Function\n\nCompares two similarity digests and outputs a score. The recommended approach is to assign a score \\( s \\) in the \\( 0 \\leq s \\leq 1 \\) range, where 0 indicates no similarity and 1 indicates high similarity. This score represents a normalized estimate of the number of matching features in the feature sets corresponding to the artifacts from which the similarity digests were created.\n\n## Normalization Strategy\n\nThe similarity function can follow one of two normalization strategies, depending on whether the algorithm describes resemblance or containment. For resemblance queries, the number of matching features will be weighed against the total number of features in both objects. In the case of containment queries, the algorithm may disregard unmatched features in the larger of the objects' two-feature sets.",
  "### Similarity Function\n\nCompares two similarity digests and outputs a score. The recommended approach is to assign a score \\( s \\) in the \\( 0 \\leq s \\leq 1 \\) range, where 0 indicates no similarity and 1 indicates high similarity. This score represents a normalized estimate of the number of matching features in the feature sets corresponding to the artifacts from which the similarity digests were created.\n\n## Normalization Strategy\n\nThe similarity function can follow one of two normalization strategies, depending on whether the algorithm describes resemblance or containment. For resemblance queries, the number of matching features will be weighed against the total number of features in both objects. In the case of containment queries, the algorithm may disregard unmatched features in the larger of the objects' two-feature sets.\n\nBecause features and feature sets can be arbitrarily complex and, furthermore, deal with byte-level structures to which meaning is not clearly assigned, the interpretation of the similarity score can prove challenging. To address this problem, some approximate matching algorithms make use of an empirically determined threshold value to attempt to correlate bytewise similarity scores with higher-level properties of interest. In such cases, the similarity score can be treated as a confidence score, where results above the threshold value are considered likely to exhibit common human-recognizable traits.\n\n## 2.3 Essential Requirements\n\nLike traditional hash functions, there are several defining characteristics that approximate matching functions should exhibit. Each algorithm should define how it incorporates each of these properties and how it satisfies the reporting requirements for those properties, where appropriate.\n\n### Similarity Preservation\n\nSimilarity digests must be constructed such that the outcome of a comparison between any two digests is uniquely determined by the similarity of the artifacts from which they were produced. That is, if \\( A' \\) is a similarity digest created from artifact \\( A \\) and \\( B' \\) is a similarity digest created from artifact \\( B \\), the results of comparing \\( A' \\) and \\( B' \\) should be uniquely determined by the similarity of \\( A \\) and \\( B \\).\n\n### Self-Evaluation\n\nThe similarity measure should be accompanied by a measure of the accuracy of the matching technique under the circumstances in which it is used, e.g., a margin of error or confidence level. The description of the output score should also state whether a score of 1 indicates an exact match.\n\n### Compression",
  "### Similarity Preservation\n\nSimilarity digests must be constructed such that the outcome of a comparison between any two digests is uniquely determined by the similarity of the artifacts from which they were produced. That is, if \\( A' \\) is a similarity digest created from artifact \\( A \\) and \\( B' \\) is a similarity digest created from artifact \\( B \\), the results of comparing \\( A' \\) and \\( B' \\) should be uniquely determined by the similarity of \\( A \\) and \\( B \\).\n\n### Self-Evaluation\n\nThe similarity measure should be accompanied by a measure of the accuracy of the matching technique under the circumstances in which it is used, e.g., a margin of error or confidence level. The description of the output score should also state whether a score of 1 indicates an exact match.\n\n### Compression\n\nA compact similarity digest is desired as it normally allows a faster comparison and requires less storage space. In the best case, it will have a fixed length like the output of traditional hash functions. If the efficiency and reliability of the results remain unchanged, then a shorter similarity digest is preferable.\n\n### Ease of Computation\n\nFirst, the algorithm description should include the results.# CURRENT_PAGE_RAW_OCR_TEXT\n\n## of testing the runtime efficiency of the feature extraction function and of the similarity function.\n\nThe former might be expressed relative to a standard hashing algorithm, such as SHA-1.\n\nSecond, the algorithm description should state the theoretical complexity for a similarity digest comparison in big O notation. For instance, common lookup complexities for comparing a single digest against a database with n entries, are:\n\n- O(1) for fixed-length digests stored in hash tables (e.g. dictionaries)\n- O(log2 n) for fixed-length digests stored in binary trees or a sorted list\n- O(n) for fixed-length digests stored in an unsorted list, or other scenarios in which no indexing or sorting is possible\n\n## 2.4 Reliability of results\n\nThe reliability of the results for a given approximate matching technique depends on three factors. Each algorithm should define how it incorporates these factors and how it satisfies their reporting requirements.\n\n### Sensitivity & robustness:",
  "## of testing the runtime efficiency of the feature extraction function and of the similarity function.\n\nThe former might be expressed relative to a standard hashing algorithm, such as SHA-1.\n\nSecond, the algorithm description should state the theoretical complexity for a similarity digest comparison in big O notation. For instance, common lookup complexities for comparing a single digest against a database with n entries, are:\n\n- O(1) for fixed-length digests stored in hash tables (e.g. dictionaries)\n- O(log2 n) for fixed-length digests stored in binary trees or a sorted list\n- O(n) for fixed-length digests stored in an unsorted list, or other scenarios in which no indexing or sorting is possible\n\n## 2.4 Reliability of results\n\nThe reliability of the results for a given approximate matching technique depends on three factors. Each algorithm should define how it incorporates these factors and how it satisfies their reporting requirements.\n\n### Sensitivity & robustness:\n\nThe algorithm should provide some measure of its robustness. A technique's robustness will define the operating conditions in which it can function effectively, also called its performance envelope. For example, robustness addresses the minimum and maximum sizes of objects that an algorithm can reliably distinguish between.\n\n### Precision & recall:\n\nThe algorithm should include a description of the methods used to determine its reliability and to select the test data. Specifically, it should indicate whether test data is culled from existing collections or developed solely to support testing. Test results may include precision, recall, F-score, and other relevant measures of effectiveness.\n\n### Security of results:\n\nThe algorithm should indicate whether it includes security properties designed to prevent attacks. Such attacks include manipulation of the matching technique or input data such that a data object appears dissimilar to another object to which it is similar or similar to another object with which it has little in common.\n\n## 3. Testing bytewise approximate matching algorithms\n\nThe section gives a general overview of the motives and methods behind testing approximate matching algorithms. Defining a universal testing criteria is difficult; the effectiveness of a given approximate matching algorithm depends largely on the particular task against which it is evaluated. As such, rather than attempt to enumerate a comprehensive evaluation strategy, we focus on key considerations that arise during test design and evaluation. We also describe several properties of approximate matching algorithms that may be useful for characterizing their behavior.\n\n### 3.1 Motivation",
  "### Security of results:\n\nThe algorithm should indicate whether it includes security properties designed to prevent attacks. Such attacks include manipulation of the matching technique or input data such that a data object appears dissimilar to another object to which it is similar or similar to another object with which it has little in common.\n\n## 3. Testing bytewise approximate matching algorithms\n\nThe section gives a general overview of the motives and methods behind testing approximate matching algorithms. Defining a universal testing criteria is difficult; the effectiveness of a given approximate matching algorithm depends largely on the particular task against which it is evaluated. As such, rather than attempt to enumerate a comprehensive evaluation strategy, we focus on key considerations that arise during test design and evaluation. We also describe several properties of approximate matching algorithms that may be useful for characterizing their behavior.\n\n### 3.1 Motivation\n\nThere are at least three major motivations for testing bytewise approximate matching algorithms. The first and most basic is to understand the classes of objects that the algorithm identifies as similar. For example, an algorithm may define its feature set such that it measures similarity as a function of the number of certain common byte sequences shared between two digital artifacts. Because this comparison occurs at a very low level, its semantic interpretation is not obvious. Testing across various tasks and types of data is necessary to determine whether# CURRENT_PAGE_RAW_OCR_TEXT\n\n## such an algorithm is useful for identifying, say, html documents that contain similar content, or pdfs with the same embedded font.\n\nSecond, as an algorithm is developed, a standard suite of tests, such as those proposed by Breitinger, Stivaktakis, and Baier [3], allows new versions to be compared against previous versions. Testing against a fixed set of tasks helps to highlight any changes in the algorithm's similarity score as a function of its input. Improvements in speed or space efficiency can also be measured.",
  "## such an algorithm is useful for identifying, say, html documents that contain similar content, or pdfs with the same embedded font.\n\nSecond, as an algorithm is developed, a standard suite of tests, such as those proposed by Breitinger, Stivaktakis, and Baier [3], allows new versions to be compared against previous versions. Testing against a fixed set of tasks helps to highlight any changes in the algorithm's similarity score as a function of its input. Improvements in speed or space efficiency can also be measured.\n\nFinally, testing allows approximate matching algorithms to be compared to each other. The use of a standard suite of tasks can be beneficial in this testing scenario also. Additional measures must be taken, however, to ensure that tests produce comparable results. Because the algorithms define their similarity score independently, direct comparison of scores is rarely meaningful. One solution for this problem is to use a threshold or some other criteria to translate scores from their [0,1] range back to a binary {0,1} output which can be evaluated against known ground truth to produce confusion matrices (Roussev's comparison of sdhash and ssdeep offers an example of this approach [2]). Alternately, the tester might define other high-level criteria for the expected behavior of an algorithm in a specific use case of interest, and evaluate the algorithms against this expectation.\n\n## 3.2 Test Data\n\nApproximate matching algorithms can be tested against either controlled (synthetic) data [7] or real data (i.e., data originally created for purposes other than testing). The former typically consists of randomly generated blocks or files; its main advantage is that ground truth is constructed and, therefore, precisely known. This allows tests to be run automatically and the results to be interpreted with standard statistical measures.\n\nThe drawback to using randomly generated data is that it is not necessarily representative. For example, random data tends to be unique, high-entropy and internally varied. These properties may be appropriate for use cases involving compressed or encrypted data, but could prove misleading for other file types (such as text documents). A potential workaround is to develop more sophisticated randomization that preserves the characteristic properties of the target file type.",
  "## 3.2 Test Data\n\nApproximate matching algorithms can be tested against either controlled (synthetic) data [7] or real data (i.e., data originally created for purposes other than testing). The former typically consists of randomly generated blocks or files; its main advantage is that ground truth is constructed and, therefore, precisely known. This allows tests to be run automatically and the results to be interpreted with standard statistical measures.\n\nThe drawback to using randomly generated data is that it is not necessarily representative. For example, random data tends to be unique, high-entropy and internally varied. These properties may be appropriate for use cases involving compressed or encrypted data, but could prove misleading for other file types (such as text documents). A potential workaround is to develop more sophisticated randomization that preserves the characteristic properties of the target file type.\n\nThe obvious advantage of using real data is that the results can be directly related to practical applications. However, the challenges of defining a representative sample, establishing the ground truth, and running experiments at scale (without a human in the loop) are non-trivial. See Appendix A1 Example test methodologies for both controlled and real data.\n\n## 3.3 Properties of Special Interest\n\nThe following properties may be especially useful for testing and comparing approximate# Matching Algorithms\n\nThis list is not meant to be exhaustive. Note that the properties enumerated here are adapted from those proposed by Breitinger, Stivaktakis, and Baier [3]. See their work on the FRASH testing framework for an example of a complete description (including full implementation details and suggested test values) of a testing framework that incorporates these properties.\n\n## 3.3.1 Efficiency\n\nThe efficiency of an algorithm is measured in terms of the space and time it requires; the latter property is further subdivided according to time needed for generating similarity digests and time needed for comparing them.\n\n### Space Efficiency\n\nTraditional hash functions return a fixed length fingerprint; in contrast, the length of similarity digests is sometimes variable and proportional to the input length. If the digest is of variable length, space efficiency measures the ratio between input and the digest and returns a percentage value.\n\n### Generation Efficiency\n\nGeneration efficiency measures the throughput of an algorithm when producing a similarity digest from raw input. To enable useful comparisons across different architectures, it is recommended that the throughput of a standard algorithm implementation, such as SHA-1 in openssh, be included as a reference point.",
  "## 3.3.1 Efficiency\n\nThe efficiency of an algorithm is measured in terms of the space and time it requires; the latter property is further subdivided according to time needed for generating similarity digests and time needed for comparing them.\n\n### Space Efficiency\n\nTraditional hash functions return a fixed length fingerprint; in contrast, the length of similarity digests is sometimes variable and proportional to the input length. If the digest is of variable length, space efficiency measures the ratio between input and the digest and returns a percentage value.\n\n### Generation Efficiency\n\nGeneration efficiency measures the throughput of an algorithm when producing a similarity digest from raw input. To enable useful comparisons across different architectures, it is recommended that the throughput of a standard algorithm implementation, such as SHA-1 in openssh, be included as a reference point.\n\n### Comparison Efficiency\n\nThe comparison efficiency measures the rate at which similarity digests can be compared. It is useful to have both a formal analysis, which provides the theoretical complexity of the comparison (in O-notation) and an empirical evaluation based on a reference data set.\n\nA related property of note is the ability of the algorithm to leverage parallel computational resources; these may include conventional multi-core CPU architectures, as well as massively parallel ones, such as GPUs. To that end, tests should include scalability analysis, which shows speedup as a function of available hardware concurrency.\n\n## 3.3.2 Sensitivity\n\nSensitivity is a measure of the ability of an approximate matching algorithm to find correlations among objects based on fine-grained commonality\u2014the smaller the features being correlated, the more sensitive the algorithm is. Clearly, there is a threshold below which the sensitivity will be too high and all objects will appear similar; it is up to the algorithm designer to identify that threshold and incorporate it into the implementation.\n\nTwo examples of tests for measuring an algorithm's sensitivity are fragment detection and single-common-block correlation.\n\n- **Fragment Detection**: Fragment detection quantifies the length of the shortest sample from a data object, for which the similarity tool reliably correlates the sample and the whole object. Common uses include correlating a disk block, or network packet to file.\n\n- **Single-Common-Block Correlation**: The single-common-block correlation test is designed to characterize the behavior of an algorithm in the case where two files share a single common object. That is, given two files f1 and f2 that share a single common object.# Common",
  "Two examples of tests for measuring an algorithm's sensitivity are fragment detection and single-common-block correlation.\n\n- **Fragment Detection**: Fragment detection quantifies the length of the shortest sample from a data object, for which the similarity tool reliably correlates the sample and the whole object. Common uses include correlating a disk block, or network packet to file.\n\n- **Single-Common-Block Correlation**: The single-common-block correlation test is designed to characterize the behavior of an algorithm in the case where two files share a single common object. That is, given two files f1 and f2 that share a single common object.# Common\n\n## Object O\n(but are otherwise dissimilar), what is the smallest O for which the similarity tool reliably correlates the two targets?\n\n### 3.3.3 Robustness\nRobustness is a measure of the ability of an approximate matching algorithm to find correlation among related objects in the presence of noise and routine transformations. Common transformations include internal fragmentation (e.g., during network transmission) and misalignment (adding content during artifact editing). Alignment robustness and white noise resistance are examples of tests that measure an algorithm's robustness.\n\n- **Alignment robustness.** Alignment robustness is an attempt to quantify the sensitivity of an algorithm to alignment shifts. Specifically, the test analyzes the impact of inserting sequences of bytes at the beginning of one of the compared artifacts.\n\n- **White noise resistance.** This test measures the amount of (uniformly) random noise that can be added to an object before the approximate matching algorithm becomes unable to correlate the original and the modified version. For example, for ssdeep it was shown that a few changes distributed over the input are sufficient to prevent a match."
]