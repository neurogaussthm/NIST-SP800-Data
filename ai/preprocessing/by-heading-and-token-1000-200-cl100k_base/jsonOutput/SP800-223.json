[
  {
    "title": "Abstract",
    "subsections": [
      {
        "content": "Security is an essential component of high-performance computing (HPC). HPC systems often differ based on the evolution of their system designs, the applications they run, and the missions they support. An HPC system may also have its own unique security requirements, follow different security guidance, and require tailored security solutions. Their complexity and uniqueness impede the sharing of security solutions and knowledge. This NIST Special Publication aims to standardize and facilitate the information and knowledge-sharing of HPC security using an HPC system reference architecture and key components as the basis of an HPC system lexicon. This publication also analyzes HPC threats, considers current HPC security postures and challenges, and makes best-practice recommendations."
      }
    ]
  },
  {
    "title": "1. Introduction",
    "subsections": [
      {
        "content": "In 2015, Executive Order 13702 established the National Strategic Computing Initiative (NSCI) to maximize the benefits of high-performance computing (HPC) for economic competitiveness and scientific discovery. The ability to process large volumes of data and perform complex calculations at high speeds is a key part of the Nation's vision for maintaining its global competitive edge."
      },
      {
        "content": "Security is essential to achieving the anticipated benefits of HPC. HPC systems bear some resemblance to enterprise IT computing, which allows for the effective application of traditional IT security solutions. However, they also have significant differences. An HPC system is designed to maximize performance, so its architecture, hardware components, software stacks, and working environment are very different from enterprise IT. Additionally, most HPC systems serve multiple projects and user communities, leading to more complex security concerns than those encountered in enterprise systems. As such, security solutions must be tailored to the HPC system's requirements. Furthermore, HPC systems are often different from one another due to the evolution of their system designs, the applications they run, and the missions they support. An HPC system frequently has its own unique method of applying security requirements and may follow different security guidance, which can impede the sharing of security solutions and knowledge."
      },
      {
        "content": "This NIST Special Publication aims to standardize and facilitate the sharing of HPC security information and knowledge through the development of an HPC system reference architecture and key components, which are introduced as the basics of the HPC system lexicon. The reference architecture divides an HPC system into four function zones. A zone-based HPC reference architecture captures the most common features across the majority of HPC systems and segues into HPC system threat analysis. Key HPC security characteristics and use requirements are laid out alongside the major threats faced by the system and individual.\n```# CURRENT_PAGE_RAW_OCR_TEXT"
      },
      {
        "content": "function zones. HPC security postures, challenges, recommendations, and some currently recognized variants are also included. This publication is intended to be a conceptual guide, not a checklist of requirements. Finally, emerging technologies \u2014 such as novel networking technology, data storage solutions, hardware acceleration technology, and others \u2014 have the potential to influence the HPC reference architecture and security posture. Consequently, this document will be updated as needed."
      },
      {
        "title": "2. HPC System Reference Architecture and Main Components",
        "content": [
          {
            "text": "The HPC system is complex and evolving, and a common lexicon can help describe and identify an HPC system's architecture, critical elements, security threats, and potential risks. An HPC system is divided into four function zones:"
          },
          {
            "text": "The high-performance computing zone consists of a pool of compute nodes connected by one or more high-speed networks. The high-performance computing zone provides key services specifically designed to run parallel jobs at scale.\n\n\nThe data storage zone comprises one or multiple high-speed parallel file systems that provide data storage service for user data. The high-speed parallel file systems are designed to store very large data sets and provide fast access to data for reading and writing.\n\n\nThe access zone has one or more nodes that are connected to external networks, such as the broader organizational network or the internet. This zone provides the means for authenticating and authorizing the access and connections of users and administrators. The access zone provides various services, including interactive shells, web-based portals, data transfer, data visualization, and others.\n\n\nThe management zone comprises multiple management nodes and/or cloud service clusters through which HPC management services are provided. The management zone allows HPC system administrators to configure and manage the HPC system, including the configuration of compute nodes, storage, networks, provisioning, identity management, auditing, system monitoring, and vulnerability assessment. Various management software modules (e.g., job schedulers, workflow management, and the Domain Name System [DNS]) run in the management zone."
          },
          {
            "text": "The high-performance computing zone consists of a pool of compute nodes connected by one or more high-speed networks. The high-performance computing zone provides key services specifically designed to run parallel jobs at scale."
          },
          {
            "text": "The data storage zone comprises one or multiple high-speed parallel file systems that provide data storage service for user data. The high-speed parallel file systems are designed to store very large data sets and provide fast access to data for reading and writing."
          },
          {
            "text": "The access zone has one or more nodes that are connected to external networks, such as the broader organizational network or the internet. This zone provides the means for authenticating and authorizing the access and connections of users and administrators. The access zone provides various services, including interactive shells, web-based portals, data transfer, data visualization, and others."
          },
          {
            "text": "The management zone comprises multiple management nodes and/or cloud service clusters through which HPC management services are provided. The management zone allows HPC system administrators to configure and manage the HPC system, including the configuration of compute nodes, storage, networks, provisioning, identity management, auditing, system monitoring, and vulnerability assessment. Various management software modules (e.g., job schedulers, workflow management, and the Domain Name System [DNS]) run in the management zone."
          },
          {
            "text": "The HPC system reference architecture is depicted in Fig. 1."
          },
          {
            "subsection": "2.1. Main Components",
            "text": []
          },
          {
            "text": "An HPC cluster consists of a collection of independent computing systems, called compute nodes, which are interconnected via high-speed networks. Compute nodes have the same components as a laptop, desktop, or server, including central processing units (CPUs), memory, disk space, and networking interface cards. However, they are architecturally tuned for the requirements of HPC workloads. In some HPC architectures, a compute node may not have local disks and instead use the data storage services of remote storage servers. In addition, there may be different types of nodes for different types of tasks, and some compute nodes are equipped with hardware accelerators to speed up specific applications. For# CURRENT_PAGE_RAW_OCR_TEXT"
          }
        ]
      },
      {
        "title": "instance, compute",
        "content": [
          {
            "text": "nodes often utilize graphics processing units (GPUs) [1] to accelerate modeling\nand simulation or AI and machine learning (ML) model training. An HPC compute node has its own software stack installed (e.g., operating system [OS], compilers, software libraries, etc.) to support applications. The installation and configuration of the software stacks are cluster-wide, centrally managed, and controlled by the management zone. The number of compute nodes in an HPC system ranges from a few nodes to hundreds and even tens of thousands of nodes."
          },
          {
            "text": "A frequent requirement of HPC networking, which interconnects computer nodes and file systems, is to have high throughput and low latency so that the compute nodes and parallel file system (PFS) in the data storage zone can work as one supercomputer. The exact requirements for bandwidth and latency are dictated by the intended workload. HPC networking often employs specifically designed protocols, networking cards, processor nodes, and switches to optimize network performance. The popular HPC interconnect networking includes InfiniBand [2], Omni-Path [3], Slingshot [4], Ethernet, and others."
          },
          {
            "text": "A high-performance computing zone typically utilizes non-high-performance communication networks, like Ethernet, as cluster internal networks that connect the high-performance computing zone with the management zone and access zone for traffic associated with maintenance activities as opposed to HPC traffic."
          },
          {
            "subsection": "2.1.2. Components of the Data Storage Zone",
            "text": []
          },
          {
            "text": "Several different classes of storage systems may be present inside of the data storage zone. In general, storage systems within this zone cannot be effectively separated from the HPC resources that they support from an administrative privilege perspective. Typical classes of storage found within this zone include PFSs and archival file systems that support campaign storage and protect against data loss. HPC systems may have other file systems that store non-user data. For instance, the management zone often has its own file system that stores the OS images and configuration files. In that case, the file system is included in the corresponding function zone."
          },
          {
            "text": "HPC applications' initial data, intermediate results, and results are stored in the data storage zone and can be accessed during the application runtime and after the application's completion. External HPC users can also access user data through the login nodes and/or data transfer nodes in the access zone."
          },
          {
            "text": "The storage capacity of these file systems is often measured in petabytes and can reach up to exabytes. File systems within the data storage zone will generally use a transport mechanism appropriate to the tier. For example, high-bandwidth file systems may be attached to the HPC resource's high-performance network, while lower bandwidth file systems may use 10 Gbps or 100 Gbps Ethernet. Access control for most HPC file systems is enforced by the# Operating System Software of Nodes"
          },
          {
            "text": "Operating system software of nodes on which these file systems are mounted. As such, file systems should not be mounted outside of their security boundaries. A rogue system that can mount a file system will have complete control of all file system data, can spoof packets on the high-speed network, and can possibly gain privileges elsewhere within other zones of the HPC security enclave."
          }
        ]
      },
      {
        "title": "2.1.3. Parallel File System",
        "content": [
          {
            "text": "Since HPC workloads can vary significantly, a PFS is often required to support read-intensive and write-intensive applications with sequential and random-access patterns at speeds of up to terabytes per second. Commonly seen file systems include Lustre [5], GPFS [6], and IBM Spectrum Scale [7]. During procurement, a PFS will typically be designed to hit a particular aggregate bandwidth target rather than a capacity requirement. These PFSs will typically consist of a cluster of systems to maintain metadata about files and locations as well as servers that act as storage targets. Clients that mount the file system will typically load the file system client software via a kernel module. Storage target servers will have backing storage arrays configured with dozens of disks in a redundant array of inexpensive disk (RAID) strategy."
          },
          {
            "text": "Both GPFS and Lustre-based PFSs are prone to performance degradation when a certain capacity threshold is reached. These file systems may be regularly pruned of unwanted files with a strategy decided by the file system administrators. Some deployments will delete files older than a certain age, which forces HPC users to transfer job output to a longer-term file system, such as campaign storage. PFSs tend to be somewhat unreliable, depending on the types of activities being performed by running jobs and users. Because these are distributed file systems, file-system software must solve distributed locking of files to ensure deterministic file updates when multiple clients are writing to the same file at once. Additionally, PFSs are susceptible to denial-of-service conditions even during legitimate user operations, such as listing a directory with millions of files or applications that perform poor file locking semantics."
          }
        ]
      },
      {
        "title": "2.1.4. Archival and Campaign Storage",
        "content": [
          {
            "text": "The term \"campaign\" is understood here as a collection of coordinated projects that are working toward a common set of goals and deliverables. Archival and campaign storage systems represent a class of storage that is more resilient to failure conditions than a PFS and is often less expensive per gigabyte. These advantages come at the cost of bandwidth and an increased latency of data transfer. While a PFS acts as a temporary short-term scratch file system, campaign storage supports the long-term storage needs of a project over its life cycle. Finished data products that support scientific publications or other high-value datasets may also be stored in an archival file system. The retention time for data on a# Campaign Storage File"
          },
          {
            "text": "The system is measured in years, while the retention of data within an archival storage system is measured in decades. Both campaign and archival storage systems might employ low-latency disks \u2014 such as solid-state drives (SSD) or non-volatile memory express (NVMe) drives within a small tier of storage that acts as a cache \u2014 and are backed by cheaper, higher capacity media, such as spinning disks and/or tape media."
          }
        ]
      },
      {
        "title": "2.1.5. Burst Buffer",
        "content": [
          {
            "text": "Burst buffer commonly refers to a caching mechanism for storage systems. For applications that require extremely low latency or high-bandwidth memory-to-disk data transfer during runtime, intermediate storage layers that contain \"burst buffers\" have been incorporated as brokers to primarily mitigate the effects of input/output (I/O) contention and the bandwidth burden on PFSs. These burst buffers can pre-fetch data from the PFS before a computing job begins and stage data out to a parallel file system after a computing job has completed. This saves job runtime that would normally be spent performing bulk I/O to the PFS and allows it to be spent on computation instead. Typical HPC infrastructures contain the following intermediate storage architectures:"
          },
          {
            "text": "Node-local burst buffer architectures: Each burst buffer is colocated with a corresponding HPC compute node. This is advantageous for its scalability and improves the checkpoint bandwidth because the aggregate bandwidth increases in proportion to the number of compute nodes.\nRemote-shared burst buffer architectures: Burst buffers are shared between multiple HPC compute nodes that are hosted on an I/O node. This is advantageous for facilitating the development, deployment, and maintenance of these architectures."
          },
          {
            "text": "There are also HPCs that can contain mixed burst buffer intermediate storage architectures, which combine the strengths of node-local and remote-shared burst buffer architectures."
          }
        ]
      }
    ]
  },
  {
    "title": "2.1.8. General Architecture and Characteristics",
    "subsections": [
      {
        "content": "One important characteristic of the management zone is that it has a separate security posture because non-privileged users do not need to access the management servers or services in a direct way. Privileged users responsible for configuring, maintaining, and operating the HPC system access the management zone servers and switches through extra security controls. For example, from the public-facing login nodes, they may go through a bastion host that is typically located in the management zone, or they may establish a virtual private network (VPN) with separate authentication and authorization or other appropriate security controls to reach the management zone. All systems are configured on networks that are not routed beyond the perimeter of the HPC system so that only nodes like compute nodes and storage nodes can access the services."
      },
      {
        "content": "The services provided by the management zone have clearly defined protocols and can be implemented as running on assigned hardware platforms or run as virtual machines (VMs) on a dedicated set of hardware resources. The fact that the management zone has a clear and separate security posture helps with risk assessment and the selection of controls to secure the management zone and manage the risk."
      }
    ]
  },
  {
    "title": "2.1.9. Basic Services",
    "subsections": [
      {
        "content": "The HPC resources inside of the computing and data storage zones need various services to operate. Examples include Domain Name Services (DNS) [14]; the Dynamic Host Configuration Protocol (DHCP) [15]; configuration definitions, authentication, and authorization services, such as those provided by an LDAP server [16]; and the Network Time Protocol (NTP) [17] for synchronization, log management, version-controlled repositories."
      },
      {
        "content": "The management zone includes storage systems to store configuration data, node images, current versions, development and test versions, and historical versions. Storing logs from the entire HPC system is also part of the management zone as well as the servers to process the logs and alert administrators of events, problems, and incidents. Many of these services will be implemented with high availability and failover capabilities to avoid failure of the HPC resources. The network switches for the management network and the fast interconnects are often managed as part of the management zone because non-privileged users do not need direct access to those resources."
      }
    ]
  },
  {
    "title": "2.1.10. Configuration Management",
    "subsections": [
      {
        "content": "Automated configuration management is crucial to ensure the stable operation of# CURRENT_PAGE_RAW_OCR_TEXT"
      },
      {
        "title": "complex",
        "content": [
          {
            "text": "systems like HPC. The systems that hold the configuration database and run the server to place configurations on compute nodes, storage servers, and network switches are part of the management zone. The nodes are subject to a regularly scheduled process to verify configuration and enforce consistency with what the configuration management nodes and databases specify."
          },
          {
            "text": "Often, the configuration management systems in the management zone have an even more restricted security posture than the management zone as a whole, with a smaller number of privileged users having access."
          },
          {
            "subsection": "2.1.11. HPC Scheduler and Workflow Management",
            "text": []
          },
          {
            "text": "Because of the distributed nature of HPC systems, requesting resources for given workloads is coordinated by a scheduler or workload manager, such as Slurm [18] or Kubernetes [19]. These services are run on servers in the management zone alongside the configurations and job logs. Non-privileged users access the scheduler through specific commands or an application programming interface (API). Access to the service is restricted to nodes within the HPC system perimeter. There may also be a web interface that provides a separate authenticated and authorized path for scheduling workloads, often within the strict constraints of certain application domains."
          },
          {
            "subsection": "2.1.12. HPC Software",
            "text": []
          },
          {
            "text": "In addition to the management software that installs, boots, configures, and manages HPC-related systems, HPC application codes rely on several layers of scientific and performance-enhancing libraries. The layers of software that are available to users are referred to as the software stack. The lower layers of this stack are typically focused on performance and include compilers, communication libraries, and user-space interfaces to HPC hardware components. The middle layer includes performance tools, math libraries, and data or computation abstraction layers. The top of the stack consists of end-user science or production applications."
          },
          {
            "text": "Each software product within this stack may require certain versions or variants of other products and have many dependencies. For example, Hierarchical Data Format Version 5 (HDF5) [20] is a scientific data formatting library with only seven dependencies, while Data Mining Classification and Regression Methods (rminer) [21] \u2014 an R-based data mining application \u2014 has 150 software dependencies. The full software stack can be split into three general categories that differ based on the maintainer: user software, facility software, and vendor software."
          },
          {
            "subsection": "2.1.13. User Software",
            "text": []
          },
          {
            "text": "Often, the end users themselves best understand how to tune their software to the bespoke hardware of an HPC system to ensure sufficient performance for their workload. Users regularly modify and recompile their software to enhance performance, fix bugs, and adapt to changes in# Current Page Raw OCR Text"
          }
        ]
      },
      {
        "title": "The Underlying Dependencies or Kernel Interfaces Over Time",
        "content": [
          {
            "text": "The sharing of user-built software between teams may be common. User software that is widely used is often open source and, therefore, subject to open-source software supply chain concerns. Continuous integration (CI) pipelines [22] and tests of scientific code on HPC platforms have recently become commonplace. Industry-standard identification of software weaknesses and the publication of Common Vulnerabilities and Exposures (CVEs) [23] is not routine, but the identification and remediation of performance regressions is generally a higher priority within the user community. There is a value-per-cycle trade-off for CI tests since cheaper cycles on commodity hardware may not expose bugs on much more expensive HPC resources. Complex test suites will eat into user allocations, and users and staff prefer that only a cardinal set of smoke tests run within user-developed testing pipelines on HPC systems."
          }
        ]
      },
      {
        "title": "2.1.14. Site-Provided Software and Vendor Software",
        "content": [
          {
            "text": "Site staff and administrators generally build applications and libraries that are most likely to be used. Tools such as Conda [24], EasyBuild [25], and Spack [26] are often used to manage the complexity of software dependency resolution. Staff may also wrap compiler and job submission utilities with custom scripts to collect usage information about software libraries, I/O read and write patterns, or other system telemetry that is useful for decision making. Vendor software includes low-level system tools to facilitate the running of other software. For instance, remote direct memory access, inter-node memory sharing, performance counters, temperature and power telemetry, and debugging are all vendor-provided software. Users can choose specific versions of installed vendor and site-provided software libraries by manipulating environment variables. Tools such as wrapper scripts or module files are usually provided to help users find and choose which versions of installed software to use."
          }
        ]
      },
      {
        "title": "2.1.15. Containerized Software in HPC",
        "content": [
          {
            "text": "A container is a software bundle that includes the application along with some or all of the dependencies, libraries, other binaries, and configuration files needed to run it. Containers provide self-contained, portable, and reproducible environments that abstract away the differences in OS distributions and underlying infrastructure. Containers can make applications more portable, easier to deploy, and easier to distribute. For instance, containers allow users to use a package manager (e.g., apt [27] or yum [28]) to install software without modifying the host system, effectively decoupling the application dependencies from the host operating system. Containers are executed by a container runtime, such as Docker [29], Containerd [30], Apptainer [31], or Charliecloud [32]. Container runtimes create an isolated execution space for the application by leveraging technologies like Linux Namespaces [33], Cgroups [34], and...# Seccomp"
          },
          {
            "text": "The runtime is responsible for preparing the execution environment, establishing an isolated execution namespace, and executing the application. Runtimes may handle other tasks, like obtaining and attaching the container image, mounting filesystems within the container, and creating pass-through access to devices. Container execution may be further abstracted using a container orchestration service, such as Kubernetes, OpenShift, or others."
          }
        ]
      },
      {
        "title": "3. HPC Threat Analysis",
        "content": [
          {
            "text": "HPC poses unique security and privacy challenges, and collaboration and resource-sharing are integral. For instance, scientific experiments frequently employ unique hardware, software, and configurations that may not be maintained or well-vetted or that present entirely new classes of vulnerabilities that are absent in more traditional environments. HPC can store large amounts of sensitive research data, personally identifiable information (PII), and intellectual property (IP) that need to be safeguarded. Finally, HPC data and computation are encumbered with a variety of different security and policy constraints that stem from the fact that HPC systems are often operated as shared resources with different user groups, each of which are required to operate under the goals and constraints set by their organizations. The solutions to protecting data, computation, and workflows must balance these trade-offs."
          },
          {
            "subsection": "3.1. Key HPC Security Characteristics and Use Requirements",
            "text": []
          },
          {
            "text": "HPC systems possess some unique security characteristics and distinctive use requirements that differentiate themselves from enterprise IT systems:"
          },
          {
            "text": "Tussles between performance and security: HPC users may consider security to be valuable only to the extent that it does not significantly slow down the HPC system and impede research. Ensuring the usability of security mechanisms with a tolerable performance penalty is therefore critical to adoption by the scientific HPC community.\n\n\nVarying security requirements for different HPC applications: Individual platforms, projects, and data may have significantly different security sensitivities and need to follow different security policies. An HPC system may need to enforce multiple security policies simultaneously.\n\n\nLimited resources for security tools: Most HPC systems are designed to devote their resources to maximizing performance rather than acquiring and operating security tools.\n\n\nOpen-source software and self-developed research software: Open-source software and self-developed research software are widely used in HPC. Open-source software is vulnerable to open-source software supply chain threats, while HPC software input data may be vulnerable to data supply chain threats. Self-developed software is susceptible to low software quality.\n\n\nGranular access control on databases: Since different research groups may have a need# CURRENT_PAGE_RAW_OCR_TEXT"
          },
          {
            "text": "Tussles between performance and security: HPC users may consider security to be valuable only to the extent that it does not significantly slow down the HPC system and impede research. Ensuring the usability of security mechanisms with a tolerable performance penalty is therefore critical to adoption by the scientific HPC community."
          },
          {
            "text": "Varying security requirements for different HPC applications: Individual platforms, projects, and data may have significantly different security sensitivities and need to follow different security policies. An HPC system may need to enforce multiple security policies simultaneously."
          },
          {
            "text": "Limited resources for security tools: Most HPC systems are designed to devote their resources to maximizing performance rather than acquiring and operating security tools."
          },
          {
            "text": "Open-source software and self-developed research software: Open-source software and self-developed research software are widely used in HPC. Open-source software is vulnerable to open-source software supply chain threats, while HPC software input data may be vulnerable to data supply chain threats. Self-developed software is susceptible to low software quality."
          },
          {
            "text": "Granular access control on databases: Since different research groups may have a need# CURRENT_PAGE_RAW_OCR_TEXT"
          },
          {
            "text": "To know for different portions of data, granular access control capabilities are necessary. Access control requirements may need to be dynamically adjusted as some scientific experiments may increase data needs based on the outcome of experiments."
          }
        ]
      },
      {
        "title": "3.2. Threats to HPC Function Zones",
        "content": [
          {
            "text": "The following subsections discuss threats to the four functional zones in the HPC reference architecture."
          },
          {
            "subsection": "3.2.1. Access Zone Threats",
            "text": []
          },
          {
            "text": "The access zone provides an interface for external users to access the HPC system and oversees the authentication and authorization of users. Among the four function zones, the access zone is the only one that is directly connected to the external networks. Hence, the nodes and their software stacks in this zone are susceptible to external attacks, such as denial-of-service (DoS) attacks, perimeter network scanning and sniffing (when not done as part of security practices), authentication attacks (e.g., brute force login attempts and password guessing), user session hijacking, and attacker-in-the-middle attacks. Some nodes are even subject to extra attacks due to their specific software stacks. For instance, a web server may be subject to website defacement, phishing, misconfiguration, and code injection attacks. The access zone also provides access to the file systems hosted in the data storage zone. It is important that permissions to directories and files are only given to authorized users. Applying multi-factor authentication (MFA) methods to HPC system access, which requires a user to provide one or more verification methods at login, is a proven method to mitigate the risk of unauthorized access."
          },
          {
            "text": "Authenticated users sometimes use external networks to download data or code for use inside of the HPC system, which introduces the risk of unintentionally downloading malicious content. The nodes in the access zone are usually configured to support limited computation (e.g., modest debugging). Access zone nodes are susceptible to computational resource abuse. The access zone is also often shared by multiple users. One user's activities, such as commands issued and jobs submitted, can be viewed by other users. A port opened by one user can potentially be used by others. Fortunately, the nodes in the access zone work similarly to enterprise servers, and general IT security tools and measures are available to harden the zone."
          },
          {
            "subsection": "3.2.2. Management Zone Threats",
            "text": []
          },
          {
            "text": "The management zone is responsible for managing the entire HPC system. It is connected to clustered internal networks through which other zones can be reached. It runs a plethora of system management, out-of-band hardware management, job scheduling, and workflow management software, all of which are susceptible to unique threats. Processes that run in the management zone, such as schedulers and data tiering or orchestration processes, act on behalf of users. These are privileged processes, and if they are...# CURRENT_PAGE_RAW_OCR_TEXT"
          }
        ]
      },
      {
        "title": "Compromised Privileges",
        "content": [
          {
            "text": "Compromised, it can lead to privilege escalation. Often, the only privilege boundaries that separate users are Portable Operating System Interface (POSIX) file system permissions and the enforcement of capabilities by the operating systems of HPC components. Due to the implied delegation of authority within distributed HPC and file systems, 'root' on a compute node may be equivalent to 'root' on all systems within the HPC zones, depending on configuration."
          },
          {
            "text": "Privilege escalation vulnerabilities are particularly impactful within an HPC environment. Only administrators with privileged access authorization are allowed to log into the management zone, where a privileged administrator logs into the access zone first and then logs into the management zone. A malicious user may attempt to log into the management zone. The management zone may also be implemented as a service running on a cloud via virtualization technologies. In such cases, the risks associated with the cloud also apply to the management zone."
          }
        ]
      },
      {
        "title": "3.2.3. High-Performance Computing Zone Threats",
        "content": [
          {
            "text": "The high-performance computing zone offers core computational functions in an HPC system. The compute nodes are shared by multiple users or tenants. The exploitation of multi-tenancy environments is a major threat (e.g., side-channel attacks, user data/program leakage, etc.). Other threat sources that often cause extreme resource consumption, performance degradation, or the outage of the HPC system entirely include accidental misconfiguration, software bugs introduced by user-developed software, and system abuse by running applications that are not aligned with the HPC mission. Container escape, side-channel attacks, and DoS can also be threats if virtualization technologies (e.g., containers) are used in the high-performance computing zone."
          },
          {
            "text": "As a security mitigating technology, the applications in HPC are mostly run in the user space, except for system calls that must run in the kernel with elevated privilege. Accelerators, high-performance interconnects, special protocols, and direct memory access between nodes are commonly used in the high-performance computing zone. Some of these technologies may not be thoroughly tested from a security point of view, and their speed, novelty, and complexity can make monitoring and detecting suspicious activity difficult. Direct memory access and communication between nodes may bypass the kernel, and the protections provided by the kernel (e.g., Security-Enhanced Linux [SELinux] [38]) are lost."
          }
        ]
      },
      {
        "title": "3.2.4. Data Storage Zone Threats",
        "content": [
          {
            "text": "Protecting the confidentiality, integrity, and availability (CIA) of user data is essential for the data storage zone. Data integrity can be compromised by malicious data deletion, corruption, pollution, or false data injection. Legitimate users may also mishandle sensitive data, leading to confidentiality breakdown. File metadata (e.g., file name, author, size, creation date) can also leak sensitive information about the files."
          },
          {
            "text": "HPC file systems in the data storage zone provide superior data access speed and much larger storage capacity compared to average enterprise file systems. Hard disk failure is a threat due...# CURRENT_PAGE_RAW_OCR_TEXT"
          },
          {
            "text": "to the large number of disks deployed in the data storage zone. Incident response and contingency planning controls may not be easy to implement, and file backup, recovery, and forensic imaging may become infeasible. The security measures that can be implemented on the enterprise file systems may take an unacceptably long time and degrade HPC file system performance in an unacceptable way."
          },
          {
            "text": "Providing data backup services is another challenge in HPC due to its large volume. By default, user data is often not backed up, and users are responsible for maintaining their own data copies. Inadvertent operations (e.g., accidentally deleting a file subdirectory) can cause the permanent loss of data, though making data READ ONLY is one way to combat such a risk. Some organizations offer backup services using their own HPC systems, but these systems may be in the same geographic locations and subject to the same environmental threats."
          },
          {
            "text": "Finally, the data stored in a HPC system may contain sensitive information, such as personally identifiable information (PII), patient health information (PHI), controlled unclassified information (CUI), and more. Such data may require compliance with security standards, such as HIPAA [39] and NIST SP 800-171 [40]. To protect this data, a range of privacy-preserving technologies are available, such as data anonymization, obfuscation, randomization, masking, and differential privacy. However, applying these technologies in a way that maintains HPC usability and performance can be challenging."
          }
        ]
      },
      {
        "title": "3.3. Other Threats",
        "content": [
          {
            "text": "In addition to the threats that are unique to individual function zones, general HPC systems face the following:"
          },
          {
            "text": "Environmental and physical threats: Physical or cyber attacks against facilities (e.g., power, cooling, water), unauthorized physical access, and natural disasters (e.g., fire, flood, earthquake, hurricane, etc.) are all potential threats to an HPC system.\n\n\nVulnerabilities introduced by prioritizing performance in HPC design and operation: HPC is designed to process large volumes of data and perform complex computations at very high speeds. Achieving the highest performance possible is a priority in HPC design and operation. Such prioritization, however, has its security implications. For instance, designers often make conscious decisions to build a less redundant system to achieve high performance, making the system less robust and potentially more vulnerable to attacks, such as DoS attacks. As another example, using a backup system to improve system robustness and high availability is a proven technology. However, building a spare or backup HPC system is often too costly. HPC systems can often lack storage backup due to the vast size of the data stored. Similarly, most HPC missions do not have a service-level agreement to justify the need for a full backup system at a# Backup Location"
          },
          {
            "text": "Environmental and physical threats: Physical or cyber attacks against facilities (e.g., power, cooling, water), unauthorized physical access, and natural disasters (e.g., fire, flood, earthquake, hurricane, etc.) are all potential threats to an HPC system."
          },
          {
            "text": "Vulnerabilities introduced by prioritizing performance in HPC design and operation: HPC is designed to process large volumes of data and perform complex computations at very high speeds. Achieving the highest performance possible is a priority in HPC design and operation. Such prioritization, however, has its security implications. For instance, designers often make conscious decisions to build a less redundant system to achieve high performance, making the system less robust and potentially more vulnerable to attacks, such as DoS attacks. As another example, using a backup system to improve system robustness and high availability is a proven technology. However, building a spare or backup HPC system is often too costly. HPC systems can often lack storage backup due to the vast size of the data stored. Similarly, most HPC missions do not have a service-level agreement to justify the need for a full backup system at a# Backup Location"
          },
          {
            "text": "All resources are poured into building the single best HPC system possible."
          }
        ]
      },
      {
        "title": "Supply Chain Threats",
        "content": [
          {
            "text": "The HPC supply chain faces a variety of threats, from the theft of proprietary information to attacks on critical hardware components and software manipulation to gain unauthorized access. Some HPC software (e.g., OS, applications), firmware, and hardware components have limited manufacturers, suppliers, and integrators, which makes diversification difficult. Limited suppliers also lead to shortages in the qualified workforce who can provide required technical support."
          }
        ]
      }
    ]
  },
  {
    "title": "HPC Security Posture, Challenges, and Recommendations",
    "subsections": [
      {
        "title": "HPC Access Control via Network Segmentation",
        "content": [
          {
            "text": "Access control is a security technique that regulates who can access and/or use resources in a computing environment. In HPC, multiple physical networks are constructed as an effective means of access control:"
          },
          {
            "subsection": "Management Network",
            "text": []
          },
          {
            "text": "The management network is a dedicated network that allows system administrators to remotely control, monitor, and configure computer nodes in an HPC system. Modern computers are often equipped with the Intelligent Platform Management Interface (IPMI) [41], which provides management and monitoring capabilities that are independent of the host system's CPU, firmware (e.g., BIOS [42], Unified Extensible Firmware Interface [UEFI] [43]), and operating system. For example, IPMI allows system administrators to remotely turn unresponsive machines on or off and install custom operating systems. IPMIs are connected to the management network, which can only be accessed by authorized system administrators. Collecting logs from the HPC system and forwarding the relevant logs to the security and information event management (SIEM) system, which could be external to the HPC system, is a component of the management zone."
          },
          {
            "subsection": "High-Performance Networks",
            "text": []
          },
          {
            "text": "High-performance networks offer high bandwidth and low latency to connect computer nodes inside of the high-performance zone and data storage zone. They also support features that are unique to HPC, such as remote direct memory access (RDMA) over the network and the message passing interface (MPI) [44]. High-performance networks often use special communications standards and architectures (e.g., InfiniBand, Slingshot, Omni-Path, etc.)."
          },
          {
            "subsection": "Auxiliary Networks",
            "text": []
          },
          {
            "text": "Additional auxiliary networks can be added to support usability and system manageability. For instance, a user network is constructed to allow users to manipulate or share data or remotely log into and access the compute nodes. Depending on the purpose of the networks, a subset of nodes from different zones are selected to be party to the networks. As an example, a user network contains the nodes.# Current Page Raw OCR Text"
          }
        ]
      },
      {
        "title": "In the Access Zone and the Computer Nodes in the High-Performance Computing Zone",
        "content": [
          {
            "text": "There are many benefits to having multiple networks in an HPC system. First, all of these networks are private and use different IP address ranges. Network traffic will remain on one network, which facilitates monitoring and measurement. Second, individual networks often serve specific purposes so only the relevant nodes are connected to the network. The networks effectively segment the HPC system into smaller segments, which improves security. Finally, multiple physical networks provide a degree of fault tolerance. When one network goes down, the system administrator can use the other network to diagnose and troubleshoot."
          },
          {
            "text": "The compute nodes in the access zone are connected to the external network and assigned public IP addresses, which allow users to remotely access the HPC systems. The user data can be shared through the login nodes or the data transfer nodes. Some systems allow storage to be exported using CIFS [10] or SMB [11] (e.g., via a SAMBA [45,7,7] server). If necessary, a network address translation (NAT) [46,3] or a proxy [47] can be installed to allow users on a private network to access the internet and download new versions of software or share software data. However, a NAT and Squid proxy can also be security risks that demand extra caution and mitigation considerations."
          },
          {
            "text": "Employing multiple physical networks is a common and effective means for access control and fault tolerance, and it is highly recommended."
          }
        ]
      },
      {
        "title": "4.2. Compute Node Sanitization",
        "content": [
          {
            "text": "High-performance compute nodes are often used by multiple tenants and projects. At the end of a task run, the previous project may leave behind a residual \"footprint,\" such as the data in memory and GPUs. It is important to sanitize the compute node so that data from previous jobs are not accidentally leaked, and the new job can start with a clean slate."
          },
          {
            "text": "Examples of compute node sanitization include:"
          },
          {
            "text": "Conducting a node health check at the end of a job\nRemoving a node or forcing a reboot if a node is deemed \"unhealthy\"\nWorking with hardware and software vendors to provide management hooks to sanitize the GPU\nResetting GPUs to remove residual data between jobs\nValidating and checking firmware\nRebooting nodes after the completion of a job at the OS level to remove accumulated residuals and ensure a consistent node state\nChecking critical files to ensure that they have not been changed"
          },
          {
            "text": "Compute node sanitization is highly recommended as the compute nodes are equipped with sophisticated hardware accelerators. Nevertheless, the process of sanitization must be carefully balanced with the goal of maximizing the utilization of HPC systems."
          }
        ]
      },
      {
        "title": "4.3. Data Integrity Protection",
        "content": [
          {
            "text": "Cryptographic mechanisms are an effective means of providing data integrity. HPC data storage systems typically support uniform encryption at the file level or block level. Such data encryption at the file system level protects data from unauthorized access.# CURRENT_PAGE_RAW_OCR_TEXT"
          },
          {
            "text": "However, it does not provide granular access (i.e., segmenting one user from others). Granular access can be achieved using user-level or group-level encryption. Not even a system administrator can access a user's data with granular access since they do not have access to the decryption key."
          },
          {
            "text": "Additionally, file systems do not authorize users. Rather, users access the file system via the HPC access zone, which is responsible for authenticating the users and their access rights. In general, file systems should not be mounted outside of their local security boundaries unless the file system protocol appropriately authenticates users. For example, an attacker-controlled system that can remotely mount a file system will have complete control over all file system data. This access could be used to gain privileges elsewhere within other zones of the HPC security enclave."
          },
          {
            "text": "Hashing is another technique for monitoring data integrity. Data files can be hashed at the beginning to acquire hashing keys. A file is not modified if its hashing key remains the same. Parallel file systems maintain many types of metadata (e.g., user ID, group ID, modification time, checksum, etc.). Hashing metadata is another way to check whether a file has been modified."
          },
          {
            "text": "Periodically scanning file systems for malware is a proven technique for monitoring data integrity. However, scanning an HPC system for malware is challenging. HPC data storage can easily contain a petabyte or more of data. Existing malware scanning tools are mainly designed for a single machine or laptop. They are not efficient or fast enough to scan large HPC data storage systems. Furthermore, the scanning operation can adversely affect the performance of running jobs. HPC systems may consider alternate implementation options for malware scanning to alleviate some of the stated impacts. For example, conduct scanning at write, and conduct subsequent batch scanning at read. Protection against ransomware attacks on HPC data is especially important. It is recommended to conduct risk assessments of ransomware attacks, and plan and implement mechanisms to protect the HPC system from ransomware attacks, as applicable [48,40,40]."
          },
          {
            "text": "Protecting data integrity is vital to HPC security. Granular data access provides the best protection and is highly recommended when possible."
          }
        ]
      }
    ]
  },
  {
    "title": "CURRENT_PAGE_RAW_OCR_TEXT",
    "subsections": [
      {
        "content": "Additionally, container contents may not be observable by some security auditing tools. In a well-managed HPC system, most software has already been installed as a baseline system environment. The applications developed using native libraries often run faster than a container. Hence, training users to develop programs in the HPC programming environment is one way to reduce exposure to container vulnerabilities. Additionally, many container ecosystems include the ability to attest that a container and its dependencies are supplied by a trusted provider, and there are tools (e.g., Qualys [49], Anchore [50], etc.) designed to audit container contents that provide supply chain and software audit management for HPC applications."
      },
      {
        "content": "Containers, container runtimes, and container orchestrators all increase the security attack surface for HPC systems, but carefully implemented container environments may potentially improve the overall security posture. Vulnerable or poorly configured container runtimes or container orchestrators can lead to exploitable environments and, in some cases, even privilege escalation. Additionally, many orchestration and runtime tools expose APIs that must be appropriately isolated and secured. However, when carefully implemented, container isolation can reduce risk by restricting a container from performing disallowed operations. For instance, data access to a container can be limited to the least required filesystem mounts, network access can be limited through network namespaces, and tools like seccomp [35] can be used to restrict containers from making disallowed system calls."
      },
      {
        "title": "4.5. Achieving Security While Maintaining HPC Performance",
        "content": [
          {
            "text": "HPC security measures often come with an undesirable performance penalty. The following are several effective ways to balance performance and security:"
          },
          {
            "text": "Conduct tests to measure the performance penalty of security tools, which can be benchmarked to determine whether they are acceptable. Testing and measurement would also encourage more performance-aware tool design.\nIncorporate security requirements in the initial HPC design rather than as an afterthought. For instance, independent add-on security tools tend to have more impact on performance than native security measures that come with the HPC software stack. Key management architecture plays an important role in HPC security but requires careful performance analysis.\nAvoid \"one size fits all\" security. Differentiate the types of nodes in the HPC system, and apply appropriate security rules and controls to different node types. For instance, classify the nodes in HPC systems into three categories: external nodes, internal nodes, and backend nodes. Apply individual security controls to each node category. Such a differentiation also mitigates performance impacts."
          }
        ]
      },
      {
        "title": "4.6. Challenges to HPC Security Tools",
        "content": [
          {
            "text": "Many enterprise security tools are designed with stand-alone devices in mind (e.g., laptops, desktops, or mobile devices). HPC is a large-scale, complex system with strict\n```# Performance Requirements"
          },
          {
            "text": "Security tools that are effective for individual devices may not work well in an HPC environment. For example, a forensic tool that aids the recovery and preservation of a hard drive and memory for a single server works well in practice. It is unreasonable, however, to install forensic tools on all compute and storage nodes in an HPC system. As another example, HPC nodes may use remotely mounted storage, which may disable some security tools."
          },
          {
            "text": "Moreover, different HPC applications may require different tools. Security tool vendors are often not accustomed to HPC use cases and requirements, which forces HPC security teams to develop analogous tools that may introduce new security vulnerabilities and are sometimes not accepted by organizations. The HPC community needs to work closely with security tool vendors to address these challenges."
          },
          {
            "text": "A Security Technical Implementation Guide (STIG) [51] is a configuration standard and offers a security baseline that reflects security guidance requirements. The security checking tool can measure how well the STIG is satisfied. However, available STIGs are typically written for servers or desktops rather than for HPC systems. In addition, security baseline checking tools developed for commodity operating systems and applications require customizations to run on HPC. The Lawrence Livermore National Laboratory [52], Sandia National Laboratories [53], and the Los Alamos National Lab [54] have collaborated with the Defense Information Systems Agency (DISA) [55] to develop the TOSS 4 STIG [56], which is geared toward HPC systems. Still, a more general STIG library and corresponding security checking tools are desirable to handle diverse subsystems and components inside an HPC system."
          }
        ]
      }
    ]
  }
]